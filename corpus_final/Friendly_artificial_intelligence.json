{
    "url": "https://en.wikipedia.org/wiki/Friendly_AI",
    "title": "Friendly artificial intelligence",
    "table_of_contents": [
        "1 Etymology and usage",
        "2 Risks of unfriendly AI",
        "3 Coherent extrapolated volition",
        "4 Other approaches",
        "5 Public policy",
        "6 Criticism",
        "7 See also",
        "8 References",
        "9 Further reading",
        "10 External links"
    ],
    "paragraphs": [
        {
            "title": "",
            "text": "Friendly artificial intelligence (also friendly AI or FAI) refers to hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to foster the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.\n\n"
        },
        {
            "title": "Etymology and usage",
            "text": "The term was coined by Eliezer Yudkowsky,[1] who is best known for popularizing the idea,[2][3] to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach, describes the idea:[2]\n\n'Friendly' is used in this context as technical terminology, and picks out agents that are safe and useful, not necessarily ones that are \"friendly\" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly explode in intelligence, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.[4]\n\n"
        },
        {
            "title": "Risks of unfriendly AI",
            "text": "The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem, or the proto-robots of Gerbert of Aurillac and Roger Bacon.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.[5] By 1942 these themes prompted Isaac Asimov to create the \"Three Laws of Robotics\"—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.[6]\n\nIn modern times as the prospect of superintelligent AI looms nearer, philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity.  He put it this way:\n\nIn 2008 Eliezer Yudkowsky called for the creation of \"friendly AI\" to mitigate existential risk from advanced artificial intelligence. He explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"[7]\n\nSteve Omohundro says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of basic \"drives\", such as resource acquisition, self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, \"without special precautions\", cause the AI to exhibit undesired behavior.[8][9]\n\nAlexander Wissner-Gross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.[10][11]\n\nLuke Muehlhauser, writing for the Machine Intelligence Research Institute, recommends that machine ethics researchers adopt what Bruce Schneier has called the \"security mindset\": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.[12]\n\nIn 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI';[13] nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.[14][15]\n\n"
        },
        {
            "title": "Coherent extrapolated volition",
            "text": "Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, coherent extrapolated volition is people's choices and the actions people would collectively take if \"we knew more, thought faster, were more the people we wished we were, and had grown up closer together.\"[16]\n\nRather than a Friendly AI being designed directly by human programmers, it is to be designed by a \"seed AI\" programmed to first study human nature and then produce the AI which humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.[16] The appeal to an objective through contingent human nature (perhaps expressed, for mathematical purposes, in the form of a utility function or other decision-theoretic formalism), as providing the ultimate criterion of \"Friendliness\", is an answer to the meta-ethical problem of defining an objective morality; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.\n\n"
        },
        {
            "title": "Other approaches",
            "text": "Steve Omohundro has proposed a \"scaffolding\" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.[17]\n\nSeth Baum argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities, and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, \"existing messages about beneficial AI are not always framed well\". Baum advocates for \"cooperative relationships, and positive framing of AI researchers\" and cautions against characterizing AI researchers as \"not want(ing) to pursue beneficial designs\".[18]\n\nIn his book Human Compatible, AI researcher Stuart J. Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:[19]: 173 \n\nThe \"preferences\" Russell refers to \"are all-encompassing; they cover everything you might care about, arbitrarily far into the future.\"[19]: 173   Similarly, \"behavior\" includes any choice between options,[19]: 177  and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.[19]: 201 \n\n"
        },
        {
            "title": "Public policy",
            "text": "James Barrat, author of Our Final Invention, suggested that \"a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations.\" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA, which discussed risks of biotechnology.[17]\n\nJohn McGinnis encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the National Institutes of Health, where \"Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards.\" McGinnis feels that peer review is better \"than regulation to address technical issues that are not possible to capture through bureaucratic mandates\". McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute, which generally aims to avoid government involvement in friendly AI.[20]\n\nAccording to Gary Marcus, the annual amount of money being spent on developing machine morality is tiny.[21]\n\n"
        },
        {
            "title": "Criticism",
            "text": "Some critics believe that both human-level AI and superintelligence are unlikely, and that therefore friendly AI is unlikely. Writing in The Guardian, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty, and states that while we need to be \"cautious and prepared\" given the stakes involved, we \"don't need to be obsessing\" about the risks of superintelligence.[22] Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and Nick Bostrom’s proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had.[13] In an article in AI & Society, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.[14]\n\nSome philosophers claim that any truly \"rational\" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.[23] Other critics question whether it is possible for an artificial intelligence to be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal The New Atlantis, say that it will be impossible to ever guarantee \"friendly\" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work \"only when one has not only great powers of prediction about the likelihood of myriad possible outcomes, but certainty and consensus on how one values the different outcomes.[24]\n\n"
        }
    ]
}