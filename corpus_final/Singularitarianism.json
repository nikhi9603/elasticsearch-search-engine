{
    "url": "https://en.wikipedia.org/wiki/Singularitarianism",
    "title": "Singularitarianism",
    "table_of_contents": [
        "1 Definition",
        "2 History",
        "3 Reception",
        "4 See also",
        "5 References",
        "6 External links"
    ],
    "paragraphs": [
        {
            "title": "",
            "text": "Singularitarianism is a movement defined by the belief that a technological singularity—the creation of superintelligence—will likely happen in the medium future, and that deliberate action ought to be taken to ensure that the singularity benefits humans.[1]\n\nSingularitarians are distinguished from other futurists who speculate on a technological singularity by their belief that the singularity is not only possible, but desirable if guided prudently. Accordingly, they might sometimes dedicate their lives to acting in ways they believe will contribute to its rapid yet safe realization.[2]\n\nTime magazine describes the worldview of Singularitarians by saying that \"even though it sounds like science fiction, it isn't, no more than a weather forecast is science fiction. It's not a fringe idea; it's a serious hypothesis about the future of life on Earth. There's an intellectual gag reflex that kicks in anytime you try to swallow an idea that involves super-intelligent immortal cyborgs, but... while the Singularity appears to be, on the face of it, preposterous, it's an idea that rewards sober, careful evaluation\".[1]\n\n"
        },
        {
            "title": "Definition",
            "text": "The term \"Singularitarian\" was originally defined by Extropian thinker Mark Plus (Mark Potts) in 1991 to mean \"one who believes the concept of a Singularity\".[citation needed] This term has since been redefined to mean \"Singularity activist\" or \"friend of the Singularity\"; that is, one who acts so as to bring about the singularity.[3]\n\nSingularitarianism can also be thought of as an orientation or an outlook that prefers the enhancement of human intelligence as a specific transhumanist goal instead of focusing on specific technologies such as A.I.[4] There are also definitions that identify a singularitarian as an activist or a friend of the concept of singularity, that is, one who acts so as to bring about a singularity.[5] Some sources described it as a moral philosophy that advocates deliberate action to bring about and steer the development of a superintelligence that will lead to a theoretical future point that emerges during a time of accelerated change.[6]\n\nInventor and futurist Ray Kurzweil, author of the 2005 book The Singularity Is Near: When Humans Transcend Biology, defines a Singularitarian as someone \"who understands the Singularity and who has reflected on its implications for his or her own life\"[citation needed] and estimates the singularity will occur around 2045.[2]\n\n"
        },
        {
            "title": "History",
            "text": "In 1993, mathematician, computer scientist, and science fiction author Vernor Vinge hypothesized that the moment might come when technology will allow \"creation of entities with greater than human intelligence\"[7] and used the term \"the Singularity\" to describe this moment.[8] He suggested that the singularity may pose an existential risk for humanity, and that it could happen through one of four means:\n\nSingularitarianism coalesced into a coherent ideology in 2000 when artificial intelligence (AI) researcher Eliezer Yudkowsky wrote The Singularitarian Principles,[2][10] in which he stated that a Singularitarian believes that the singularity is a secular, non-mystical event which is possible and beneficial to the world and is worked towards by its adherents.[10] Yudkowsky's conceptualization of singularity offered a broad definition developed to be inclusive of various interpretations.[4] There are theorists such as Michael Anissimov who argued for a strict definition, one that refers only to the advocacy of the development of posthuman (greater than human) intelligence.[4]\n\nIn June 2000 Yudkowsky, with the support of Internet entrepreneurs Brian Atkins and Sabine Atkins, founded the Machine Intelligence Research Institute to work towards the creation of self-improving Friendly AI. MIRI's writings argue for the idea that an AI with the ability to improve upon its own design (Seed AI) would rapidly lead to superintelligence. These Singularitarians believe that reaching the singularity swiftly and safely is the best possible way to minimize net existential risk.\n\nMany people believe a technological singularity is possible without adopting Singularitarianism as a moral philosophy. Although the exact numbers are hard to quantify, Singularitarianism is a small movement, which includes transhumanist philosopher Nick Bostrom. Inventor and futurist Ray Kurzweil, who predicts that the Singularity will occur circa 2045, greatly contributed to popularizing Singularitarianism with his 2005 book The Singularity Is Near: When Humans Transcend Biology .[2]\n\nWith the support of NASA, Google and a broad range of technology forecasters and technocapitalists, the Singularity University opened in June 2009 at the NASA Research Park in Silicon Valley with the goal of preparing the next generation of leaders to address the challenges of accelerating change.\n\nIn July 2009, many prominent Singularitarians participated in a conference organized by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard (i.e., cybernetic revolt). They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They warned that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They asserted that self-awareness as depicted in science fiction is probably unlikely, but that there were other potential hazards and pitfalls.[8] Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[11] The President of the AAAI has commissioned a study to look at this issue.[12]\n\n"
        },
        {
            "title": "Reception",
            "text": "There are several objections to Kurzweil's singularitarianism and these even include criticisms from optimists within the A.I. field. For instance, Pulitzer Prize winning author Douglas Hofstadter argued that Kurzweil's predicted achievement of human-level A.I. by 2045 is not viable.[13] Even Gordon Moore, who is credited for introducing the Moore's Law that predicated[14] the notion of singularity, maintained that it will never occur.[15] According to some observers, these criticisms do not diminish enthusiasm for singularity because it has assumed a quasi-religious response to the fear of death, allowing its adherents to enjoy the benefits of religion without its ontological burdens.[13] Science journalist John Horgan provided more insights into this notion as he likened singularitarianism to a religion:\n\nKurzweil rejects this categorization, stating that his predictions about the singularity are driven by the data that increases in computational technology have been exponential in the past.[17] He also stressed that critics who challenge his view mistakenly take an intuitive linear view of technological advancement.[18]\n\n"
        }
    ]
}