{
    "url": "https://en.wikipedia.org/wiki/Friendly_AI",
    "title": "Friendly artificial intelligence",
    "html": "<!DOCTYPE html>\n<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n<head>\n<meta charset=\"utf-8\"/>\n<title>Friendly artificial intelligence - Wikipedia</title>\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"38017821-7eca-407d-9bdf-5b9b9d7ba666\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Friendly_artificial_intelligence\",\"wgTitle\":\"Friendly artificial intelligence\",\"wgCurRevisionId\":1121465029,\"wgRevisionId\":1121465029,\"wgArticleId\":351887,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles with short description\",\"Short description is different from Wikidata\",\"Philosophy of artificial intelligence\",\"Singularitarianism\",\"Transhumanism\",\"Affective computing\"],\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\n\"wgRelevantPageName\":\"Friendly_artificial_intelligence\",\"wgRelevantArticleId\":351887,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgRedirectedFrom\":\"Friendly_AI\",\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":20000,\"wgNoticeProject\":\"wikipedia\",\"wgVector2022PreviewPages\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":10,\"wgULSCurrentAutonym\":\"English\",\"wgInternalRedirectTargetUrl\":\"/wiki/Friendly_artificial_intelligence\",\"wgEditSubmitButtonLabelPublish\":true,\"wgCentralAuthMobileDomain\":false,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":true,\"wgWikibaseItemId\":\"Q4168796\",\n\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"skins.vector.styles.legacy\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\"};RLPAGEMODULES=[\"mediawiki.action.view.redirect\",\"ext.cite.ux-enhancements\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.legacy.js\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.cx.eventlogging.campaigns\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\n\"ext.gadget.ReferenceTooltips\",\"ext.gadget.charinsert\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.popups\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.growthExperiments.SuggestedEditSession\"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement(\"user.options@12s5i\",function($,jQuery,require,module){mw.user.tokens.set({\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});});});</script>\n<link href=\"/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<script async=\"\" src=\"/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector\"></script>\n<meta content=\"\" name=\"ResourceLoaderDynamicStyles\"/>\n<link href=\"/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<meta content=\"MediaWiki 1.40.0-wmf.8\" name=\"generator\"/>\n<meta content=\"origin\" name=\"referrer\"/>\n<meta content=\"origin-when-crossorigin\" name=\"referrer\"/>\n<meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n<meta content=\"max-image-preview:standard\" name=\"robots\"/>\n<meta content=\"telephone=no\" name=\"format-detection\"/>\n<meta content=\"width=1000\" name=\"viewport\"/>\n<meta content=\"Friendly artificial intelligence - Wikipedia\" property=\"og:title\"/>\n<meta content=\"website\" property=\"og:type\"/>\n<link href=\"//upload.wikimedia.org\" rel=\"preconnect\"/>\n<link href=\"//en.m.wikipedia.org/wiki/Friendly_artificial_intelligence\" media=\"only screen and (max-width: 720px)\" rel=\"alternate\"/>\n<link href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit\" rel=\"alternate\" title=\"Edit this page\" type=\"application/x-wiki\"/>\n<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>\n<link href=\"/static/favicon/wikipedia.ico\" rel=\"icon\"/>\n<link href=\"/w/opensearch_desc.php\" rel=\"search\" title=\"Wikipedia (en)\" type=\"application/opensearchdescription+xml\"/>\n<link href=\"//en.wikipedia.org/w/api.php?action=rsd\" rel=\"EditURI\" type=\"application/rsd+xml\"/>\n<link href=\"https://creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\"/>\n<link href=\"https://en.wikipedia.org/wiki/Friendly_artificial_intelligence\" rel=\"canonical\"/>\n<link href=\"//meta.wikimedia.org\" rel=\"dns-prefetch\"/>\n<link href=\"//login.wikimedia.org\" rel=\"dns-prefetch\"/>\n</head>\n<body class=\"skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Friendly_artificial_intelligence rootpage-Friendly_artificial_intelligence skin-vector action-view vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-sticky-header-edit-disabled vector-feature-table-of-contents-legacy-toc-disabled vector-feature-visual-enhancement-next-disabled vector-feature-page-tools-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled\"><div class=\"noprint\" id=\"mw-page-base\"></div>\n<div class=\"noprint\" id=\"mw-head-base\"></div>\n<div class=\"mw-body\" id=\"content\" role=\"main\">\n<a id=\"top\"></a>\n<div id=\"siteNotice\"><!-- CentralNotice --><!--esi <esi:include src=\"/esitest-fa8a495983347898/content\" /> --> </div>\n<div class=\"mw-indicators\">\n</div>\n<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\"><span class=\"mw-page-title-main\">Friendly artificial intelligence</span></h1>\n<div class=\"vector-body\" id=\"bodyContent\">\n<div class=\"noprint\" id=\"siteSub\">From Wikipedia, the free encyclopedia</div>\n<div id=\"contentSub\"><span class=\"mw-redirectedfrom\">(Redirected from <a class=\"mw-redirect\" href=\"/w/index.php?title=Friendly_AI&amp;redirect=no\" title=\"Friendly AI\">Friendly AI</a>)</span></div>\n<div id=\"contentSub2\"></div>\n<div id=\"jump-to-nav\"></div>\n<a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\n<a class=\"mw-jump-link\" href=\"#searchInput\">Jump to search</a>\n<div class=\"mw-body-content mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\"><div class=\"mw-parser-output\"><div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">AI to benefit humanity</div>\n<style data-mw-deduplicate=\"TemplateStyles:r1045330069\">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><table class=\"sidebar sidebar-collapse nomobile nowraplinks hlist\"><tbody><tr><td class=\"sidebar-pretitle\">Part of a series on</td></tr><tr><th class=\"sidebar-title-with-pretitle\"><a href=\"/wiki/Outline_of_artificial_intelligence\" title=\"Outline of artificial intelligence\">Artificial intelligence</a></th></tr><tr><td class=\"sidebar-image\"><div class=\"center\"><div class=\"floatnone\"><a class=\"image\" href=\"/wiki/File:Anatomy-1751201_1280.png\"><img alt=\"Anatomy-1751201 1280.png\" data-file-height=\"1088\" data-file-width=\"1280\" decoding=\"async\" height=\"85\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Anatomy-1751201_1280.png/100px-Anatomy-1751201_1280.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Anatomy-1751201_1280.png/150px-Anatomy-1751201_1280.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Anatomy-1751201_1280.png/200px-Anatomy-1751201_1280.png 2x\" width=\"100\"/></a></div></div></td></tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\"><a href=\"/wiki/Artificial_intelligence#Goals\" title=\"Artificial intelligence\">Major goals</a></div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">Artificial general intelligence</a></li>\n<li><a href=\"/wiki/Automated_planning_and_scheduling\" title=\"Automated planning and scheduling\">Planning</a></li>\n<li><a href=\"/wiki/Computer_vision\" title=\"Computer vision\">Computer vision</a></li>\n<li><a href=\"/wiki/General_game_playing\" title=\"General game playing\">General game playing</a></li>\n<li><a href=\"/wiki/Knowledge_representation_and_reasoning\" title=\"Knowledge representation and reasoning\">Knowledge reasoning</a></li>\n<li><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></li>\n<li><a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></li>\n<li><a href=\"/wiki/Robotics\" title=\"Robotics\">Robotics</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\">Approaches</div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Symbolic_artificial_intelligence\" title=\"Symbolic artificial intelligence\">Symbolic</a></li>\n<li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li>\n<li><a href=\"/wiki/Bayesian_network\" title=\"Bayesian network\">Bayesian networks</a></li>\n<li><a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">Evolutionary algorithms</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\"><a href=\"/wiki/Philosophy_of_artificial_intelligence\" title=\"Philosophy of artificial intelligence\">Philosophy</a></div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a></li>\n<li><a class=\"mw-selflink selflink\">Friendly AI</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/AI_control_problem\" title=\"AI control problem\">Control problem</a>/<a href=\"/wiki/AI_takeover\" title=\"AI takeover\">Takeover</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk</a></li>\n<li><a href=\"/wiki/Turing_test\" title=\"Turing test\">Turing test</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\"><a href=\"/wiki/History_of_artificial_intelligence\" title=\"History of artificial intelligence\">History</a></div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Timeline_of_artificial_intelligence\" title=\"Timeline of artificial intelligence\">Timeline</a></li>\n<li><a href=\"/wiki/Progress_in_artificial_intelligence\" title=\"Progress in artificial intelligence\">Progress</a></li>\n<li><a href=\"/wiki/AI_winter\" title=\"AI winter\">AI winter</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\">Technology</div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Applications_of_artificial_intelligence\" title=\"Applications of artificial intelligence\">Applications</a></li>\n<li><a href=\"/wiki/List_of_artificial_intelligence_projects\" title=\"List of artificial intelligence projects\">Projects</a></li>\n<li><a href=\"/wiki/List_of_programming_languages_for_artificial_intelligence\" title=\"List of programming languages for artificial intelligence\">Programming languages</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\">Glossary</div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Glossary_of_artificial_intelligence\" title=\"Glossary of artificial intelligence\">Glossary</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-navbar\"><style data-mw-deduplicate=\"TemplateStyles:r1063604349\">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Artificial_intelligence\" title=\"Template:Artificial intelligence\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Artificial_intelligence\" title=\"Template talk:Artificial intelligence\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Artificial_intelligence&amp;action=edit\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p><b>Friendly artificial intelligence</b> (also <b>friendly AI</b> or <b>FAI</b>) refers to hypothetical <a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">artificial general intelligence</a> (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to foster the improvement of the human species. It is a part of the <a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">ethics of artificial intelligence</a> and is closely related to <a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">machine ethics</a>. While machine ethics is concerned with how an artificially intelligent agent <i>should</i> behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.\n</p>\n<div aria-labelledby=\"mw-toc-heading\" class=\"toc\" id=\"toc\" role=\"navigation\"><input class=\"toctogglecheckbox\" id=\"toctogglecheckbox\" role=\"button\" style=\"display:none\" type=\"checkbox\"/><div class=\"toctitle\" dir=\"ltr\" lang=\"en\"><h2 id=\"mw-toc-heading\">Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Etymology_and_usage\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Etymology and usage</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Risks_of_unfriendly_AI\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Risks of unfriendly AI</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Coherent_extrapolated_volition\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Coherent extrapolated volition</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Other_approaches\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Other approaches</span></a></li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Public_policy\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Public policy</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#Criticism\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Criticism</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#See_also\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#References\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#Further_reading\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">Further reading</span></a></li>\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#External_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"Etymology_and_usage\">Etymology and usage</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=1\" title=\"Edit section: Etymology and usage\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a class=\"image\" href=\"/wiki/File:Eliezer_Yudkowsky,_Stanford_2006_(square_crop).jpg\"><img alt=\"\" class=\"thumbimage\" data-file-height=\"1724\" data-file-width=\"1724\" decoding=\"async\" height=\"220\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/3/35/Eliezer_Yudkowsky%2C_Stanford_2006_%28square_crop%29.jpg/220px-Eliezer_Yudkowsky%2C_Stanford_2006_%28square_crop%29.jpg\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/3/35/Eliezer_Yudkowsky%2C_Stanford_2006_%28square_crop%29.jpg/330px-Eliezer_Yudkowsky%2C_Stanford_2006_%28square_crop%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/35/Eliezer_Yudkowsky%2C_Stanford_2006_%28square_crop%29.jpg/440px-Eliezer_Yudkowsky%2C_Stanford_2006_%28square_crop%29.jpg 2x\" width=\"220\"/></a> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:Eliezer_Yudkowsky,_Stanford_2006_(square_crop).jpg\" title=\"Enlarge\"></a></div><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a>, AI researcher and creator of the term Friendly artificial intelligence</div></div></div>\n<p>The term was coined by <a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a>,<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup> who is best known for popularizing the idea,<sup class=\"reference\" id=\"cite_ref-aima_2-0\"><a href=\"#cite_note-aima-2\">[2]</a></sup><sup class=\"reference\" id=\"cite_ref-3\"><a href=\"#cite_note-3\">[3]</a></sup> to discuss <a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">superintelligent</a> artificial agents that reliably implement human values. <a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a> and <a href=\"/wiki/Peter_Norvig\" title=\"Peter Norvig\">Peter Norvig</a>'s leading <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> textbook, <i><a href=\"/wiki/Artificial_Intelligence:_A_Modern_Approach\" title=\"Artificial Intelligence: A Modern Approach\">Artificial Intelligence: A Modern Approach</a></i>, describes the idea:<sup class=\"reference\" id=\"cite_ref-aima_2-1\"><a href=\"#cite_note-aima-2\">[2]</a></sup>\n</p>\n<blockquote><p>Yudkowsky (2008) goes into more detail about how to design a <b>Friendly AI</b>. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.</p></blockquote>\n<p>'Friendly' is used in this context as <a class=\"mw-redirect\" href=\"/wiki/Technical_terminology\" title=\"Technical terminology\">technical terminology</a>, and picks out agents that are safe and useful, not necessarily ones that are \"friendly\" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly <a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">explode in intelligence</a>, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.<sup class=\"reference\" id=\"cite_ref-4\"><a href=\"#cite_note-4\">[4]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Risks_of_unfriendly_AI\">Risks of unfriendly AI</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=2\" title=\"Edit section: Risks of unfriendly AI\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r1033289096\">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div class=\"hatnote navigation-not-searchable\" role=\"note\">Main article: <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></div>\n<p>The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the <a href=\"/wiki/Golem\" title=\"Golem\">golem</a>, or the proto-robots of <a class=\"mw-redirect\" href=\"/wiki/Gerbert_of_Aurillac\" title=\"Gerbert of Aurillac\">Gerbert of Aurillac</a> and <a href=\"/wiki/Roger_Bacon\" title=\"Roger Bacon\">Roger Bacon</a>.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.<sup class=\"reference\" id=\"cite_ref-5\"><a href=\"#cite_note-5\">[5]</a></sup> By 1942 these themes prompted <a href=\"/wiki/Isaac_Asimov\" title=\"Isaac Asimov\">Isaac Asimov</a> to create the \"<a href=\"/wiki/Three_Laws_of_Robotics\" title=\"Three Laws of Robotics\">Three Laws of Robotics</a>\"—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.<sup class=\"reference\" id=\"cite_ref-6\"><a href=\"#cite_note-6\">[6]</a></sup>\n</p><p>In modern times as the prospect of <a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">superintelligent AI</a> looms nearer, philosopher <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity.  He put it this way:\n</p>\n<blockquote><p>Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.'</p></blockquote>\n<p>In 2008 Eliezer Yudkowsky called for the creation of \"friendly AI\" to mitigate <a class=\"mw-redirect\" href=\"/wiki/Existential_risk_from_advanced_artificial_intelligence\" title=\"Existential risk from advanced artificial intelligence\">existential risk from advanced artificial intelligence</a>. He explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"<sup class=\"reference\" id=\"cite_ref-7\"><a href=\"#cite_note-7\">[7]</a></sup>\n</p><p><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a> says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of <a href=\"/wiki/Instrumental_convergence#Basic_AI_drives\" title=\"Instrumental convergence\">basic \"drives\"</a>, such as resource acquisition, self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, \"without special precautions\", cause the AI to exhibit undesired behavior.<sup class=\"reference\" id=\"cite_ref-8\"><a href=\"#cite_note-8\">[8]</a></sup><sup class=\"reference\" id=\"cite_ref-9\"><a href=\"#cite_note-9\">[9]</a></sup>\n</p><p><a href=\"/wiki/Alexander_Wissner-Gross\" title=\"Alexander Wissner-Gross\">Alexander Wissner-Gross</a> says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.<sup class=\"reference\" id=\"cite_ref-10\"><a href=\"#cite_note-10\">[10]</a></sup><sup class=\"reference\" id=\"cite_ref-11\"><a href=\"#cite_note-11\">[11]</a></sup>\n</p><p>Luke Muehlhauser, writing for the <a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a>, recommends that <a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">machine ethics</a> researchers adopt what <a href=\"/wiki/Bruce_Schneier\" title=\"Bruce Schneier\">Bruce Schneier</a> has called the \"security mindset\": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.<sup class=\"reference\" id=\"cite_ref-MuehlhauserSecurity2013_12-0\"><a href=\"#cite_note-MuehlhauserSecurity2013-12\">[12]</a></sup>\n</p><p>In 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI';<sup class=\"reference\" id=\"cite_ref-think13_13-0\"><a href=\"#cite_note-think13-13\">[13]</a></sup> nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.<sup class=\"reference\" id=\"cite_ref-boyles2019_14-0\"><a href=\"#cite_note-boyles2019-14\">[14]</a></sup><sup class=\"reference\" id=\"cite_ref-15\"><a href=\"#cite_note-15\">[15]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Coherent_extrapolated_volition\">Coherent extrapolated volition</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=3\" title=\"Edit section: Coherent extrapolated volition\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, coherent extrapolated volition is people's choices and the actions people would collectively take if \"we knew more, thought faster, were more the people we wished we were, and had grown up closer together.\"<sup class=\"reference\" id=\"cite_ref-cevpaper_16-0\"><a href=\"#cite_note-cevpaper-16\">[16]</a></sup>\n</p><p>Rather than a Friendly AI being designed directly by human programmers, it is to be designed by a \"seed AI\" programmed to first study <a href=\"/wiki/Human_nature\" title=\"Human nature\">human nature</a> and then produce the AI which humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.<sup class=\"reference\" id=\"cite_ref-cevpaper_16-1\"><a href=\"#cite_note-cevpaper-16\">[16]</a></sup> The appeal to an <a href=\"/wiki/Evolutionary_psychology\" title=\"Evolutionary psychology\">objective through contingent human nature</a> (perhaps expressed, for mathematical purposes, in the form of a <a class=\"mw-redirect\" href=\"/wiki/Utility_function\" title=\"Utility function\">utility function</a> or other <a href=\"/wiki/Decision_theory\" title=\"Decision theory\">decision-theoretic</a> formalism), as providing the ultimate criterion of \"Friendliness\", is an answer to the <a class=\"mw-redirect\" href=\"/wiki/Metaethics\" title=\"Metaethics\">meta-ethical</a> problem of defining an <a href=\"/wiki/Moral_universalism\" title=\"Moral universalism\">objective morality</a>; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.\n</p>\n<h2><span class=\"mw-headline\" id=\"Other_approaches\">Other approaches</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=4\" title=\"Edit section: Other approaches\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a class=\"mw-redirect\" href=\"/wiki/AI_control_problem#Alignment\" title=\"AI control problem\">AI control problem § Alignment</a></div>\n<p><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a> has proposed a \"scaffolding\" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.<sup class=\"reference\" id=\"cite_ref-Hendry2014_17-0\"><a href=\"#cite_note-Hendry2014-17\">[17]</a></sup>\n</p><p><a href=\"/wiki/Seth_Baum\" title=\"Seth Baum\">Seth Baum</a> argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities, and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, \"existing messages about beneficial AI are not always framed well\". Baum advocates for \"cooperative relationships, and positive framing of AI researchers\" and cautions against characterizing AI researchers as \"not want(ing) to pursue beneficial designs\".<sup class=\"reference\" id=\"cite_ref-18\"><a href=\"#cite_note-18\">[18]</a></sup>\n</p><p>In his book <i><a href=\"/wiki/Human_Compatible\" title=\"Human Compatible\">Human Compatible</a></i>, AI researcher <a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a> lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:<sup class=\"reference\" id=\"cite_ref-HC_19-0\"><a href=\"#cite_note-HC-19\">[19]</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 173\">: 173 </span></sup>\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r996844942\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class=\"templatequote\"><p>1. The machine's only objective is to maximize the realization of human preferences.\n</p><p>2. The machine is initially uncertain about what those preferences are.\n</p><p>\n3. The ultimate source of information about human preferences is human behavior.</p></blockquote>\n<p>The \"preferences\" Russell refers to \"are all-encompassing; they cover everything you might care about, arbitrarily far into the future.\"<sup class=\"reference\" id=\"cite_ref-HC_19-1\"><a href=\"#cite_note-HC-19\">[19]</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 173\">: 173 </span></sup>  Similarly, \"behavior\" includes any choice between options,<sup class=\"reference\" id=\"cite_ref-HC_19-2\"><a href=\"#cite_note-HC-19\">[19]</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 177\">: 177 </span></sup> and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.<sup class=\"reference\" id=\"cite_ref-HC_19-3\"><a href=\"#cite_note-HC-19\">[19]</a></sup><sup class=\"reference nowrap\"><span title=\"Page / location: 201\">: 201 </span></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Public_policy\">Public policy</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=5\" title=\"Edit section: Public policy\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><a href=\"/wiki/James_Barrat\" title=\"James Barrat\">James Barrat</a>, author of <i><a href=\"/wiki/Our_Final_Invention\" title=\"Our Final Invention\">Our Final Invention</a></i>, suggested that \"a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations.\" He urges AI researchers to convene a meeting similar to the <a href=\"/wiki/Asilomar_Conference_on_Recombinant_DNA\" title=\"Asilomar Conference on Recombinant DNA\">Asilomar Conference on Recombinant DNA</a>, which discussed risks of biotechnology.<sup class=\"reference\" id=\"cite_ref-Hendry2014_17-1\"><a href=\"#cite_note-Hendry2014-17\">[17]</a></sup>\n</p><p><a href=\"/wiki/John_McGinnis\" title=\"John McGinnis\">John McGinnis</a> encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the <a href=\"/wiki/National_Institutes_of_Health\" title=\"National Institutes of Health\">National Institutes of Health</a>, where \"Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards.\" McGinnis feels that peer review is better \"than regulation to address technical issues that are not possible to capture through bureaucratic mandates\". McGinnis notes that his proposal stands in contrast to that of the <a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a>, which generally aims to avoid government involvement in friendly AI.<sup class=\"reference\" id=\"cite_ref-McGinnis2010_20-0\"><a href=\"#cite_note-McGinnis2010-20\">[20]</a></sup>\n</p><p>According to <a href=\"/wiki/Gary_Marcus\" title=\"Gary Marcus\">Gary Marcus</a>, the annual amount of money being spent on developing machine morality is tiny.<sup class=\"reference\" id=\"cite_ref-21\"><a href=\"#cite_note-21\">[21]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Criticism\">Criticism</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=6\" title=\"Edit section: Criticism\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a href=\"/wiki/Technological_singularity#Criticisms\" title=\"Technological singularity\">Technological singularity § Criticisms</a></div>\n<p>Some critics believe that both human-level AI and superintelligence are unlikely, and that therefore friendly AI is unlikely. Writing in <i><a href=\"/wiki/The_Guardian\" title=\"The Guardian\">The Guardian</a></i>, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty, and states that while we need to be \"cautious and prepared\" given the stakes involved, we \"don't need to be obsessing\" about the risks of superintelligence.<sup class=\"reference\" id=\"cite_ref-22\"><a href=\"#cite_note-22\">[22]</a></sup> Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a>’s proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had.<sup class=\"reference\" id=\"cite_ref-think13_13-1\"><a href=\"#cite_note-think13-13\">[13]</a></sup> In an article in <i><a href=\"/wiki/AI_%26_Society\" title=\"AI &amp; Society\">AI &amp; Society</a></i>, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.<sup class=\"reference\" id=\"cite_ref-boyles2019_14-1\"><a href=\"#cite_note-boyles2019-14\">[14]</a></sup>\n</p><p>Some philosophers claim that any truly \"rational\" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.<sup class=\"reference\" id=\"cite_ref-23\"><a href=\"#cite_note-23\">[23]</a></sup> Other critics question whether it is possible for an artificial intelligence to be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal <i><a href=\"/wiki/The_New_Atlantis_(journal)\" title=\"The New Atlantis (journal)\">The New Atlantis</a></i>, say that it will be impossible to ever guarantee \"friendly\" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work \"only when one has not only great powers of prediction about the likelihood of myriad possible outcomes, but certainty and consensus on how one values the different outcomes.<sup class=\"reference\" id=\"cite_ref-24\"><a href=\"#cite_note-24\">[24]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=7\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r998391716\">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style><div class=\"div-col\" style=\"column-width: 30em;\">\n<ul><li><a href=\"/wiki/Affective_computing\" title=\"Affective computing\">Affective computing</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/AI_control_problem\" title=\"AI control problem\">AI control problem</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/Artificial_intelligence_arms_race\" title=\"Artificial intelligence arms race\">Artificial intelligence arms race</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Emotion_recognition\" title=\"Emotion recognition\">Emotion recognition</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">Intelligence explosion</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li>\n<li><a href=\"/wiki/Regulation_of_algorithms\" title=\"Regulation of algorithms\">Regulation of algorithms</a></li>\n<li><a href=\"/wiki/Roko%27s_basilisk\" title=\"Roko's basilisk\">Roko's basilisk</a></li>\n<li><a href=\"/wiki/Sentiment_analysis\" title=\"Sentiment analysis\">Sentiment analysis</a></li>\n<li><a href=\"/wiki/Singularitarianism\" title=\"Singularitarianism\">Singularitarianism</a> – a moral philosophy advocated by proponents of Friendly AI</li>\n<li><a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">Technological singularity</a></li>\n<li><a href=\"/wiki/Three_Laws_of_Robotics\" title=\"Three Laws of Robotics\">Three Laws of Robotics</a></li></ul></div>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=8\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r1011085734\">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class=\"reflist reflist-columns references-column-width\" style=\"column-width: 30em;\">\n<ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><style data-mw-deduplicate=\"TemplateStyles:r1067248974\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite class=\"citation book cs1\" id=\"CITEREFTegmark2014\">Tegmark, Max (2014). \"Life, Our Universe and Everything\". <a class=\"mw-redirect\" href=\"/wiki/Our_Mathematical_Universe:_My_Quest_for_the_Ultimate_Nature_of_Reality\" title=\"Our Mathematical Universe: My Quest for the Ultimate Nature of Reality\"><i>Our Mathematical Universe: My Quest for the Ultimate Nature of Reality</i></a> (First ed.). <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/9780307744258\" title=\"Special:BookSources/9780307744258\"><bdi>9780307744258</bdi></a>. <q>Its owner may cede control to what Eliezer Yudkowsky terms a \"Friendly AI,\"...</q></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Life%2C+Our+Universe+and+Everything&amp;rft.btitle=Our+Mathematical+Universe%3A+My+Quest+for+the+Ultimate+Nature+of+Reality&amp;rft.edition=First&amp;rft.date=2014&amp;rft.isbn=9780307744258&amp;rft.aulast=Tegmark&amp;rft.aufirst=Max&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-aima-2\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-aima_2-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-aima_2-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRussellNorvig2009\"><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Russell, Stuart</a>; <a href=\"/wiki/Peter_Norvig\" title=\"Peter Norvig\">Norvig, Peter</a> (2009). <a href=\"/wiki/Artificial_Intelligence:_A_Modern_Approach\" title=\"Artificial Intelligence: A Modern Approach\"><i>Artificial Intelligence: A Modern Approach</i></a>. Prentice Hall. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-13-604259-4\" title=\"Special:BookSources/978-0-13-604259-4\"><bdi>978-0-13-604259-4</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pub=Prentice+Hall&amp;rft.date=2009&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFLeighton2011\">Leighton, Jonathan (2011). <i>The Battle for Compassion: Ethics in an Apathetic Universe</i>. Algora. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-87586-870-7\" title=\"Special:BookSources/978-0-87586-870-7\"><bdi>978-0-87586-870-7</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Battle+for+Compassion%3A+Ethics+in+an+Apathetic+Universe&amp;rft.pub=Algora&amp;rft.date=2011&amp;rft.isbn=978-0-87586-870-7&amp;rft.aulast=Leighton&amp;rft.aufirst=Jonathan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFWallachAllen2009\">Wallach, Wendell; Allen, Colin (2009). <i>Moral Machines: Teaching Robots Right from Wrong</i>. Oxford University Press, Inc. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-19-537404-9\" title=\"Special:BookSources/978-0-19-537404-9\"><bdi>978-0-19-537404-9</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Moral+Machines%3A+Teaching+Robots+Right+from+Wrong&amp;rft.pub=Oxford+University+Press%2C+Inc.&amp;rft.date=2009&amp;rft.isbn=978-0-19-537404-9&amp;rft.aulast=Wallach&amp;rft.aufirst=Wendell&amp;rft.au=Allen%2C+Colin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKevin_LaGrandeur2011\"><a class=\"new\" href=\"/w/index.php?title=Kevin_LaGrandeur&amp;action=edit&amp;redlink=1\" title=\"Kevin LaGrandeur (page does not exist)\">Kevin LaGrandeur</a> (2011). <a class=\"external text\" href=\"https://www.academia.edu/704751\" rel=\"nofollow\">\"The Persistent Peril of the Artificial Slave\"</a>. <i>Science Fiction Studies</i>. <b>38</b> (2): 232. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.5621%2Fsciefictstud.38.2.0232\" rel=\"nofollow\">10.5621/sciefictstud.38.2.0232</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2013-05-06</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science+Fiction+Studies&amp;rft.atitle=The+Persistent+Peril+of+the+Artificial+Slave&amp;rft.volume=38&amp;rft.issue=2&amp;rft.pages=232&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.5621%2Fsciefictstud.38.2.0232&amp;rft.au=Kevin+LaGrandeur&amp;rft_id=https%3A%2F%2Fwww.academia.edu%2F704751&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFIsaac_Asimov1964\">Isaac Asimov (1964). <span class=\"cs1-lock-registration\" title=\"Free registration required\"><a class=\"external text\" href=\"https://archive.org/details/restofrobots00asim\" rel=\"nofollow\">\"Introduction\"</a></span>. <i>The Rest of the Robots</i>. Doubleday. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/0-385-09041-2\" title=\"Special:BookSources/0-385-09041-2\"><bdi>0-385-09041-2</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Introduction&amp;rft.btitle=The+Rest+of+the+Robots&amp;rft.pub=Doubleday&amp;rft.date=1964&amp;rft.isbn=0-385-09041-2&amp;rft.au=Isaac+Asimov&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Frestofrobots00asim&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFEliezer_Yudkowsky2008\"><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a> (2008). <a class=\"external text\" href=\"http://intelligence.org/files/AIPosNegFactor.pdf\" rel=\"nofollow\">\"Artificial Intelligence as a Positive and Negative Factor in Global Risk\"</a> <span class=\"cs1-format\">(PDF)</span>.  In Nick Bostrom; Milan M. Ćirković (eds.). <i>Global Catastrophic Risks</i>. pp. 308–345.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Artificial+Intelligence+as+a+Positive+and+Negative+Factor+in+Global+Risk&amp;rft.btitle=Global+Catastrophic+Risks&amp;rft.pages=308-345&amp;rft.date=2008&amp;rft.au=Eliezer+Yudkowsky&amp;rft_id=http%3A%2F%2Fintelligence.org%2Ffiles%2FAIPosNegFactor.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFOmohundro2008\">Omohundro, S. M. (February 2008). \"The basic AI drives\". <i>Artificial General Intelligence</i>. <b>171</b>: 483–492. <a class=\"mw-redirect\" href=\"/wiki/CiteSeerX_(identifier)\" title=\"CiteSeerX (identifier)\">CiteSeerX</a> <span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.393.8356\" rel=\"nofollow\">10.1.1.393.8356</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Artificial+General+Intelligence&amp;rft.atitle=The+basic+AI+drives&amp;rft.volume=171&amp;rft.pages=483-492&amp;rft.date=2008-02&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.393.8356%23id-name%3DCiteSeerX&amp;rft.aulast=Omohundro&amp;rft.aufirst=S.+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFBostrom2014\">Bostrom, Nick (2014). \"Chapter 7: The Superintelligent Will\". <a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\"><i>Superintelligence: Paths, Dangers, Strategies</i></a>. Oxford: Oxford University Press. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/9780199678112\" title=\"Special:BookSources/9780199678112\"><bdi>9780199678112</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+7%3A+The+Superintelligent+Will&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.isbn=9780199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFDvorsky2013\">Dvorsky, George (2013-04-26). <a class=\"external text\" href=\"https://gizmodo.com/how-skynet-might-emerge-from-simple-physics-482402911\" rel=\"nofollow\">\"How Skynet Might Emerge From Simple Physics\"</a>. <i>Gizmodo</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Gizmodo&amp;rft.atitle=How+Skynet+Might+Emerge+From+Simple+Physics&amp;rft.date=2013-04-26&amp;rft.aulast=Dvorsky&amp;rft.aufirst=George&amp;rft_id=https%3A%2F%2Fgizmodo.com%2Fhow-skynet-might-emerge-from-simple-physics-482402911&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFWissner-GrossFreer2013\"><a href=\"/wiki/Alexander_Wissner-Gross\" title=\"Alexander Wissner-Gross\">Wissner-Gross, A. D.</a>; <a class=\"new\" href=\"/w/index.php?title=Cameron_Freer&amp;action=edit&amp;redlink=1\" title=\"Cameron Freer (page does not exist)\">Freer, C. E.</a> (2013). <a class=\"external text\" href=\"https://doi.org/10.1103%2FPhysRevLett.110.168702\" rel=\"nofollow\">\"Causal entropic forces\"</a>. <i>Physical Review Letters</i>. <b>110</b> (16): 168702. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2013PhRvL.110p8702W\" rel=\"nofollow\">2013PhRvL.110p8702W</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.1103%2FPhysRevLett.110.168702\" rel=\"nofollow\">10.1103/PhysRevLett.110.168702</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/23679649\" rel=\"nofollow\">23679649</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physical+Review+Letters&amp;rft.atitle=Causal+entropic+forces&amp;rft.volume=110&amp;rft.issue=16&amp;rft.pages=168702&amp;rft.date=2013&amp;rft_id=info%3Apmid%2F23679649&amp;rft_id=info%3Adoi%2F10.1103%2FPhysRevLett.110.168702&amp;rft_id=info%3Abibcode%2F2013PhRvL.110p8702W&amp;rft.aulast=Wissner-Gross&amp;rft.aufirst=A.+D.&amp;rft.au=Freer%2C+C.+E.&amp;rft_id=%2F%2Fdoi.org%2F10.1103%252FPhysRevLett.110.168702&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-MuehlhauserSecurity2013-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-MuehlhauserSecurity2013_12-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMuehlhauser2013\">Muehlhauser, Luke (31 Jul 2013). <a class=\"external text\" href=\"http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/\" rel=\"nofollow\">\"AI Risk and the Security Mindset\"</a>. <i>Machine Intelligence Research Institute</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">15 July</span> 2014</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Machine+Intelligence+Research+Institute&amp;rft.atitle=AI+Risk+and+the+Security+Mindset&amp;rft.date=2013-07-31&amp;rft.aulast=Muehlhauser&amp;rft.aufirst=Luke&amp;rft_id=http%3A%2F%2Fintelligence.org%2F2013%2F07%2F31%2Fai-risk-and-the-security-mindset%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-think13-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-think13_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-think13_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFMuehlhauserBostrom2013\">Muehlhauser, Luke; Bostrom, Nick (2013-12-17). \"Why We Need Friendly AI\". <i>Think</i>. <b>13</b> (36): 41–47. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1017%2Fs1477175613000316\" rel=\"nofollow\">10.1017/s1477175613000316</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1477-1756\" rel=\"nofollow\">1477-1756</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:143657841\" rel=\"nofollow\">143657841</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Think&amp;rft.atitle=Why+We+Need+Friendly+AI&amp;rft.volume=13&amp;rft.issue=36&amp;rft.pages=41-47&amp;rft.date=2013-12-17&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A143657841%23id-name%3DS2CID&amp;rft.issn=1477-1756&amp;rft_id=info%3Adoi%2F10.1017%2Fs1477175613000316&amp;rft.aulast=Muehlhauser&amp;rft.aufirst=Luke&amp;rft.au=Bostrom%2C+Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-boyles2019-14\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-boyles2019_14-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-boyles2019_14-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFBoylesJoaquin2019\">Boyles, Robert James M.; Joaquin, Jeremiah Joven (2019-07-23). \"Why friendly AIs won't be that friendly: a friendly reply to Muehlhauser and Bostrom\". <i>AI &amp; Society</i>. <b>35</b> (2): 505–507. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs00146-019-00903-0\" rel=\"nofollow\">10.1007/s00146-019-00903-0</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0951-5666\" rel=\"nofollow\">0951-5666</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:198190745\" rel=\"nofollow\">198190745</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+%26+Society&amp;rft.atitle=Why+friendly+AIs+won%27t+be+that+friendly%3A+a+friendly+reply+to+Muehlhauser+and+Bostrom&amp;rft.volume=35&amp;rft.issue=2&amp;rft.pages=505-507&amp;rft.date=2019-07-23&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A198190745%23id-name%3DS2CID&amp;rft.issn=0951-5666&amp;rft_id=info%3Adoi%2F10.1007%2Fs00146-019-00903-0&amp;rft.aulast=Boyles&amp;rft.aufirst=Robert+James+M.&amp;rft.au=Joaquin%2C+Jeremiah+Joven&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFChan2020\">Chan, Berman (2020-03-04). \"The rise of artificial intelligence and the crisis of moral passivity\". <i>AI &amp; Society</i>. <b>35</b> (4): 991–993. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs00146-020-00953-9\" rel=\"nofollow\">10.1007/s00146-020-00953-9</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1435-5655\" rel=\"nofollow\">1435-5655</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:212407078\" rel=\"nofollow\">212407078</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+%26+Society&amp;rft.atitle=The+rise+of+artificial+intelligence+and+the+crisis+of+moral+passivity&amp;rft.volume=35&amp;rft.issue=4&amp;rft.pages=991-993&amp;rft.date=2020-03-04&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A212407078%23id-name%3DS2CID&amp;rft.issn=1435-5655&amp;rft_id=info%3Adoi%2F10.1007%2Fs00146-020-00953-9&amp;rft.aulast=Chan&amp;rft.aufirst=Berman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-cevpaper-16\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-cevpaper_16-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-cevpaper_16-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFEliezer_Yudkowsky2004\">Eliezer Yudkowsky (2004). <a class=\"external text\" href=\"https://intelligence.org/files/CEV.pdf\" rel=\"nofollow\">\"Coherent Extrapolated Volition\"</a> <span class=\"cs1-format\">(PDF)</span>. Singularity Institute for Artificial Intelligence<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2015-09-12</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Coherent+Extrapolated+Volition&amp;rft.pub=Singularity+Institute+for+Artificial+Intelligence&amp;rft.date=2004&amp;rft.au=Eliezer+Yudkowsky&amp;rft_id=https%3A%2F%2Fintelligence.org%2Ffiles%2FCEV.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-Hendry2014-17\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Hendry2014_17-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Hendry2014_17-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFHendry2014\">Hendry, Erica R. (21 Jan 2014). <a class=\"external text\" href=\"http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/\" rel=\"nofollow\">\"What Happens When Artificial Intelligence Turns On Us?\"</a>. <i>Smithsonian Magazine</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">15 July</span> 2014</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Smithsonian+Magazine&amp;rft.atitle=What+Happens+When+Artificial+Intelligence+Turns+On+Us%3F&amp;rft.date=2014-01-21&amp;rft.aulast=Hendry&amp;rft.aufirst=Erica+R.&amp;rft_id=http%3A%2F%2Fwww.smithsonianmag.com%2Finnovation%2Fwhat-happens-when-artificial-intelligence-turns-us-180949415%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFBaum2016\">Baum, Seth D. (2016-09-28). \"On the promotion of safe and socially beneficial artificial intelligence\". <i>AI &amp; Society</i>. <b>32</b> (4): 543–551. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs00146-016-0677-0\" rel=\"nofollow\">10.1007/s00146-016-0677-0</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0951-5666\" rel=\"nofollow\">0951-5666</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:29012168\" rel=\"nofollow\">29012168</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+%26+Society&amp;rft.atitle=On+the+promotion+of+safe+and+socially+beneficial+artificial+intelligence&amp;rft.volume=32&amp;rft.issue=4&amp;rft.pages=543-551&amp;rft.date=2016-09-28&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A29012168%23id-name%3DS2CID&amp;rft.issn=0951-5666&amp;rft_id=info%3Adoi%2F10.1007%2Fs00146-016-0677-0&amp;rft.aulast=Baum&amp;rft.aufirst=Seth+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-HC-19\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-HC_19-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-HC_19-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-HC_19-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-HC_19-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRussell2019\"><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Russell, Stuart</a> (October 8, 2019). <span class=\"cs1-lock-registration\" title=\"Free registration required\"><a class=\"external text\" href=\"https://archive.org/details/humancompatiblea0000russ\" rel=\"nofollow\"><i>Human Compatible: Artificial Intelligence and the Problem of Control</i></a></span>. United States: Viking. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-525-55861-3\" title=\"Special:BookSources/978-0-525-55861-3\"><bdi>978-0-525-55861-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1083694322\" rel=\"nofollow\">1083694322</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+Compatible%3A+Artificial+Intelligence+and+the+Problem+of+Control&amp;rft.place=United+States&amp;rft.pub=Viking&amp;rft.date=2019-10-08&amp;rft_id=info%3Aoclcnum%2F1083694322&amp;rft.isbn=978-0-525-55861-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fhumancompatiblea0000russ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-McGinnis2010-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-McGinnis2010_20-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFMcGinnis2010\">McGinnis, John O. (Summer 2010). <a class=\"external text\" href=\"http://www.law.northwestern.edu/LAWREVIEW/Colloquy/2010/12/\" rel=\"nofollow\">\"Accelerating AI\"</a>. <i>Northwestern University Law Review</i>. <b>104</b> (3): 1253–1270<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 July</span> 2014</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Northwestern+University+Law+Review&amp;rft.atitle=Accelerating+AI&amp;rft.ssn=summer&amp;rft.volume=104&amp;rft.issue=3&amp;rft.pages=1253-1270&amp;rft.date=2010&amp;rft.aulast=McGinnis&amp;rft.aufirst=John+O.&amp;rft_id=http%3A%2F%2Fwww.law.northwestern.edu%2FLAWREVIEW%2FColloquy%2F2010%2F12%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation magazine cs1\" id=\"CITEREFMarcus2012\">Marcus, Gary (24 November 2012). <a class=\"external text\" href=\"http://www.newyorker.com/news/news-desk/moral-machines\" rel=\"nofollow\">\"Moral Machines\"</a>. <i><a href=\"/wiki/The_New_Yorker\" title=\"The New Yorker\">The New Yorker</a></i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">30 July</span> 2014</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Yorker&amp;rft.atitle=Moral+Machines&amp;rft.date=2012-11-24&amp;rft.aulast=Marcus&amp;rft.aufirst=Gary&amp;rft_id=http%3A%2F%2Fwww.newyorker.com%2Fnews%2Fnews-desk%2Fmoral-machines&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFWinfield2014\">Winfield, Alan (9 August 2014). <a class=\"external text\" href=\"https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield\" rel=\"nofollow\">\"Artificial intelligence will not turn into a Frankenstein's monster\"</a>. <i><a href=\"/wiki/The_Guardian\" title=\"The Guardian\">The Guardian</a></i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">17 September</span> 2014</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Artificial+intelligence+will+not+turn+into+a+Frankenstein%27s+monster&amp;rft.date=2014-08-09&amp;rft.aulast=Winfield&amp;rft.aufirst=Alan&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2014%2Faug%2F10%2Fartificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKornai2014\">Kornai, András (2014-05-15). \"Bounding the impact of AGI\". <i>Journal of Experimental &amp; Theoretical Artificial Intelligence</i>. Informa UK Limited. <b>26</b> (3): 417–438. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1080%2F0952813x.2014.895109\" rel=\"nofollow\">10.1080/0952813x.2014.895109</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0952-813X\" rel=\"nofollow\">0952-813X</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:7067517\" rel=\"nofollow\">7067517</a>. <q>...the essence of AGIs is their reasoning facilities, and it is the very logic of their being that will compel them to behave in a moral fashion... The real nightmare scenario (is one where) humans find it advantageous to strongly couple themselves to AGIs, with no guarantees against self-deception.</q></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+%26+Theoretical+Artificial+Intelligence&amp;rft.atitle=Bounding+the+impact+of+AGI&amp;rft.volume=26&amp;rft.issue=3&amp;rft.pages=417-438&amp;rft.date=2014-05-15&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7067517%23id-name%3DS2CID&amp;rft.issn=0952-813X&amp;rft_id=info%3Adoi%2F10.1080%2F0952813x.2014.895109&amp;rft.aulast=Kornai&amp;rft.aufirst=Andr%C3%A1s&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation magazine cs1\" id=\"CITEREFKeiperSchulman2011\">Keiper, Adam; Schulman, Ari N. (Summer 2011). <a class=\"external text\" href=\"http://www.thenewatlantis.com/publications/the-problem-with-friendly-artificial-intelligence\" rel=\"nofollow\">\"The Problem with 'Friendly' Artificial Intelligence\"</a>. <i>The New Atlantis</i>. No. 32. p. 80-89<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2012-01-16</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Atlantis&amp;rft.atitle=The+Problem+with+%27Friendly%27+Artificial+Intelligence&amp;rft.ssn=summer&amp;rft.issue=32&amp;rft.pages=80-89&amp;rft.date=2011&amp;rft.aulast=Keiper&amp;rft.aufirst=Adam&amp;rft.au=Schulman%2C+Ari+N.&amp;rft_id=http%3A%2F%2Fwww.thenewatlantis.com%2Fpublications%2Fthe-problem-with-friendly-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence\"></span></span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"Further_reading\">Further reading</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=9\" title=\"Edit section: Further reading\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li>Yudkowsky, E. <a class=\"external text\" href=\"http://intelligence.org/files/AIPosNegFactor.pdf\" rel=\"nofollow\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a>. In <i>Global Catastrophic Risks</i>, Oxford University Press, 2008.<br/>Discusses Artificial Intelligence from the perspective of <a class=\"mw-redirect\" href=\"/wiki/Existential_risk\" title=\"Existential risk\">Existential risk</a>.  In particular, Sections 1-4 give background to the definition of Friendly AI in Section 5.  Section 6 gives two classes of mistakes (technical and philosophical) which would both lead to the accidental creation of non-Friendly AIs.  Sections 7-13 discuss further related issues.</li>\n<li>Omohundro, S.  2008 The Basic AI Drives Appeared in AGI-08 - Proceedings of the First Conference on Artificial General Intelligence</li>\n<li>Mason, C. 2008 <a class=\"external text\" href=\"https://aaai.org/Papers/Workshops/2008/WS-08-07/WS08-07-023.pdf\" rel=\"nofollow\">Human-Level AI Requires Compassionate Intelligence</a> Appears in <a class=\"mw-redirect\" href=\"/wiki/AAAI\" title=\"AAAI\">AAAI</a> 2008 Workshop on Meta-Reasoning:Thinking About Thinking</li>\n<li>Froding, B. and Peterson, M 2021 <a class=\"external text\" href=\"https://link.springer.com/article/10.1007/s10676-020-09556-w\" rel=\"nofollow\">Friendly AI</a> Ethics and Information Technology volume 23, pp 207–214.</li></ul>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=10\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a class=\"external text\" href=\"https://nickbostrom.com/ethics/ai\" rel=\"nofollow\">Ethical Issues in Advanced Artificial Intelligence</a> by Nick Bostrom</li>\n<li><a class=\"external text\" href=\"https://intelligence.org/ie-faq/#WhatIsFriendlyAI\" rel=\"nofollow\">What is Friendly AI?</a> — A brief description of Friendly AI by the Machine Intelligence Research Institute.</li>\n<li><a class=\"external text\" href=\"https://intelligence.org/files/CFAI.pdf\" rel=\"nofollow\">Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures</a> — A near book-length description from the MIRI</li>\n<li><a class=\"external text\" href=\"http://www.ssec.wisc.edu/~billh/g/SIAI_critique.html\" rel=\"nofollow\">Critique of the MIRI Guidelines on Friendly AI</a> — by <a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a></li>\n<li><a class=\"external text\" href=\"http://www.optimal.org/peter/siai_guidelines.htm\" rel=\"nofollow\">Commentary on MIRI's Guidelines on Friendly AI</a> — by Peter Voss.</li>\n<li><a class=\"external text\" href=\"https://www.thenewatlantis.com/publications/the-problem-with-friendly-artificial-intelligence\" rel=\"nofollow\">The Problem with ‘Friendly’ Artificial Intelligence</a> — On the motives for and impossibility of FAI; by Adam Keiper and Ari N. Schulman.</li></ul>\n<div class=\"navbox-styles nomobile\"><style data-mw-deduplicate=\"TemplateStyles:r1061467846\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div aria-labelledby=\"Existential_risk_from_artificial_intelligence\" class=\"navbox\" role=\"navigation\" style=\"padding:3px\"><table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th class=\"navbox-title\" colspan=\"2\" scope=\"col\"><link href=\"mw-data:TemplateStyles:r1063604349\" rel=\"mw-deduplicated-inline-style\"/><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Existential_risk_from_artificial_intelligence\" title=\"Template:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Existential_risk_from_artificial_intelligence\" title=\"Template talk:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Existential_risk_from_artificial_intelligence\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk</a> from <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a></div></th></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Concepts</th><td class=\"navbox-list-with-group navbox-list navbox-odd hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/AI_alignment\" title=\"AI alignment\">AI alignment</a></li>\n<li><a href=\"/wiki/AI_capability_control\" title=\"AI capability control\">AI capability control</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/Accelerating_change\" title=\"Accelerating change\">Accelerating change</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a class=\"mw-selflink selflink\">Friendly artificial intelligence</a></li>\n<li><a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">Instrumental convergence</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">Intelligence explosion</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">Superintelligence</a></li>\n<li><a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">Technological singularity</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Organizations</th><td class=\"navbox-list-with-group navbox-list navbox-even hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Allen_Institute_for_AI\" title=\"Allen Institute for AI\">Allen Institute for AI</a></li>\n<li><a href=\"/wiki/Center_for_Applied_Rationality\" title=\"Center for Applied Rationality\">Center for Applied Rationality</a></li>\n<li><a href=\"/wiki/Center_for_Human-Compatible_Artificial_Intelligence\" title=\"Center for Human-Compatible Artificial Intelligence\">Center for Human-Compatible Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Centre_for_the_Study_of_Existential_Risk\" title=\"Centre for the Study of Existential Risk\">Centre for the Study of Existential Risk</a></li>\n<li><a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></li>\n<li><a href=\"/wiki/Foundational_Questions_Institute\" title=\"Foundational Questions Institute\">Foundational Questions Institute</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a href=\"/wiki/Future_of_Life_Institute\" title=\"Future of Life Institute\">Future of Life Institute</a></li>\n<li><a href=\"/wiki/Humanity%2B\" title=\"Humanity+\">Humanity+</a></li>\n<li><a href=\"/wiki/Institute_for_Ethics_and_Emerging_Technologies\" title=\"Institute for Ethics and Emerging Technologies\">Institute for Ethics and Emerging Technologies</a></li>\n<li><a href=\"/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence\" title=\"Leverhulme Centre for the Future of Intelligence\">Leverhulme Centre for the Future of Intelligence</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">People</th><td class=\"navbox-list-with-group navbox-list navbox-odd hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Slate_Star_Codex\" title=\"Slate Star Codex\">Scott Alexander</a></li>\n<li><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a></li>\n<li><a href=\"/wiki/K._Eric_Drexler\" title=\"K. Eric Drexler\">Eric Drexler</a></li>\n<li><a href=\"/wiki/Sam_Harris\" title=\"Sam Harris\">Sam Harris</a></li>\n<li><a href=\"/wiki/Stephen_Hawking\" title=\"Stephen Hawking\">Stephen Hawking</a></li>\n<li><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a></li>\n<li><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Bill Joy</a></li>\n<li><a href=\"/wiki/Elon_Musk\" title=\"Elon Musk\">Elon Musk</a></li>\n<li><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a></li>\n<li><a href=\"/wiki/Huw_Price\" title=\"Huw Price\">Huw Price</a></li>\n<li><a href=\"/wiki/Martin_Rees\" title=\"Martin Rees\">Martin Rees</a></li>\n<li><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a></li>\n<li><a href=\"/wiki/Jaan_Tallinn\" title=\"Jaan Tallinn\">Jaan Tallinn</a></li>\n<li><a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Max Tegmark</a></li>\n<li><a href=\"/wiki/Frank_Wilczek\" title=\"Frank Wilczek\">Frank Wilczek</a></li>\n<li><a href=\"/wiki/Roman_Yampolskiy\" title=\"Roman Yampolskiy\">Roman Yampolskiy</a></li>\n<li><a href=\"/wiki/Andrew_Yang\" title=\"Andrew Yang\">Andrew Yang</a></li>\n<li><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Other</th><td class=\"navbox-list-with-group navbox-list navbox-even hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Global_catastrophic_risk#Artificial_intelligence\" title=\"Global catastrophic risk\">Artificial intelligence as a global catastrophic risk</a></li>\n<li><a href=\"/wiki/Artificial_general_intelligence#Controversies_and_dangers\" title=\"Artificial general intelligence\">Controversies and dangers of artificial general intelligence</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Suffering_risks\" title=\"Suffering risks\">Suffering risks</a></li>\n<li><i><a href=\"/wiki/Human_Compatible\" title=\"Human Compatible\">Human Compatible</a></i></li>\n<li><a href=\"/wiki/Open_Letter_on_Artificial_Intelligence\" title=\"Open Letter on Artificial Intelligence\">Open Letter on Artificial Intelligence</a></li>\n<li><i><a href=\"/wiki/Our_Final_Invention\" title=\"Our Final Invention\">Our Final Invention</a></i></li>\n<li><i><a href=\"/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\" title=\"The Precipice: Existential Risk and the Future of Humanity\">The Precipice</a></i></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li>\n<li><i><a href=\"/wiki/Do_You_Trust_This_Computer%3F\" title=\"Do You Trust This Computer?\">Do You Trust This Computer?</a></i></li></ul>\n</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div><img alt=\"\" class=\"noviewer\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x\" title=\"Category\" width=\"16\"/> <a href=\"/wiki/Category:Existential_risk_from_artificial_general_intelligence\" title=\"Category:Existential risk from artificial general intelligence\">Category</a></div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw2314\nCached time: 20221112123330\nCache expiry: 1814400\nReduced expiry: false\nComplications: [vary‐revision‐sha1, show‐toc]\nCPU time usage: 0.373 seconds\nReal time usage: 0.498 seconds\nPreprocessor visited node count: 2751/1000000\nPost‐expand include size: 76797/2097152 bytes\nTemplate argument size: 2978/2097152 bytes\nHighest expansion depth: 16/100\nExpensive parser function count: 3/500\nUnstrip recursion depth: 1/20\nUnstrip post‐expand size: 83829/5000000 bytes\nLua time usage: 0.228/10.000 seconds\nLua memory usage: 7617728/52428800 bytes\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  436.979      1 -total\n 43.61%  190.569      1 Template:Reflist\n 22.41%   97.923      8 Template:Cite_book\n 13.25%   57.908      1 Template:Short_description\n 11.48%   50.147      1 Template:Artificial_intelligence\n 10.79%   47.171      1 Template:Sidebar_with_collapsible_lists\n  8.78%   38.351      4 Template:Rp\n  8.57%   37.463      9 Template:Cite_journal\n  8.42%   36.796      1 Template:Existential_risk_from_artificial_intelligence\n  7.99%   34.911      4 Template:R/superscript\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:351887-0!canonical and timestamp 20221112123330 and revision id 1121465029.\n -->\n</div><noscript><img alt=\"\" height=\"1\" src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" style=\"border: none; position: absolute;\" title=\"\" width=\"1\"/></noscript>\n<div class=\"printfooter\" data-nosnippet=\"\">Retrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&amp;oldid=1121465029\">https://en.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&amp;oldid=1121465029</a>\"</div></div>\n<div class=\"catlinks\" data-mw=\"interface\" id=\"catlinks\"><div class=\"mw-normal-catlinks\" id=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Philosophy_of_artificial_intelligence\" title=\"Category:Philosophy of artificial intelligence\">Philosophy of artificial intelligence</a></li><li><a href=\"/wiki/Category:Singularitarianism\" title=\"Category:Singularitarianism\">Singularitarianism</a></li><li><a href=\"/wiki/Category:Transhumanism\" title=\"Category:Transhumanism\">Transhumanism</a></li><li><a href=\"/wiki/Category:Affective_computing\" title=\"Category:Affective computing\">Affective computing</a></li></ul></div><div class=\"mw-hidden-catlinks mw-hidden-cats-hidden\" id=\"mw-hidden-catlinks\">Hidden categories: <ul><li><a href=\"/wiki/Category:Articles_with_short_description\" title=\"Category:Articles with short description\">Articles with short description</a></li><li><a href=\"/wiki/Category:Short_description_is_different_from_Wikidata\" title=\"Category:Short description is different from Wikidata\">Short description is different from Wikidata</a></li></ul></div></div>\n</div>\n</div>\n<div id=\"mw-navigation\">\n<h2>Navigation menu</h2>\n<div id=\"mw-head\">\n<nav aria-labelledby=\"p-personal-label\" class=\"vector-menu mw-portlet mw-portlet-personal vector-user-menu-legacy\" id=\"p-personal\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n<span class=\"vector-menu-heading-label\">Personal tools</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"pt-anonuserpage\"><span title=\"The user page for the IP address you are editing as\">Not logged in</span></li><li class=\"mw-list-item\" id=\"pt-anontalk\"><a accesskey=\"n\" href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\"><span>Talk</span></a></li><li class=\"mw-list-item\" id=\"pt-anoncontribs\"><a accesskey=\"y\" href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\"><span>Contributions</span></a></li><li class=\"mw-list-item\" id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Friendly+artificial+intelligence\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\"><span>Create account</span></a></li><li class=\"mw-list-item\" id=\"pt-login\"><a accesskey=\"o\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Friendly+artificial+intelligence\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\"><span>Log in</span></a></li></ul>\n</div>\n</nav>\n<div id=\"left-navigation\">\n<nav aria-labelledby=\"p-namespaces-label\" class=\"vector-menu mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy\" id=\"p-namespaces\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n<span class=\"vector-menu-heading-label\">Namespaces</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"selected mw-list-item\" id=\"ca-nstab-main\"><a accesskey=\"c\" href=\"/wiki/Friendly_artificial_intelligence\" title=\"View the content page [c]\"><span>Article</span></a></li><li class=\"mw-list-item\" id=\"ca-talk\"><a accesskey=\"t\" href=\"/wiki/Talk:Friendly_artificial_intelligence\" rel=\"discussion\" title=\"Discuss improvements to the content page [t]\"><span>Talk</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-variants-label\" class=\"vector-menu mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu-dropdown\" id=\"p-variants\" role=\"navigation\">\n<input aria-haspopup=\"true\" aria-labelledby=\"p-variants-label\" class=\"vector-menu-checkbox\" data-event-name=\"ui.dropdown-p-variants\" id=\"p-variants-checkbox\" role=\"button\" type=\"checkbox\"/>\n<label aria-label=\"Change language variant\" class=\"vector-menu-heading\" id=\"p-variants-label\">\n<span class=\"vector-menu-heading-label\">English</span>\n</label>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"></ul>\n</div>\n</nav>\n</div>\n<div id=\"right-navigation\">\n<nav aria-labelledby=\"p-views-label\" class=\"vector-menu mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy\" id=\"p-views\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n<span class=\"vector-menu-heading-label\">Views</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"selected mw-list-item\" id=\"ca-view\"><a href=\"/wiki/Friendly_artificial_intelligence\"><span>Read</span></a></li><li class=\"mw-list-item\" id=\"ca-edit\"><a accesskey=\"e\" href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit\" title=\"Edit this page [e]\"><span>Edit</span></a></li><li class=\"mw-list-item\" id=\"ca-history\"><a accesskey=\"h\" href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=history\" title=\"Past revisions of this page [h]\"><span>View history</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-cactions-label\" class=\"vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu-dropdown\" id=\"p-cactions\" role=\"navigation\" title=\"More options\">\n<input aria-haspopup=\"true\" aria-labelledby=\"p-cactions-label\" class=\"vector-menu-checkbox\" data-event-name=\"ui.dropdown-p-cactions\" id=\"p-cactions-checkbox\" role=\"button\" type=\"checkbox\"/>\n<label class=\"vector-menu-heading\" id=\"p-cactions-label\">\n<span class=\"vector-menu-heading-label\">More</span>\n</label>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"></ul>\n</div>\n</nav>\n<div class=\"vector-search-box-vue vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box\" id=\"p-search\" role=\"search\">\n<div>\n<h3>\n<label for=\"searchInput\">Search</label>\n</h3>\n<form action=\"/w/index.php\" class=\"vector-search-box-form\" id=\"searchform\">\n<div class=\"vector-search-box-inner\" data-search-loc=\"header-navigation\" id=\"simpleSearch\">\n<input accesskey=\"f\" aria-label=\"Search Wikipedia\" autocapitalize=\"sentences\" class=\"vector-search-box-input\" id=\"searchInput\" name=\"search\" placeholder=\"Search Wikipedia\" title=\"Search Wikipedia [f]\" type=\"search\"/>\n<input name=\"title\" type=\"hidden\" value=\"Special:Search\"/>\n<input class=\"searchButton mw-fallbackSearchButton\" id=\"mw-searchButton\" name=\"fulltext\" title=\"Search Wikipedia for this text\" type=\"submit\" value=\"Search\"/>\n<input class=\"searchButton\" id=\"searchButton\" name=\"go\" title=\"Go to a page with this exact name if it exists\" type=\"submit\" value=\"Go\"/>\n</div>\n</form>\n</div>\n</div>\n</div>\n</div>\n<div id=\"mw-panel\">\n<div id=\"p-logo\" role=\"banner\">\n<a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a>\n</div>\n<nav aria-labelledby=\"p-navigation-label\" class=\"vector-menu mw-portlet mw-portlet-navigation vector-menu-portal portal\" id=\"p-navigation\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n<span class=\"vector-menu-heading-label\">Navigation</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"n-mainpage-description\"><a accesskey=\"z\" href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\"><span>Main page</span></a></li><li class=\"mw-list-item\" id=\"n-contents\"><a href=\"/wiki/Wikipedia:Contents\" title=\"Guides to browsing Wikipedia\"><span>Contents</span></a></li><li class=\"mw-list-item\" id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Articles related to current events\"><span>Current events</span></a></li><li class=\"mw-list-item\" id=\"n-randompage\"><a accesskey=\"x\" href=\"/wiki/Special:Random\" title=\"Visit a randomly selected article [x]\"><span>Random article</span></a></li><li class=\"mw-list-item\" id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Learn about Wikipedia and how it works\"><span>About Wikipedia</span></a></li><li class=\"mw-list-item\" id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\"><span>Contact us</span></a></li><li class=\"mw-list-item\" id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us by donating to the Wikimedia Foundation\"><span>Donate</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-interaction-label\" class=\"vector-menu mw-portlet mw-portlet-interaction vector-menu-portal portal\" id=\"p-interaction\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n<span class=\"vector-menu-heading-label\">Contribute</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\"><span>Help</span></a></li><li class=\"mw-list-item\" id=\"n-introduction\"><a href=\"/wiki/Help:Introduction\" title=\"Learn how to edit Wikipedia\"><span>Learn to edit</span></a></li><li class=\"mw-list-item\" id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"The hub for editors\"><span>Community portal</span></a></li><li class=\"mw-list-item\" id=\"n-recentchanges\"><a accesskey=\"r\" href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes to Wikipedia [r]\"><span>Recent changes</span></a></li><li class=\"mw-list-item\" id=\"n-upload\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Add images or other media for use on Wikipedia\"><span>Upload file</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-tb-label\" class=\"vector-menu mw-portlet mw-portlet-tb vector-menu-portal portal\" id=\"p-tb\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n<span class=\"vector-menu-heading-label\">Tools</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"t-whatlinkshere\"><a accesskey=\"j\" href=\"/wiki/Special:WhatLinksHere/Friendly_artificial_intelligence\" title=\"List of all English Wikipedia pages containing links to this page [j]\"><span>What links here</span></a></li><li class=\"mw-list-item\" id=\"t-recentchangeslinked\"><a accesskey=\"k\" href=\"/wiki/Special:RecentChangesLinked/Friendly_artificial_intelligence\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\"><span>Related changes</span></a></li><li class=\"mw-list-item\" id=\"t-upload\"><a accesskey=\"u\" href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\"><span>Upload file</span></a></li><li class=\"mw-list-item\" id=\"t-specialpages\"><a accesskey=\"q\" href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\"><span>Special pages</span></a></li><li class=\"mw-list-item\" id=\"t-permalink\"><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;oldid=1121465029\" title=\"Permanent link to this revision of this page\"><span>Permanent link</span></a></li><li class=\"mw-list-item\" id=\"t-info\"><a href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;action=info\" title=\"More information about this page\"><span>Page information</span></a></li><li class=\"mw-list-item\" id=\"t-cite\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Friendly_artificial_intelligence&amp;id=1121465029&amp;wpFormIdentifier=titleform\" title=\"Information on how to cite this page\"><span>Cite this page</span></a></li><li class=\"mw-list-item\" id=\"t-wikibase\"><a accesskey=\"g\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q4168796\" title=\"Structured data on this page hosted by Wikidata [g]\"><span>Wikidata item</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-coll-print_export-label\" class=\"vector-menu mw-portlet mw-portlet-coll-print_export vector-menu-portal portal\" id=\"p-coll-print_export\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n<span class=\"vector-menu-heading-label\">Print/export</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"coll-download-as-rl\"><a href=\"/w/index.php?title=Special:DownloadAsPdf&amp;page=Friendly_artificial_intelligence&amp;action=show-download-screen\" title=\"Download this page as a PDF file\"><span>Download as PDF</span></a></li><li class=\"mw-list-item\" id=\"t-print\"><a accesskey=\"p\" href=\"/w/index.php?title=Friendly_artificial_intelligence&amp;printable=yes\" title=\"Printable version of this page [p]\"><span>Printable version</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-lang-label\" class=\"vector-menu mw-portlet mw-portlet-lang vector-menu-portal portal\" id=\"p-lang\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n<span class=\"vector-menu-heading-label\">Languages</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"interlanguage-link interwiki-ar mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A_%D8%A7%D9%84%D8%B5%D8%AF%D9%8A%D9%82\" hreflang=\"ar\" lang=\"ar\" title=\"الذكاء الاصطناعي الصديق – Arabic\"><span>العربية</span></a></li><li class=\"interlanguage-link interwiki-es mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://es.wikipedia.org/wiki/Inteligencia_artificial_amigable\" hreflang=\"es\" lang=\"es\" title=\"Inteligencia artificial amigable – Spanish\"><span>Español</span></a></li><li class=\"interlanguage-link interwiki-fa mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://fa.wikipedia.org/wiki/%D9%87%D9%88%D8%B4_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C_%D8%AF%D9%88%D8%B3%D8%AA%D8%A7%D9%86%D9%87\" hreflang=\"fa\" lang=\"fa\" title=\"هوش مصنوعی دوستانه – Persian\"><span>فارسی</span></a></li><li class=\"interlanguage-link interwiki-fr mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://fr.wikipedia.org/wiki/Intelligence_artificielle_amicale\" hreflang=\"fr\" lang=\"fr\" title=\"Intelligence artificielle amicale – French\"><span>Français</span></a></li><li class=\"interlanguage-link interwiki-hy mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://hy.wikipedia.org/wiki/%D4%B8%D5%B6%D5%AF%D5%A5%D6%80%D5%A1%D5%AF%D5%A1%D5%B6_%D5%A1%D6%80%D5%B0%D5%A5%D5%BD%D5%BF%D5%A1%D5%AF%D5%A1%D5%B6_%D5%A2%D5%A1%D5%B6%D5%A1%D5%AF%D5%A1%D5%B6%D5%B8%D6%82%D5%A9%D5%B5%D5%B8%D6%82%D5%B6\" hreflang=\"hy\" lang=\"hy\" title=\"Ընկերական արհեստական բանականություն – Armenian\"><span>Հայերեն</span></a></li><li class=\"interlanguage-link interwiki-ja mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ja.wikipedia.org/wiki/Friendly_artificial_intelligence\" hreflang=\"ja\" lang=\"ja\" title=\"Friendly artificial intelligence – Japanese\"><span>日本語</span></a></li><li class=\"interlanguage-link interwiki-ru mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ru.wikipedia.org/wiki/%D0%94%D1%80%D1%83%D0%B6%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82\" hreflang=\"ru\" lang=\"ru\" title=\"Дружественный искусственный интеллект – Russian\"><span>Русский</span></a></li><li class=\"interlanguage-link interwiki-sv mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://sv.wikipedia.org/wiki/V%C3%A4nlig_artificiell_intelligens\" hreflang=\"sv\" lang=\"sv\" title=\"Vänlig artificiell intelligens – Swedish\"><span>Svenska</span></a></li><li class=\"interlanguage-link interwiki-uk mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://uk.wikipedia.org/wiki/%D0%94%D1%80%D1%83%D0%B6%D0%BD%D1%96%D0%B9_%D1%88%D1%82%D1%83%D1%87%D0%BD%D0%B8%D0%B9_%D1%96%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82\" hreflang=\"uk\" lang=\"uk\" title=\"Дружній штучний інтелект – Ukrainian\"><span>Українська</span></a></li><li class=\"interlanguage-link interwiki-zh-yue mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://zh-yue.wikipedia.org/wiki/%E5%8F%8B%E5%A5%BD%E5%98%85%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD\" hreflang=\"yue\" lang=\"yue\" title=\"友好嘅人工智能 – Cantonese\"><span>粵語</span></a></li></ul>\n<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-edit wb-langlinks-link\"><a class=\"wbc-editpage\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q4168796#sitelinks-wikipedia\" title=\"Edit interlanguage links\">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class=\"mw-footer\" id=\"footer\" role=\"contentinfo\">\n<ul id=\"footer-info\">\n<li id=\"footer-info-lastmod\"> This page was last edited on 12 November 2022, at 12:33<span class=\"anonymous-show\"> (UTC)</span>.</li>\n<li id=\"footer-info-copyright\">Text is available under the <a href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\" rel=\"license\">Creative Commons Attribution-ShareAlike License 3.0</a><a href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\" style=\"display:none;\"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id=\"footer-places\">\n<li id=\"footer-places-privacy\"><a href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\">Privacy policy</a></li>\n<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\">About Wikipedia</a></li>\n<li id=\"footer-places-disclaimers\"><a href=\"/wiki/Wikipedia:General_disclaimer\">Disclaimers</a></li>\n<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n<li id=\"footer-places-mobileview\"><a class=\"noprint stopMobileRedirectToggle\" href=\"//en.m.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&amp;mobileaction=toggle_view_mobile\">Mobile view</a></li>\n<li id=\"footer-places-developers\"><a href=\"https://developer.wikimedia.org\">Developers</a></li>\n<li id=\"footer-places-statslink\"><a href=\"https://stats.wikimedia.org/#/en.wikipedia.org\">Statistics</a></li>\n<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\n</ul>\n<ul class=\"noprint\" id=\"footer-icons\">\n<li id=\"footer-copyrightico\"><a href=\"https://wikimediafoundation.org/\"><img alt=\"Wikimedia Foundation\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/wikimedia-button.png\" srcset=\"/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x\" width=\"88\"/></a></li>\n<li id=\"footer-poweredbyico\"><a href=\"https://www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a></li>\n</ul>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.373\",\"walltime\":\"0.498\",\"ppvisitednodes\":{\"value\":2751,\"limit\":1000000},\"postexpandincludesize\":{\"value\":76797,\"limit\":2097152},\"templateargumentsize\":{\"value\":2978,\"limit\":2097152},\"expansiondepth\":{\"value\":16,\"limit\":100},\"expensivefunctioncount\":{\"value\":3,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":83829,\"limit\":5000000},\"entityaccesscount\":{\"value\":0,\"limit\":400},\"timingprofile\":[\"100.00%  436.979      1 -total\",\" 43.61%  190.569      1 Template:Reflist\",\" 22.41%   97.923      8 Template:Cite_book\",\" 13.25%   57.908      1 Template:Short_description\",\" 11.48%   50.147      1 Template:Artificial_intelligence\",\" 10.79%   47.171      1 Template:Sidebar_with_collapsible_lists\",\"  8.78%   38.351      4 Template:Rp\",\"  8.57%   37.463      9 Template:Cite_journal\",\"  8.42%   36.796      1 Template:Existential_risk_from_artificial_intelligence\",\"  7.99%   34.911      4 Template:R/superscript\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.228\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":7617728,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw2314\",\"timestamp\":\"20221112123330\",\"ttl\":1814400,\"transientcontent\":false}}});});</script>\n<script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"Friendly artificial intelligence\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/Friendly_artificial_intelligence\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q4168796\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q4168796\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2003-10-28T23:55:53Z\",\"dateModified\":\"2022-11-12T12:33:27Z\",\"headline\":\"hypothetical artificial general intelligence that would have a positive effect on humanity\"}</script><script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"Friendly artificial intelligence\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/Friendly_artificial_intelligence\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q4168796\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q4168796\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2003-10-28T23:55:53Z\",\"dateModified\":\"2022-11-12T12:33:27Z\",\"headline\":\"hypothetical artificial general intelligence that would have a positive effect on humanity\"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgBackendResponseTime\":93,\"wgHostname\":\"mw2333\"});});</script>\n</body>\n</html>",
    "table_of_contents": [
        "1 Etymology and usage",
        "2 Risks of unfriendly AI",
        "3 Coherent extrapolated volition",
        "4 Other approaches",
        "5 Public policy",
        "6 Criticism",
        "7 See also",
        "8 References",
        "9 Further reading",
        "10 External links"
    ],
    "graphics": [
        {
            "url": "/wiki/File:Eliezer_Yudkowsky,_Stanford_2006_(square_crop).jpg",
            "caption": "Eliezer Yudkowsky, AI researcher and creator of the term Friendly artificial intelligence"
        }
    ],
    "paragraphs": [
        {
            "title": "",
            "text": "Friendly artificial intelligence (also friendly AI or FAI) refers to hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to foster the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.\n\n"
        },
        {
            "title": "Etymology and usage",
            "text": "The term was coined by Eliezer Yudkowsky,[1] who is best known for popularizing the idea,[2][3] to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach, describes the idea:[2]\n\n'Friendly' is used in this context as technical terminology, and picks out agents that are safe and useful, not necessarily ones that are \"friendly\" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly explode in intelligence, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.[4]\n\n"
        },
        {
            "title": "Risks of unfriendly AI",
            "text": "The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem, or the proto-robots of Gerbert of Aurillac and Roger Bacon.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.[5] By 1942 these themes prompted Isaac Asimov to create the \"Three Laws of Robotics\"—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.[6]\n\nIn modern times as the prospect of superintelligent AI looms nearer, philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity.  He put it this way:\n\nIn 2008 Eliezer Yudkowsky called for the creation of \"friendly AI\" to mitigate existential risk from advanced artificial intelligence. He explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"[7]\n\nSteve Omohundro says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of basic \"drives\", such as resource acquisition, self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, \"without special precautions\", cause the AI to exhibit undesired behavior.[8][9]\n\nAlexander Wissner-Gross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.[10][11]\n\nLuke Muehlhauser, writing for the Machine Intelligence Research Institute, recommends that machine ethics researchers adopt what Bruce Schneier has called the \"security mindset\": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.[12]\n\nIn 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI';[13] nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.[14][15]\n\n"
        },
        {
            "title": "Coherent extrapolated volition",
            "text": "Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, coherent extrapolated volition is people's choices and the actions people would collectively take if \"we knew more, thought faster, were more the people we wished we were, and had grown up closer together.\"[16]\n\nRather than a Friendly AI being designed directly by human programmers, it is to be designed by a \"seed AI\" programmed to first study human nature and then produce the AI which humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.[16] The appeal to an objective through contingent human nature (perhaps expressed, for mathematical purposes, in the form of a utility function or other decision-theoretic formalism), as providing the ultimate criterion of \"Friendliness\", is an answer to the meta-ethical problem of defining an objective morality; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.\n\n"
        },
        {
            "title": "Other approaches",
            "text": "Steve Omohundro has proposed a \"scaffolding\" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.[17]\n\nSeth Baum argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities, and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, \"existing messages about beneficial AI are not always framed well\". Baum advocates for \"cooperative relationships, and positive framing of AI researchers\" and cautions against characterizing AI researchers as \"not want(ing) to pursue beneficial designs\".[18]\n\nIn his book Human Compatible, AI researcher Stuart J. Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:[19]: 173 \n\nThe \"preferences\" Russell refers to \"are all-encompassing; they cover everything you might care about, arbitrarily far into the future.\"[19]: 173   Similarly, \"behavior\" includes any choice between options,[19]: 177  and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.[19]: 201 \n\n"
        },
        {
            "title": "Public policy",
            "text": "James Barrat, author of Our Final Invention, suggested that \"a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations.\" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA, which discussed risks of biotechnology.[17]\n\nJohn McGinnis encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the National Institutes of Health, where \"Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards.\" McGinnis feels that peer review is better \"than regulation to address technical issues that are not possible to capture through bureaucratic mandates\". McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute, which generally aims to avoid government involvement in friendly AI.[20]\n\nAccording to Gary Marcus, the annual amount of money being spent on developing machine morality is tiny.[21]\n\n"
        },
        {
            "title": "Criticism",
            "text": "Some critics believe that both human-level AI and superintelligence are unlikely, and that therefore friendly AI is unlikely. Writing in The Guardian, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty, and states that while we need to be \"cautious and prepared\" given the stakes involved, we \"don't need to be obsessing\" about the risks of superintelligence.[22] Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and Nick Bostrom’s proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had.[13] In an article in AI & Society, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.[14]\n\nSome philosophers claim that any truly \"rational\" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.[23] Other critics question whether it is possible for an artificial intelligence to be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal The New Atlantis, say that it will be impossible to ever guarantee \"friendly\" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work \"only when one has not only great powers of prediction about the likelihood of myriad possible outcomes, but certainty and consensus on how one values the different outcomes.[24]\n\n"
        }
    ],
    "links": [
        "https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
        "https://en.wikipedia.org/wiki/Computer_vision",
        "https://en.wikipedia.org/wiki/General_game_playing",
        "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
        "https://en.wikipedia.org/wiki/Machine_learning",
        "https://en.wikipedia.org/wiki/Natural_language_processing",
        "https://en.wikipedia.org/wiki/Robotics",
        "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Deep_learning",
        "https://en.wikipedia.org/wiki/Bayesian_network",
        "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
        "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Chinese_room",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Turing_test",
        "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_winter",
        "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_projects",
        "https://en.wikipedia.org/wiki/List_of_programming_languages_for_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Peter_Norvig",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Technical_terminology",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Golem",
        "https://en.wikipedia.org/wiki/Gerbert_of_Aurillac",
        "https://en.wikipedia.org/wiki/Roger_Bacon",
        "https://en.wikipedia.org/wiki/Isaac_Asimov",
        "https://en.wikipedia.org/wiki/Three_Laws_of_Robotics",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Existential_risk_from_advanced_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Steve_Omohundro",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Bruce_Schneier",
        "https://en.wikipedia.org/wiki/Human_nature",
        "https://en.wikipedia.org/wiki/Evolutionary_psychology",
        "https://en.wikipedia.org/wiki/Utility_function",
        "https://en.wikipedia.org/wiki/Decision_theory",
        "https://en.wikipedia.org/wiki/Metaethics",
        "https://en.wikipedia.org/wiki/Moral_universalism",
        "https://en.wikipedia.org/wiki/Steve_Omohundro",
        "https://en.wikipedia.org/wiki/Seth_Baum",
        "https://en.wikipedia.org/wiki/Human_Compatible",
        "https://en.wikipedia.org/wiki/James_Barrat",
        "https://en.wikipedia.org/wiki/Our_Final_Invention",
        "https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA",
        "https://en.wikipedia.org/wiki/John_McGinnis",
        "https://en.wikipedia.org/wiki/National_Institutes_of_Health",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/Gary_Marcus",
        "https://en.wikipedia.org/wiki/The_Guardian",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Affective_computing",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Emotion_recognition",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/OpenAI",
        "https://en.wikipedia.org/wiki/Regulation_of_algorithms",
        "https://en.wikipedia.org/wiki/Sentiment_analysis",
        "https://en.wikipedia.org/wiki/Singularitarianism",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/Three_Laws_of_Robotics",
        "https://en.wikipedia.org/wiki/Peter_Norvig",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/The_New_Yorker",
        "https://en.wikipedia.org/wiki/The_Guardian",
        "https://en.wikipedia.org/wiki/Existential_risk",
        "https://en.wikipedia.org/wiki/AAAI",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_alignment",
        "https://en.wikipedia.org/wiki/AI_capability_control",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Accelerating_change",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/Allen_Institute_for_AI",
        "https://en.wikipedia.org/wiki/Center_for_Applied_Rationality",
        "https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/Foundational_Questions_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Humanity_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Life_Institute",
        "https://en.wikipedia.org/wiki/Institute_for_Ethics_and_Emerging_Technologies",
        "https://en.wikipedia.org/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/OpenAI",
        "https://en.wikipedia.org/wiki/Slate_Star_Codex",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Sam_Harris",
        "https://en.wikipedia.org/wiki/Stephen_Hawking",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/Bill_Joy",
        "https://en.wikipedia.org/wiki/Elon_Musk",
        "https://en.wikipedia.org/wiki/Steve_Omohundro",
        "https://en.wikipedia.org/wiki/Huw_Price",
        "https://en.wikipedia.org/wiki/Martin_Rees",
        "https://en.wikipedia.org/wiki/Jaan_Tallinn",
        "https://en.wikipedia.org/wiki/Max_Tegmark",
        "https://en.wikipedia.org/wiki/Frank_Wilczek",
        "https://en.wikipedia.org/wiki/Roman_Yampolskiy",
        "https://en.wikipedia.org/wiki/Andrew_Yang",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Suffering_risks",
        "https://en.wikipedia.org/wiki/Human_Compatible",
        "https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence",
        "https://en.wikipedia.org/wiki/Our_Final_Invention",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Main_Page",
        "https://en.wikipedia.org/wiki/Main_Page"
    ]
}