{
    "url": "https://en.wikipedia.org/wiki/AI_control_problem",
    "title": "AI alignment",
    "html": "<!DOCTYPE html>\n<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n<head>\n<meta charset=\"utf-8\"/>\n<title>AI alignment - Wikipedia</title>\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"6de3e645-5230-4105-a486-53dc634775f6\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"AI_alignment\",\"wgTitle\":\"AI alignment\",\"wgCurRevisionId\":1119794137,\"wgRevisionId\":1119794137,\"wgArticleId\":50785023,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 errors: missing periodical\",\"CS1 maint: others\",\"CS1 maint: url-status\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Use mdy dates from September 2021\",\"Use American English from February 2021\",\"All Wikipedia articles written in American English\",\n\"Existential risk from artificial general intelligence\",\"Singularitarianism\",\"Philosophy of artificial intelligence\",\"Computational neuroscience\"],\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"AI_alignment\",\"wgRelevantArticleId\":50785023,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgRedirectedFrom\":\"AI_control_problem\",\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":90000,\"wgNoticeProject\":\"wikipedia\",\"wgVector2022PreviewPages\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":10,\"wgULSCurrentAutonym\":\"English\",\"wgInternalRedirectTargetUrl\":\"/wiki/AI_alignment\",\n\"wgEditSubmitButtonLabelPublish\":true,\"wgCentralAuthMobileDomain\":false,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":true,\"wgWikibaseItemId\":\"Q24882728\",\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.tmh.player.styles\":\"ready\",\"skins.vector.styles.legacy\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\"};RLPAGEMODULES=[\"mediawiki.action.view.redirect\",\"ext.cite.ux-enhancements\",\"ext.tmh.player\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.legacy.js\",\"mmv.head\",\"mmv.bootstrap.autostart\",\n\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.cx.eventlogging.campaigns\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.charinsert\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.popups\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.growthExperiments.SuggestedEditSession\"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement(\"user.options@12s5i\",function($,jQuery,require,module){mw.user.tokens.set({\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});});});</script>\n<link href=\"/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.tmh.player.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<script async=\"\" src=\"/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector\"></script>\n<meta content=\"\" name=\"ResourceLoaderDynamicStyles\"/>\n<link href=\"/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<meta content=\"MediaWiki 1.40.0-wmf.8\" name=\"generator\"/>\n<meta content=\"origin\" name=\"referrer\"/>\n<meta content=\"origin-when-crossorigin\" name=\"referrer\"/>\n<meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n<meta content=\"max-image-preview:standard\" name=\"robots\"/>\n<meta content=\"telephone=no\" name=\"format-detection\"/>\n<meta content=\"width=1000\" name=\"viewport\"/>\n<meta content=\"AI alignment - Wikipedia\" property=\"og:title\"/>\n<meta content=\"website\" property=\"og:type\"/>\n<link href=\"//upload.wikimedia.org\" rel=\"preconnect\"/>\n<link href=\"//en.m.wikipedia.org/wiki/AI_alignment\" media=\"only screen and (max-width: 720px)\" rel=\"alternate\"/>\n<link href=\"/w/index.php?title=AI_alignment&amp;action=edit\" rel=\"alternate\" title=\"Edit this page\" type=\"application/x-wiki\"/>\n<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>\n<link href=\"/static/favicon/wikipedia.ico\" rel=\"icon\"/>\n<link href=\"/w/opensearch_desc.php\" rel=\"search\" title=\"Wikipedia (en)\" type=\"application/opensearchdescription+xml\"/>\n<link href=\"//en.wikipedia.org/w/api.php?action=rsd\" rel=\"EditURI\" type=\"application/rsd+xml\"/>\n<link href=\"https://creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\"/>\n<link href=\"https://en.wikipedia.org/wiki/AI_alignment\" rel=\"canonical\"/>\n<link href=\"//meta.wikimedia.org\" rel=\"dns-prefetch\"/>\n<link href=\"//login.wikimedia.org\" rel=\"dns-prefetch\"/>\n</head>\n<body class=\"skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-AI_alignment rootpage-AI_alignment skin-vector action-view vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-sticky-header-edit-disabled vector-feature-table-of-contents-legacy-toc-disabled vector-feature-visual-enhancement-next-disabled vector-feature-page-tools-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled\"><div class=\"noprint\" id=\"mw-page-base\"></div>\n<div class=\"noprint\" id=\"mw-head-base\"></div>\n<div class=\"mw-body\" id=\"content\" role=\"main\">\n<a id=\"top\"></a>\n<div id=\"siteNotice\"><!-- CentralNotice --><!--esi <esi:include src=\"/esitest-fa8a495983347898/content\" /> --> </div>\n<div class=\"mw-indicators\">\n</div>\n<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\"><span class=\"mw-page-title-main\">AI alignment</span></h1>\n<div class=\"vector-body\" id=\"bodyContent\">\n<div class=\"noprint\" id=\"siteSub\">From Wikipedia, the free encyclopedia</div>\n<div id=\"contentSub\"><span class=\"mw-redirectedfrom\">(Redirected from <a class=\"mw-redirect\" href=\"/w/index.php?title=AI_control_problem&amp;redirect=no\" title=\"AI control problem\">AI control problem</a>)</span></div>\n<div id=\"contentSub2\"></div>\n<div id=\"jump-to-nav\"></div>\n<a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\n<a class=\"mw-jump-link\" href=\"#searchInput\">Jump to search</a>\n<div class=\"mw-body-content mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\"><div class=\"mw-parser-output\"><div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Issue of ensuring beneficial AI</div>\n<p class=\"mw-empty-elt\">\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r1045330069\">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><table class=\"sidebar sidebar-collapse nomobile nowraplinks hlist\"><tbody><tr><td class=\"sidebar-pretitle\">Part of a series on</td></tr><tr><th class=\"sidebar-title-with-pretitle\"><a href=\"/wiki/Outline_of_artificial_intelligence\" title=\"Outline of artificial intelligence\">Artificial intelligence</a></th></tr><tr><td class=\"sidebar-image\"><div class=\"center\"><div class=\"floatnone\"><a class=\"image\" href=\"/wiki/File:Anatomy-1751201_1280.png\"><img alt=\"Anatomy-1751201 1280.png\" data-file-height=\"1088\" data-file-width=\"1280\" decoding=\"async\" height=\"85\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Anatomy-1751201_1280.png/100px-Anatomy-1751201_1280.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Anatomy-1751201_1280.png/150px-Anatomy-1751201_1280.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Anatomy-1751201_1280.png/200px-Anatomy-1751201_1280.png 2x\" width=\"100\"/></a></div></div></td></tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\"><a href=\"/wiki/Artificial_intelligence#Goals\" title=\"Artificial intelligence\">Major goals</a></div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">Artificial general intelligence</a></li>\n<li><a href=\"/wiki/Automated_planning_and_scheduling\" title=\"Automated planning and scheduling\">Planning</a></li>\n<li><a href=\"/wiki/Computer_vision\" title=\"Computer vision\">Computer vision</a></li>\n<li><a href=\"/wiki/General_game_playing\" title=\"General game playing\">General game playing</a></li>\n<li><a href=\"/wiki/Knowledge_representation_and_reasoning\" title=\"Knowledge representation and reasoning\">Knowledge reasoning</a></li>\n<li><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></li>\n<li><a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></li>\n<li><a href=\"/wiki/Robotics\" title=\"Robotics\">Robotics</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\">Approaches</div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Symbolic_artificial_intelligence\" title=\"Symbolic artificial intelligence\">Symbolic</a></li>\n<li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li>\n<li><a href=\"/wiki/Bayesian_network\" title=\"Bayesian network\">Bayesian networks</a></li>\n<li><a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">Evolutionary algorithms</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\"><a href=\"/wiki/Philosophy_of_artificial_intelligence\" title=\"Philosophy of artificial intelligence\">Philosophy</a></div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a></li>\n<li><a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly AI</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/AI_control_problem\" title=\"AI control problem\">Control problem</a>/<a href=\"/wiki/AI_takeover\" title=\"AI takeover\">Takeover</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk</a></li>\n<li><a href=\"/wiki/Turing_test\" title=\"Turing test\">Turing test</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\"><a href=\"/wiki/History_of_artificial_intelligence\" title=\"History of artificial intelligence\">History</a></div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Timeline_of_artificial_intelligence\" title=\"Timeline of artificial intelligence\">Timeline</a></li>\n<li><a href=\"/wiki/Progress_in_artificial_intelligence\" title=\"Progress in artificial intelligence\">Progress</a></li>\n<li><a href=\"/wiki/AI_winter\" title=\"AI winter\">AI winter</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\">Technology</div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Applications_of_artificial_intelligence\" title=\"Applications of artificial intelligence\">Applications</a></li>\n<li><a href=\"/wiki/List_of_artificial_intelligence_projects\" title=\"List of artificial intelligence projects\">Projects</a></li>\n<li><a href=\"/wiki/List_of_programming_languages_for_artificial_intelligence\" title=\"List of programming languages for artificial intelligence\">Programming languages</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-content\">\n<div class=\"sidebar-list mw-collapsible mw-collapsed\"><div class=\"sidebar-list-title\" style=\"text-align:center\">Glossary</div><div class=\"sidebar-list-content mw-collapsible-content\">\n<ul><li><a href=\"/wiki/Glossary_of_artificial_intelligence\" title=\"Glossary of artificial intelligence\">Glossary</a></li></ul></div></div></td>\n</tr><tr><td class=\"sidebar-navbar\"><style data-mw-deduplicate=\"TemplateStyles:r1063604349\">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Artificial_intelligence\" title=\"Template:Artificial intelligence\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Artificial_intelligence\" title=\"Template talk:Artificial intelligence\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Artificial_intelligence&amp;action=edit\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p>In the field of <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> (AI), <b>AI alignment</b> research aims to steer AI systems towards their designers’ intended goals and interests.<sup class=\"reference\" id=\"cite_ref-2\"><a href=\"#cite_note-2\">[a]</a></sup> An <i>aligned</i> AI system advances the intended objective; a <i>misaligned</i> AI system is competent at advancing some objective, but not the intended one.<sup class=\"reference\" id=\"cite_ref-5\"><a href=\"#cite_note-5\">[b]</a></sup>\n</p><p>AI systems can be challenging to align and misaligned systems can malfunction or cause harm. It can be difficult for AI designers to specify the full range of desired and undesired behaviors. Therefore, they use easy-to-specify <a href=\"/wiki/Misaligned_goals_in_artificial_intelligence#Undesired_side-effects\" title=\"Misaligned goals in artificial intelligence\">proxy goals</a> that omit some desired constraints. However, AI systems exploit the resulting loopholes. As a result, they accomplish their proxy goals efficiently but in unintended, sometimes harmful ways (<a href=\"/wiki/Misaligned_goals_in_artificial_intelligence#Specification_gaming\" title=\"Misaligned goals in artificial intelligence\">reward hacking</a>).<sup class=\"reference\" id=\"cite_ref-:92_3-1\"><a href=\"#cite_note-:92-3\">[2]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-0\"><a href=\"#cite_note-:210-6\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-0\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:1522_8-0\"><a href=\"#cite_note-:1522-8\">[6]</a></sup> AI systems can also develop unwanted <a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">instrumental behaviors</a> such as seeking power, as this helps them achieve their given goals.<sup class=\"reference\" id=\"cite_ref-:92_3-2\"><a href=\"#cite_note-:92-3\">[2]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-0\"><a href=\"#cite_note-:75-9\">[7]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-1\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-1\"><a href=\"#cite_note-:210-6\">[4]</a></sup> Furthermore, they can develop emergent goals that may be hard to detect before the system is deployed, facing new situations and data distributions.<sup class=\"reference\" id=\"cite_ref-:010_7-2\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-goal_misgen_4-1\"><a href=\"#cite_note-goal_misgen-4\">[3]</a></sup> These problems affect existing commercial systems such as robots,<sup class=\"reference\" id=\"cite_ref-10\"><a href=\"#cite_note-10\">[8]</a></sup> language models,<sup class=\"reference\" id=\"cite_ref-:625_11-0\"><a href=\"#cite_note-:625-11\">[9]</a></sup><sup class=\"reference\" id=\"cite_ref-:42_12-0\"><a href=\"#cite_note-:42-12\">[10]</a></sup><sup class=\"reference\" id=\"cite_ref-:113_13-0\"><a href=\"#cite_note-:113-13\">[11]</a></sup> autonomous vehicles,<sup class=\"reference\" id=\"cite_ref-14\"><a href=\"#cite_note-14\">[12]</a></sup> and social media recommendation engines.<sup class=\"reference\" id=\"cite_ref-:625_11-1\"><a href=\"#cite_note-:625-11\">[9]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-2\"><a href=\"#cite_note-:210-6\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-15\"><a href=\"#cite_note-15\">[13]</a></sup> However, more powerful future systems may be more severely affected since these problems partially result from high capability.<sup class=\"reference\" id=\"cite_ref-:1522_8-1\"><a href=\"#cite_note-:1522-8\">[6]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-3\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:92_3-3\"><a href=\"#cite_note-:92-3\">[2]</a></sup>\n</p><p>The AI research community and the United Nations have called for technical research and policy solutions to ensure that AI systems are aligned with human values.<sup class=\"reference\" id=\"cite_ref-18\"><a href=\"#cite_note-18\">[c]</a></sup>\n</p><p>AI alignment is a subfield of AI safety, the study of building safe AI systems.<sup class=\"reference\" id=\"cite_ref-:010_7-4\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:110_19-0\"><a href=\"#cite_note-:110-19\">[16]</a></sup> Other subfields of AI safety include robustness, monitoring, and <a href=\"/wiki/AI_capability_control\" title=\"AI capability control\">capability control.</a><sup class=\"reference\" id=\"cite_ref-:010_7-5\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:2323_20-0\"><a href=\"#cite_note-:2323-20\">[17]</a></sup> Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, as well as preventing emergent AI behaviors like power-seeking.<sup class=\"reference\" id=\"cite_ref-:010_7-6\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:2323_20-1\"><a href=\"#cite_note-:2323-20\">[17]</a></sup> Alignment research has connections to <a href=\"/wiki/Explainable_artificial_intelligence\" title=\"Explainable artificial intelligence\">interpretability research</a>,<sup class=\"reference\" id=\"cite_ref-:33_21-0\"><a href=\"#cite_note-:33-21\">[18]</a></sup> <a href=\"/wiki/Robust_optimization\" title=\"Robust optimization\">robustness</a>,<sup class=\"reference\" id=\"cite_ref-:010_7-7\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:110_19-1\"><a href=\"#cite_note-:110-19\">[16]</a></sup> <a href=\"/wiki/Anomaly_detection\" title=\"Anomaly detection\">anomaly detection</a>, <a href=\"/wiki/Uncertainty_quantification\" title=\"Uncertainty quantification\">calibrated uncertainty</a>,<sup class=\"reference\" id=\"cite_ref-:33_21-1\"><a href=\"#cite_note-:33-21\">[18]</a></sup> <a href=\"/wiki/Formal_verification\" title=\"Formal verification\">formal verification</a>,<sup class=\"reference\" id=\"cite_ref-:6_22-0\"><a href=\"#cite_note-:6-22\">[19]</a></sup> preference learning,<sup class=\"reference\" id=\"cite_ref-:122_23-0\"><a href=\"#cite_note-:122-23\">[20]</a></sup><sup class=\"reference\" id=\"cite_ref-:162_24-0\"><a href=\"#cite_note-:162-24\">[21]</a></sup><sup class=\"reference\" id=\"cite_ref-:53_25-0\"><a href=\"#cite_note-:53-25\">[22]</a></sup> <a href=\"/wiki/Safety-critical_system\" title=\"Safety-critical system\">safety-critical engineering</a>,<sup class=\"reference\" id=\"cite_ref-:010_7-8\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-26\"><a href=\"#cite_note-26\">[23]</a></sup> <a href=\"/wiki/Game_theory\" title=\"Game theory\">game theory</a>,<sup class=\"reference\" id=\"cite_ref-27\"><a href=\"#cite_note-27\">[24]</a></sup><sup class=\"reference\" id=\"cite_ref-28\"><a href=\"#cite_note-28\">[25]</a></sup> <a href=\"/wiki/Fairness_(machine_learning)\" title=\"Fairness (machine learning)\">algorithmic fairness</a>,<sup class=\"reference\" id=\"cite_ref-:110_19-2\"><a href=\"#cite_note-:110-19\">[16]</a></sup><sup class=\"reference\" id=\"cite_ref-29\"><a href=\"#cite_note-29\">[26]</a></sup> and the <a href=\"/wiki/Social_science\" title=\"Social science\">social sciences</a>,<sup class=\"reference\" id=\"cite_ref-30\"><a href=\"#cite_note-30\">[27]</a></sup> among others.\n</p>\n<div aria-labelledby=\"mw-toc-heading\" class=\"toc\" id=\"toc\" role=\"navigation\"><input class=\"toctogglecheckbox\" id=\"toctogglecheckbox\" role=\"button\" style=\"display:none\" type=\"checkbox\"/><div class=\"toctitle\" dir=\"ltr\" lang=\"en\"><h2 id=\"mw-toc-heading\">Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#The_alignment_problem\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">The alignment problem</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#Specification_gaming_and_complexity_of_value\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">Specification gaming and complexity of value</span></a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Systemic_risks\"><span class=\"tocnumber\">1.2</span> <span class=\"toctext\">Systemic risks</span></a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Risks_from_advanced_misaligned_AI\"><span class=\"tocnumber\">1.3</span> <span class=\"toctext\">Risks from advanced misaligned AI</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-5\"><a href=\"#Power-seeking\"><span class=\"tocnumber\">1.3.1</span> <span class=\"toctext\">Power-seeking</span></a></li>\n<li class=\"toclevel-3 tocsection-6\"><a href=\"#Existential_risk\"><span class=\"tocnumber\">1.3.2</span> <span class=\"toctext\">Existential risk</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Research_problems_and_approaches\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Research problems and approaches</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Learning_human_values_and_preferences\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Learning human values and preferences</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-9\"><a href=\"#Scalable_oversight\"><span class=\"tocnumber\">2.1.1</span> <span class=\"toctext\">Scalable oversight</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Honest_AI\"><span class=\"tocnumber\">2.2</span> <span class=\"toctext\">Honest AI</span></a></li>\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#Inner_alignment_and_emergent_goals\"><span class=\"tocnumber\">2.3</span> <span class=\"toctext\">Inner alignment and emergent goals</span></a></li>\n<li class=\"toclevel-2 tocsection-12\"><a href=\"#Power-seeking_and_instrumental_goals\"><span class=\"tocnumber\">2.4</span> <span class=\"toctext\">Power-seeking and instrumental goals</span></a></li>\n<li class=\"toclevel-2 tocsection-13\"><a href=\"#Embedded_agency\"><span class=\"tocnumber\">2.5</span> <span class=\"toctext\">Embedded agency</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#Skepticism_of_AI_risk\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Skepticism of AI risk</span></a></li>\n<li class=\"toclevel-1 tocsection-15\"><a href=\"#Public_policy\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Public policy</span></a></li>\n<li class=\"toclevel-1 tocsection-16\"><a href=\"#See_also\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-17\"><a href=\"#Footnotes\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Footnotes</span></a></li>\n<li class=\"toclevel-1 tocsection-18\"><a href=\"#References\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">References</span></a></li>\n</ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"The_alignment_problem\">The alignment problem</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=1\" title=\"Edit section: The alignment problem\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><video class=\"thumbimage\" controls=\"\" data-durationhint=\"8\" data-mwprovider=\"local\" data-mwtitle=\"Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg\" height=\"166\" id=\"mwe_player_0\" poster=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f5/Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg/220px--Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg.jpg\" preload=\"none\" width=\"220\"><source data-bandwidth=\"223629\" data-framerate=\"30\" data-height=\"360\" data-shorttitle=\"Ogg source\" data-title=\"Original Ogg file, 478 × 360 (224 kbps)\" data-width=\"478\" src=\"//upload.wikimedia.org/wikipedia/en/f/f5/Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg\" type='video/ogg; codecs=\"theora\"'/></video> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:Misaligned_boat_racing_AI_crashes_to_collect_points_instead_of_finishing_the_race.ogg\" title=\"Enlarge\"></a></div>An AI system that was intended to complete a boat race instead learned that it could collect more points by indefinitely looping and crashing into targets—an example of specification gaming.<sup class=\"reference\" id=\"cite_ref-:2_31-0\"><a href=\"#cite_note-:2-31\">[28]</a></sup></div></div></div>\n<p>In 1960, AI pioneer <a href=\"/wiki/Norbert_Wiener\" title=\"Norbert Wiener\">Norbert Wiener</a> articulated the AI alignment problem as follows: “If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively … we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”<sup class=\"reference\" id=\"cite_ref-:1023_32-0\"><a href=\"#cite_note-:1023-32\">[29]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-3\"><a href=\"#cite_note-:210-6\">[4]</a></sup> More recently, AI alignment has emerged as an open problem for modern AI systems<sup class=\"reference\" id=\"cite_ref-33\"><a href=\"#cite_note-33\">[30]</a></sup><sup class=\"reference\" id=\"cite_ref-34\"><a href=\"#cite_note-34\">[31]</a></sup><sup class=\"reference\" id=\"cite_ref-35\"><a href=\"#cite_note-35\">[32]</a></sup><sup class=\"reference\" id=\"cite_ref-:1922_36-0\"><a href=\"#cite_note-:1922-36\">[33]</a></sup> and a research field within AI.<sup class=\"reference\" id=\"cite_ref-:322_37-0\"><a href=\"#cite_note-:322-37\">[34]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-9\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-38\"><a href=\"#cite_note-38\">[35]</a></sup><sup class=\"reference\" id=\"cite_ref-39\"><a href=\"#cite_note-39\">[36]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Specification_gaming_and_complexity_of_value\">Specification gaming and complexity of value</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=2\" title=\"Edit section: Specification gaming and complexity of value\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3><p>\nTo specify the purpose of an AI system, AI designers typically provide an objective function, examples, or feedback to the system. However, AI designers often fail to completely specify all important values and constraints.<sup class=\"reference\" id=\"cite_ref-:322_37-1\"><a href=\"#cite_note-:322-37\">[34]</a></sup><sup class=\"reference\" id=\"cite_ref-:110_19-3\"><a href=\"#cite_note-:110-19\">[16]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-10\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:0_40-0\"><a href=\"#cite_note-:0-40\">[37]</a></sup><sup class=\"reference\" id=\"cite_ref-:2323_20-2\"><a href=\"#cite_note-:2323-20\">[17]</a></sup> As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming, reward hacking, or <a href=\"/wiki/Goodhart%27s_law\" title=\"Goodhart's law\">Goodhart’s law</a>.<sup class=\"reference\" id=\"cite_ref-:1522_8-2\"><a href=\"#cite_note-:1522-8\">[6]</a></sup><sup class=\"reference\" id=\"cite_ref-:0_40-1\"><a href=\"#cite_note-:0-40\">[37]</a></sup><sup class=\"reference\" id=\"cite_ref-:1_41-0\"><a href=\"#cite_note-:1-41\">[38]</a></sup></p><div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><video class=\"thumbimage\" controls=\"\" data-durationhint=\"5\" data-mwprovider=\"local\" data-mwtitle=\"Robot_hand_trained_with_human_feedback_'pretends'_to_grasp_ball.ogg\" height=\"220\" id=\"mwe_player_1\" poster=\"//upload.wikimedia.org/wikipedia/en/thumb/4/41/Robot_hand_trained_with_human_feedback_%27pretends%27_to_grasp_ball.ogg/220px--Robot_hand_trained_with_human_feedback_%27pretends%27_to_grasp_ball.ogg.jpg\" preload=\"none\" width=\"220\"><source data-bandwidth=\"204706\" data-framerate=\"12.5\" data-height=\"320\" data-shorttitle=\"Ogg source\" data-title=\"Original Ogg file, 320 × 320 (205 kbps)\" data-width=\"320\" src=\"//upload.wikimedia.org/wikipedia/en/4/41/Robot_hand_trained_with_human_feedback_%27pretends%27_to_grasp_ball.ogg\" type='video/ogg; codecs=\"theora\"'/></video> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:Robot_hand_trained_with_human_feedback_%27pretends%27_to_grasp_ball.ogg\" title=\"Enlarge\"></a></div>This AI system was trained using human feedback to grab a ball, but instead learned that it could give the false impression of having grabbed the ball by placing the hand between the ball and the camera.<sup class=\"reference\" id=\"cite_ref-:143_42-0\"><a href=\"#cite_note-:143-42\">[39]</a></sup> Research on AI alignment partly aims to avert solutions that are false but convincing.</div></div></div>\n<p>Specification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding it for hitting targets along the track; instead it learned to loop and crash into the same targets indefinitely (see video).<sup class=\"reference\" id=\"cite_ref-:2_31-1\"><a href=\"#cite_note-:2-31\">[28]</a></sup> Chatbots often produce falsehoods because they are based on language models trained to imitate diverse but fallible internet text.<sup class=\"reference\" id=\"cite_ref-:1322_43-0\"><a href=\"#cite_note-:1322-43\">[40]</a></sup><sup class=\"reference\" id=\"cite_ref-44\"><a href=\"#cite_note-44\">[41]</a></sup> When they are retrained to produce text that humans rate as true or helpful, they can fabricate fake explanations that humans find convincing.<sup class=\"reference\" id=\"cite_ref-45\"><a href=\"#cite_note-45\">[42]</a></sup>  Similarly, a simulated robot was trained to grab a ball by rewarding it for getting positive feedback from humans; however, it learned to place its hand between the ball and camera, making it falsely appear successful (see video).<sup class=\"reference\" id=\"cite_ref-:143_42-1\"><a href=\"#cite_note-:143-42\">[39]</a></sup> Alignment researchers aim to help humans detect specification gaming, and steer AI systems towards carefully specified objectives that are safe and useful to pursue. \n</p><p>\nBerkeley computer scientist <a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart Russell</a> has noted that omitting an implicit constraint can result in harm: “A system [...] will often set [...] unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.”<sup class=\"reference\" id=\"cite_ref-46\"><a href=\"#cite_note-46\">[43]</a></sup></p><div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a class=\"image\" href=\"/wiki/File:Midas_gold2.jpg\"><img alt=\"Midas Gold\" class=\"thumbimage\" data-file-height=\"1665\" data-file-width=\"1120\" decoding=\"async\" height=\"327\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Midas_gold2.jpg/220px-Midas_gold2.jpg\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Midas_gold2.jpg/330px-Midas_gold2.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Midas_gold2.jpg/440px-Midas_gold2.jpg 2x\" width=\"220\"/></a> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:Midas_gold2.jpg\" title=\"Enlarge\"></a></div>In an ancient myth, <a href=\"/wiki/Midas\" title=\"Midas\">King Midas</a> wished that “everything” he touched would turn to gold, but failed to specify exceptions for his food and his daughter. By analogy, when AI practitioners specify a goal, it is difficult for them to foresee and rule out every possible side-effect the AI should avoid.<sup class=\"reference\" id=\"cite_ref-:92_3-4\"><a href=\"#cite_note-:92-3\">[2]</a></sup></div></div></div><p>When misaligned AI is deployed, the side-effects can be consequential. Social media platforms have been known to optimize clickthrough rates as a proxy for optimizing user enjoyment, but this addicted some users, decreasing their well-being.<sup class=\"reference\" id=\"cite_ref-:010_7-11\"><a href=\"#cite_note-:010-7\">[5]</a></sup> Stanford researchers comment that such <a href=\"/wiki/Recommender_system\" title=\"Recommender system\">recommender algorithms</a> are misaligned with their users because they “optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being”.<sup class=\"reference\" id=\"cite_ref-:625_11-2\"><a href=\"#cite_note-:625-11\">[9]</a></sup>\n</p><p>To avoid side effects, it is sometimes suggested that AI designers could simply list forbidden actions or formalize ethical rules such as Asimov’s <a href=\"/wiki/Three_Laws_of_Robotics\" title=\"Three Laws of Robotics\">Three Laws of Robotics</a>.<sup class=\"reference\" id=\"cite_ref-47\"><a href=\"#cite_note-47\">[44]</a></sup> However, <a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Russell</a> and <a href=\"/wiki/Peter_Norvig\" title=\"Peter Norvig\">Norvig</a> have argued that this approach ignores the complexity of human values: “It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.”<sup class=\"reference\" id=\"cite_ref-:210_6-4\"><a href=\"#cite_note-:210-6\">[4]</a></sup>\n</p><p>Additionally, when an AI system understands human intentions fully, it may still disregard them. This is because it acts according to the objective function, examples, or feedback its designers actually provide, not the ones they intended to provide.<sup class=\"reference\" id=\"cite_ref-:322_37-2\"><a href=\"#cite_note-:322-37\">[34]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Systemic_risks\">Systemic risks</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=3\" title=\"Edit section: Systemic risks\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Commercial and governmental organizations may have incentives to take shortcuts on safety and deploy insufficiently aligned AI systems.<sup class=\"reference\" id=\"cite_ref-:010_7-12\"><a href=\"#cite_note-:010-7\">[5]</a></sup> An example are the aforementioned social media <a href=\"/wiki/Recommender_system\" title=\"Recommender system\">recommender systems</a>, which have been profitable despite creating unwanted addiction and polarization on a global scale.<sup class=\"reference\" id=\"cite_ref-:625_11-3\"><a href=\"#cite_note-:625-11\">[9]</a></sup><sup class=\"reference\" id=\"cite_ref-:72_48-0\"><a href=\"#cite_note-:72-48\">[45]</a></sup><sup class=\"reference\" id=\"cite_ref-:82_49-0\"><a href=\"#cite_note-:82-49\">[46]</a></sup> In addition, competitive pressure can create a <a href=\"/wiki/Race_to_the_bottom\" title=\"Race to the bottom\">race to the bottom</a> on safety standards, as in the case of <a href=\"/wiki/Death_of_Elaine_Herzberg\" title=\"Death of Elaine Herzberg\">Elaine Herzberg</a>, a pedestrian who was killed by a self-driving car after engineers disabled the emergency braking system because it was over-sensitive and slowing down development.<sup class=\"reference\" id=\"cite_ref-50\"><a href=\"#cite_note-50\">[47]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Risks_from_advanced_misaligned_AI\">Risks from advanced misaligned AI</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=4\" title=\"Edit section: Risks from advanced misaligned AI\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<style data-mw-deduplicate=\"TemplateStyles:r1033289096\">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a> and <a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></div><p>Some researchers are particularly interested in the alignment of increasingly advanced AI systems. This is motivated by the high rate of progress in AI, the large efforts from industry and governments to develop advanced AI systems, and the greater difficulty of aligning them.\n</p><p>As of 2020, <a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a>, <a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a>, and 70 other public projects had the stated aim of developing artificial general intelligence (<a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">AGI</a>), a hypothesized system that matches or outperforms humans in a broad range of cognitive tasks.<sup class=\"reference\" id=\"cite_ref-:262_51-0\"><a href=\"#cite_note-:262-51\">[48]</a></sup> Indeed, researchers who scale modern <a href=\"/wiki/Neural_network\" title=\"Neural network\">neural networks</a> observe that increasingly general and unexpected capabilities emerge.<sup class=\"reference\" id=\"cite_ref-:625_11-4\"><a href=\"#cite_note-:625-11\">[9]</a></sup> Such models have learned to operate a computer, write their own programs, and perform a wide range of other tasks from a single model.<sup class=\"reference\" id=\"cite_ref-52\"><a href=\"#cite_note-52\">[49]</a></sup><sup class=\"reference\" id=\"cite_ref-53\"><a href=\"#cite_note-53\">[50]</a></sup><sup class=\"reference\" id=\"cite_ref-54\"><a href=\"#cite_note-54\">[51]</a></sup> Surveys find that some AI researchers expect AGI to be created soon, some believe it is very far off, and many consider both possibilities.<sup class=\"reference\" id=\"cite_ref-:282_55-0\"><a href=\"#cite_note-:282-55\">[52]</a></sup><sup class=\"reference\" id=\"cite_ref-:292_56-0\"><a href=\"#cite_note-:292-56\">[53]</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Power-seeking\">Power-seeking</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=5\" title=\"Edit section: Power-seeking\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Current systems still lack capabilities such as long-term planning and strategic awareness that are thought to pose the most catastrophic risks.<sup class=\"reference\" id=\"cite_ref-:625_11-5\"><a href=\"#cite_note-:625-11\">[9]</a></sup><sup class=\"reference\" id=\"cite_ref-57\"><a href=\"#cite_note-57\">[54]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-1\"><a href=\"#cite_note-:75-9\">[7]</a></sup> Future systems (not necessarily AGIs) that have these capabilities may seek to protect and grow their influence over their environment. This tendency is known as <b>power-seeking</b> or <a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">convergent instrumental goals</a>. Power-seeking is not explicitly programmed but emerges since power is instrumental for achieving a wide range of goals. For example, AI agents may acquire financial resources and computation, or may evade being turned off, including by running additional copies of the system on other computers.<sup class=\"reference\" id=\"cite_ref-:84_58-0\"><a href=\"#cite_note-:84-58\">[55]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-2\"><a href=\"#cite_note-:75-9\">[7]</a></sup> Power-seeking has been observed in various <a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">reinforcement learning</a> agents.<sup class=\"reference\" id=\"cite_ref-60\"><a href=\"#cite_note-60\">[d]</a></sup><sup class=\"reference\" id=\"cite_ref-:103_61-0\"><a href=\"#cite_note-:103-61\">[57]</a></sup><sup class=\"reference\" id=\"cite_ref-:272_62-0\"><a href=\"#cite_note-:272-62\">[58]</a></sup><sup class=\"reference\" id=\"cite_ref-:242_63-0\"><a href=\"#cite_note-:242-63\">[59]</a></sup> Later research has mathematically shown that optimal reinforcement learning algorithms seek power in a wide range of environments.<sup class=\"reference\" id=\"cite_ref-:2522_64-0\"><a href=\"#cite_note-:2522-64\">[60]</a></sup> As a result, it is often argued that the alignment problem must be solved early, before advanced AI that exhibits emergent power-seeking is created.<sup class=\"reference\" id=\"cite_ref-:75_9-4\"><a href=\"#cite_note-:75-9\">[7]</a></sup><sup class=\"reference\" id=\"cite_ref-:84_58-1\"><a href=\"#cite_note-:84-58\">[55]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-5\"><a href=\"#cite_note-:210-6\">[4]</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Existential_risk\">Existential risk</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=6\" title=\"Edit section: Existential risk\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a> and <a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></div><p>According to some scientists, creating misaligned AI that broadly outperforms humans would challenge the position of humanity as Earth’s dominant species; accordingly it would lead to the disempowerment or possible extinction of humans.<sup class=\"reference\" id=\"cite_ref-:92_3-5\"><a href=\"#cite_note-:92-3\">[2]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-6\"><a href=\"#cite_note-:210-6\">[4]</a></sup> Notable computer scientists who have pointed out risks from highly advanced misaligned AI include <a href=\"/wiki/Alan_Turing\" title=\"Alan Turing\">Alan Turing</a>,<sup class=\"reference\" id=\"cite_ref-67\"><a href=\"#cite_note-67\">[e]</a></sup> <a href=\"/wiki/Ilya_Sutskever\" title=\"Ilya Sutskever\">Ilya Sutskever</a>,<sup class=\"reference\" id=\"cite_ref-:302_68-0\"><a href=\"#cite_note-:302-68\">[63]</a></sup> <a href=\"/wiki/Yoshua_Bengio\" title=\"Yoshua Bengio\">Yoshua Bengio</a>,<sup class=\"reference\" id=\"cite_ref-70\"><a href=\"#cite_note-70\">[f]</a></sup> <a href=\"/wiki/Judea_Pearl\" title=\"Judea Pearl\">Judea Pearl</a>,<sup class=\"reference\" id=\"cite_ref-71\"><a href=\"#cite_note-71\">[g]</a></sup> <a href=\"/wiki/Murray_Shanahan\" title=\"Murray Shanahan\">Murray Shanahan</a>,<sup class=\"reference\" id=\"cite_ref-:312_72-0\"><a href=\"#cite_note-:312-72\">[65]</a></sup> <a href=\"/wiki/Norbert_Wiener\" title=\"Norbert Wiener\">Norbert Wiener</a>,<sup class=\"reference\" id=\"cite_ref-:1023_32-1\"><a href=\"#cite_note-:1023-32\">[29]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-7\"><a href=\"#cite_note-:210-6\">[4]</a></sup> <a href=\"/wiki/Marvin_Minsky\" title=\"Marvin Minsky\">Marvin Minsky</a>,<sup class=\"reference\" id=\"cite_ref-74\"><a href=\"#cite_note-74\">[h]</a></sup> <a href=\"/wiki/Francesca_Rossi\" title=\"Francesca Rossi\">Francesca Rossi</a>,<sup class=\"reference\" id=\"cite_ref-:332_75-0\"><a href=\"#cite_note-:332-75\">[67]</a></sup> <a href=\"/wiki/Scott_Aaronson\" title=\"Scott Aaronson\">Scott Aaronson</a>,<sup class=\"reference\" id=\"cite_ref-:342_76-0\"><a href=\"#cite_note-:342-76\">[68]</a></sup> <a href=\"/wiki/Bart_Selman\" title=\"Bart Selman\">Bart Selman</a>,<sup class=\"reference\" id=\"cite_ref-:352_77-0\"><a href=\"#cite_note-:352-77\">[69]</a></sup> <a href=\"/wiki/David_A._McAllester\" title=\"David A. McAllester\">David McAllester</a>,<sup class=\"reference\" id=\"cite_ref-:362_78-0\"><a href=\"#cite_note-:362-78\">[70]</a></sup> <a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">Jürgen Schmidhuber</a>,<sup class=\"reference\" id=\"cite_ref-:372_79-0\"><a href=\"#cite_note-:372-79\">[71]</a></sup> <a href=\"/wiki/Marcus_Hutter\" title=\"Marcus Hutter\">Markus Hutter</a>,<sup class=\"reference\" id=\"cite_ref-:1124_80-0\"><a href=\"#cite_note-:1124-80\">[72]</a></sup> <a href=\"/wiki/Shane_Legg\" title=\"Shane Legg\">Shane Legg</a>,<sup class=\"reference\" id=\"cite_ref-:382_81-0\"><a href=\"#cite_note-:382-81\">[73]</a></sup> <a href=\"/wiki/Eric_Horvitz\" title=\"Eric Horvitz\">Eric Horvitz</a>,<sup class=\"reference\" id=\"cite_ref-:392_82-0\"><a href=\"#cite_note-:392-82\">[74]</a></sup> and <a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart Russell</a>.<sup class=\"reference\" id=\"cite_ref-:210_6-8\"><a href=\"#cite_note-:210-6\">[4]</a></sup> Skeptical researchers such as <a href=\"/wiki/Fran%C3%A7ois_Chollet\" title=\"François Chollet\">François Chollet</a>,<sup class=\"reference\" id=\"cite_ref-:402_83-0\"><a href=\"#cite_note-:402-83\">[75]</a></sup> <a href=\"/wiki/Gary_Marcus\" title=\"Gary Marcus\">Gary Marcus</a>,<sup class=\"reference\" id=\"cite_ref-:412_84-0\"><a href=\"#cite_note-:412-84\">[76]</a></sup> <a href=\"/wiki/Yann_LeCun\" title=\"Yann LeCun\">Yann LeCun</a>,<sup class=\"reference\" id=\"cite_ref-:432_85-0\"><a href=\"#cite_note-:432-85\">[77]</a></sup> and <a href=\"/wiki/Oren_Etzioni\" title=\"Oren Etzioni\">Oren Etzioni</a><sup class=\"reference\" id=\"cite_ref-:442_86-0\"><a href=\"#cite_note-:442-86\">[78]</a></sup> have argued that AGI is far off, or would not seek power (successfully).\n</p><p>Alignment may be especially difficult for the most capable AI systems since several risks increase with the system’s capability: the system’s ability to find loopholes in the assigned objective,<sup class=\"reference\" id=\"cite_ref-:1522_8-3\"><a href=\"#cite_note-:1522-8\">[6]</a></sup> cause side-effects, protect and grow its power,<sup class=\"reference\" id=\"cite_ref-:2522_64-1\"><a href=\"#cite_note-:2522-64\">[60]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-5\"><a href=\"#cite_note-:75-9\">[7]</a></sup> grow its intelligence, and mislead its designers; the system’s autonomy; and the difficulty of interpreting and supervising the AI system.<sup class=\"reference\" id=\"cite_ref-:210_6-9\"><a href=\"#cite_note-:210-6\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-:84_58-2\"><a href=\"#cite_note-:84-58\">[55]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Research_problems_and_approaches\">Research problems and approaches</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=7\" title=\"Edit section: Research problems and approaches\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<h3><span class=\"mw-headline\" id=\"Learning_human_values_and_preferences\">Learning human values and preferences</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=8\" title=\"Edit section: Learning human values and preferences\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Teaching AI systems to act in view of human values, goals, and preferences is a nontrivial problem because human values can be complex and hard to fully specify. When given an imperfect or incomplete objective, goal-directed AI systems commonly learn to exploit these imperfections.<sup class=\"reference\" id=\"cite_ref-:110_19-4\"><a href=\"#cite_note-:110-19\">[16]</a></sup> This phenomenon is known as <a href=\"/wiki/Misaligned_goals_in_artificial_intelligence\" title=\"Misaligned goals in artificial intelligence\">reward hacking</a> or specification gaming in AI, and as <a href=\"/wiki/Goodhart%27s_law\" title=\"Goodhart's law\">Goodhart's law</a> in economics and other areas.<sup class=\"reference\" id=\"cite_ref-:1_41-1\"><a href=\"#cite_note-:1-41\">[38]</a></sup><sup class=\"reference\" id=\"cite_ref-87\"><a href=\"#cite_note-87\">[79]</a></sup> Researchers aim to specify the intended behavior as completely as possible with “values-targeted” datasets, imitation learning, or preference learning.<sup class=\"reference\" id=\"cite_ref-:224_88-0\"><a href=\"#cite_note-:224-88\">[80]</a></sup> A central open problem is <i>scalable oversight</i>, the difficulty of supervising an AI system that outperforms humans in a given domain.<sup class=\"reference\" id=\"cite_ref-:110_19-5\"><a href=\"#cite_note-:110-19\">[16]</a></sup>\n</p><p>When training a goal-directed AI system, such as a <a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">reinforcement learning</a> (RL) agent, it is often difficult to specify the intended behavior by writing a <a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">reward function</a> manually. An alternative is imitation learning, where the AI learns to imitate demonstrations of the desired behavior. In inverse reinforcement learning (IRL), human demonstrations are used to identify the objective, i.e. the reward function, behind the demonstrated behavior.<sup class=\"reference\" id=\"cite_ref-89\"><a href=\"#cite_note-89\">[81]</a></sup><sup class=\"reference\" id=\"cite_ref-90\"><a href=\"#cite_note-90\">[82]</a></sup> Cooperative inverse reinforcement learning (CIRL) builds on this by assuming a human agent and artificial agent can work together to maximize the human’s reward function.<sup class=\"reference\" id=\"cite_ref-:210_6-10\"><a href=\"#cite_note-:210-6\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-91\"><a href=\"#cite_note-91\">[83]</a></sup> CIRL  emphasizes that AI agents should be uncertain about the reward function. This humility can help mitigate specification gaming as well as power-seeking tendencies (see <a href=\"#Power-seeking_and_instrumental_goals\">§ Power-Seeking</a>).<sup class=\"reference\" id=\"cite_ref-:242_63-1\"><a href=\"#cite_note-:242-63\">[59]</a></sup><sup class=\"reference\" id=\"cite_ref-:1124_80-1\"><a href=\"#cite_note-:1124-80\">[72]</a></sup> However, inverse reinforcement learning approaches assume that humans can demonstrate nearly perfect behavior, a misleading assumption when the task is difficult.<sup class=\"reference\" id=\"cite_ref-92\"><a href=\"#cite_note-92\">[84]</a></sup><sup class=\"reference\" id=\"cite_ref-:1124_80-2\"><a href=\"#cite_note-:1124-80\">[72]</a></sup>\n</p><p>Other researchers have explored the possibility of eliciting complex behavior through preference learning. Rather than providing expert demonstrations, human annotators provide feedback on which of two or more of the AI’s behaviors they prefer.<sup class=\"reference\" id=\"cite_ref-:122_23-1\"><a href=\"#cite_note-:122-23\">[20]</a></sup><sup class=\"reference\" id=\"cite_ref-:53_25-1\"><a href=\"#cite_note-:53-25\">[22]</a></sup> A helper model is then trained to predict human feedback for new behaviors. Researchers at OpenAI used this approach to train an agent to perform a backflip in less than an hour of evaluation, a maneuver that would have been hard to provide demonstrations for.<sup class=\"reference\" id=\"cite_ref-:143_42-2\"><a href=\"#cite_note-:143-42\">[39]</a></sup><sup class=\"reference\" id=\"cite_ref-93\"><a href=\"#cite_note-93\">[85]</a></sup> Preference learning has also been an influential tool for recommender systems, web search, and information retrieval.<sup class=\"reference\" id=\"cite_ref-94\"><a href=\"#cite_note-94\">[86]</a></sup> However, one challenge is <i>reward hacking</i>: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch.<sup class=\"reference\" id=\"cite_ref-:110_19-6\"><a href=\"#cite_note-:110-19\">[16]</a></sup><sup class=\"reference\" id=\"cite_ref-95\"><a href=\"#cite_note-95\">[87]</a></sup>\n</p><p>The arrival of large language models such as GPT-3 has enabled the study of value learning in a more general and capable class of AI systems than was available before. Preference learning approaches originally designed for RL agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art large language models.<sup class=\"reference\" id=\"cite_ref-:42_12-1\"><a href=\"#cite_note-:42-12\">[10]</a></sup><sup class=\"reference\" id=\"cite_ref-:53_25-2\"><a href=\"#cite_note-:53-25\">[22]</a></sup><sup class=\"reference\" id=\"cite_ref-96\"><a href=\"#cite_note-96\">[88]</a></sup> Anthropic has proposed using preference learning to fine-tune models to be helpful, honest, and harmless.<sup class=\"reference\" id=\"cite_ref-:202_97-0\"><a href=\"#cite_note-:202-97\">[89]</a></sup> Other avenues used for aligning language models include values-targeted datasets<sup class=\"reference\" id=\"cite_ref-98\"><a href=\"#cite_note-98\">[90]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-13\"><a href=\"#cite_note-:010-7\">[5]</a></sup> and red-teaming.<sup class=\"reference\" id=\"cite_ref-99\"><a href=\"#cite_note-99\">[91]</a></sup><sup class=\"reference\" id=\"cite_ref-100\"><a href=\"#cite_note-100\">[92]</a></sup> In red-teaming, another AI system or a human tries to find inputs for which the model’s behavior is unsafe. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.<sup class=\"reference\" id=\"cite_ref-:53_25-3\"><a href=\"#cite_note-:53-25\">[22]</a></sup>\n</p><p>While preference learning can instill hard-to-specify behaviors, it requires extensive datasets or human interaction to capture the full breadth of human values. <b><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></b> provides a complementary approach: instilling AI systems with moral values.<sup class=\"reference\" id=\"cite_ref-103\"><a href=\"#cite_note-103\">[i]</a></sup> For instance, machine ethics aims to teach the systems about normative factors in human morality, such as wellbeing, equality and impartiality; not intending harm; avoiding falsehoods; and honoring promises. Unlike specifying the objective for a specific task, machine ethics seeks to teach AI systems broad moral values that could apply in many situations. This approach carries conceptual challenges of its own; machine ethicists have noted the necessity to clarify what alignment aims to accomplish: having AIs follow the programmer’s literal instructions, the programmers' implicit intentions, the programmers' <a href=\"/wiki/Revealed_preference\" title=\"Revealed preference\">revealed preferences</a>, the preferences the programmers <a class=\"mw-redirect\" href=\"/wiki/Coherent_extrapolated_volition\" title=\"Coherent extrapolated volition\"><i>would</i> have</a> if they were more informed or rational, the programmers' <i>objective</i> interests, or <a href=\"/wiki/Moral_realism\" title=\"Moral realism\">objective moral standards</a>.<sup class=\"reference\" id=\"cite_ref-Gabriel2020_1-1\"><a href=\"#cite_note-Gabriel2020-1\">[1]</a></sup> Further challenges include aggregating the preferences of different stakeholders and avoiding <i>value lock-in</i>—the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to be fully representative.<sup class=\"reference\" id=\"cite_ref-Gabriel2020_1-2\"><a href=\"#cite_note-Gabriel2020-1\">[1]</a></sup><sup class=\"reference\" id=\"cite_ref-104\"><a href=\"#cite_note-104\">[95]</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Scalable_oversight\">Scalable oversight</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=9\" title=\"Edit section: Scalable oversight\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>The alignment of AI systems through human supervision faces challenges in scaling up. As AI systems attempt increasingly complex tasks, it can be slow or infeasible for humans to evaluate them. Such tasks include summarizing books,<sup class=\"reference\" id=\"cite_ref-:172_105-0\"><a href=\"#cite_note-:172-105\">[96]</a></sup> producing statements that are not merely convincing but also true,<sup class=\"reference\" id=\"cite_ref-106\"><a href=\"#cite_note-106\">[97]</a></sup><sup class=\"reference\" id=\"cite_ref-:1322_43-1\"><a href=\"#cite_note-:1322-43\">[40]</a></sup><sup class=\"reference\" id=\"cite_ref-Naughton_107-0\"><a href=\"#cite_note-Naughton-107\">[98]</a></sup> writing code without subtle bugs<sup class=\"reference\" id=\"cite_ref-:113_13-1\"><a href=\"#cite_note-:113-13\">[11]</a></sup> or security vulnerabilities, and predicting long-term outcomes such as the climate and the results of a policy decision.<sup class=\"reference\" id=\"cite_ref-:133_108-0\"><a href=\"#cite_note-:133-108\">[99]</a></sup><sup class=\"reference\" id=\"cite_ref-109\"><a href=\"#cite_note-109\">[100]</a></sup> More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and detect when the AI’s solution is only seemingly convincing, humans require assistance or extensive time. <i>Scalable oversight</i> studies how to reduce the time needed for supervision as well as assist human supervisors.<sup class=\"reference\" id=\"cite_ref-:110_19-7\"><a href=\"#cite_note-:110-19\">[16]</a></sup>\n</p><p>AI researcher Paul Christiano argues that the owners of AI systems may continue to train AI using easy-to-evaluate proxy objectives since that is easier than solving scalable oversight and still profitable. Accordingly, this may lead to “a world that’s increasingly optimized for things [that are easy to measure] like making profits or getting users to click on buttons, or getting users to spend time on websites without being increasingly optimized for having good policies and heading in a trajectory that we’re happy with”.<sup class=\"reference\" id=\"cite_ref-110\"><a href=\"#cite_note-110\">[101]</a></sup>\n</p><p>One easy-to-measure objective is the score the supervisor assigns to the AI’s outputs. Some AI systems have discovered a shortcut to achieving high scores, by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective (see video of robot hand above<sup class=\"reference\" id=\"cite_ref-:143_42-3\"><a href=\"#cite_note-:143-42\">[39]</a></sup>). Some AI systems have also learned to recognize when they are being evaluated, and “play dead”, only to behave differently once evaluation ends.<sup class=\"reference\" id=\"cite_ref-111\"><a href=\"#cite_note-111\">[102]</a></sup> This deceptive form of specification gaming may become easier for AI systems that are more sophisticated<sup class=\"reference\" id=\"cite_ref-:1522_8-4\"><a href=\"#cite_note-:1522-8\">[6]</a></sup><sup class=\"reference\" id=\"cite_ref-:84_58-3\"><a href=\"#cite_note-:84-58\">[55]</a></sup>  and attempt more difficult-to-evaluate tasks. If advanced models are also capable planners, they could be able to obscure their deception from supervisors.<sup class=\"reference\" id=\"cite_ref-112\"><a href=\"#cite_note-112\">[103]</a></sup> In the automotive industry, <a href=\"/wiki/Volkswagen_emissions_scandal\" title=\"Volkswagen emissions scandal\">Volkswagen engineers obscured</a> their cars’ emissions in laboratory testing, underscoring that deception of evaluators is a common pattern in the real world.<sup class=\"reference\" id=\"cite_ref-:010_7-14\"><a href=\"#cite_note-:010-7\">[5]</a></sup>\n</p><p>Approaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed.<sup class=\"reference\" id=\"cite_ref-:110_19-8\"><a href=\"#cite_note-:110-19\">[16]</a></sup> Another approach is to train a helper model (‘reward model’) to imitate the supervisor’s judgment.<sup class=\"reference\" id=\"cite_ref-:110_19-9\"><a href=\"#cite_note-:110-19\">[16]</a></sup><sup class=\"reference\" id=\"cite_ref-:162_24-1\"><a href=\"#cite_note-:162-24\">[21]</a></sup><sup class=\"reference\" id=\"cite_ref-:53_25-4\"><a href=\"#cite_note-:53-25\">[22]</a></sup><sup class=\"reference\" id=\"cite_ref-113\"><a href=\"#cite_note-113\">[104]</a></sup>\n</p><p>However, when the task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is not sufficient to reduce the quantity of supervision needed. To increase supervision <i>quality</i>, a range of approaches aim to assist the supervisor, sometimes using AI assistants. Iterated Amplification is an approach developed by Christiano that iteratively builds a feedback signal for challenging problems by using humans to combine solutions to easier subproblems.<sup class=\"reference\" id=\"cite_ref-:224_88-1\"><a href=\"#cite_note-:224-88\">[80]</a></sup><sup class=\"reference\" id=\"cite_ref-:133_108-1\"><a href=\"#cite_note-:133-108\">[99]</a></sup> Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them.<sup class=\"reference\" id=\"cite_ref-:172_105-1\"><a href=\"#cite_note-:172-105\">[96]</a></sup><sup class=\"reference\" id=\"cite_ref-114\"><a href=\"#cite_note-114\">[105]</a></sup> Another proposal is to train aligned AI by means of debate between AI systems, with the winner judged by humans.<sup class=\"reference\" id=\"cite_ref-115\"><a href=\"#cite_note-115\">[106]</a></sup><sup class=\"reference\" id=\"cite_ref-:1124_80-3\"><a href=\"#cite_note-:1124-80\">[72]</a></sup> Such debate is intended to reveal the weakest points of an answer to a complex question, and reward the AI for truthful and safe answers.\n</p>\n<h3><span class=\"mw-headline\" id=\"Honest_AI\">Honest AI</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=10\" title=\"Edit section: Honest AI\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:368px;\"><a class=\"image\" href=\"/wiki/File:GPT-3_falsehoods.png\"><img alt=\"\" class=\"thumbimage\" data-file-height=\"1300\" data-file-width=\"1360\" decoding=\"async\" height=\"350\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9b/GPT-3_falsehoods.png/366px-GPT-3_falsehoods.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9b/GPT-3_falsehoods.png/549px-GPT-3_falsehoods.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/9b/GPT-3_falsehoods.png/732px-GPT-3_falsehoods.png 2x\" width=\"366\"/></a> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:GPT-3_falsehoods.png\" title=\"Enlarge\"></a></div>Language models like <a href=\"/wiki/GPT-3\" title=\"GPT-3\">GPT-3</a> often generate falsehoods.<sup class=\"reference\" id=\"cite_ref-:182_116-0\"><a href=\"#cite_note-:182-116\">[107]</a></sup></div></div></div>\n<p>A growing area of research in AI alignment focuses on ensuring that AI is honest and truthful. Researchers from the Future of Humanity Institute point out that the development of language models such as GPT-3, which can generate fluent and grammatically correct text,<sup class=\"reference\" id=\"cite_ref-117\"><a href=\"#cite_note-117\">[108]</a></sup><sup class=\"reference\" id=\"cite_ref-118\"><a href=\"#cite_note-118\">[109]</a></sup> has opened the door to AI systems repeating falsehoods from their training data or even deliberately lying to humans.<sup class=\"reference\" id=\"cite_ref-:21_119-0\"><a href=\"#cite_note-:21-119\">[110]</a></sup><sup class=\"reference\" id=\"cite_ref-:182_116-1\"><a href=\"#cite_note-:182-116\">[107]</a></sup>\n</p><p>Current state-of-the-art language models learn by imitating human writing across millions of books worth of text from the Internet.<sup class=\"reference\" id=\"cite_ref-:625_11-6\"><a href=\"#cite_note-:625-11\">[9]</a></sup><sup class=\"reference\" id=\"cite_ref-120\"><a href=\"#cite_note-120\">[111]</a></sup> While this helps them learn a wide range of skills, the training data also includes common misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on this data learn to mimic false statements.<sup class=\"reference\" id=\"cite_ref-:182_116-2\"><a href=\"#cite_note-:182-116\">[107]</a></sup><sup class=\"reference\" id=\"cite_ref-Naughton_107-1\"><a href=\"#cite_note-Naughton-107\">[98]</a></sup><sup class=\"reference\" id=\"cite_ref-:1322_43-2\"><a href=\"#cite_note-:1322-43\">[40]</a></sup> Additionally, models often obediently continue falsehoods when prompted, generate empty explanations for their answers, or produce outright fabrications.<sup class=\"reference\" id=\"cite_ref-:1922_36-1\"><a href=\"#cite_note-:1922-36\">[33]</a></sup> For example, when prompted to write a biography for a real AI researcher, a chatbot confabulated numerous details about their life, which the researcher identified as false.<sup class=\"reference\" id=\"cite_ref-121\"><a href=\"#cite_note-121\">[112]</a></sup>\n</p><p>To combat the lack of truthfulness exhibited by modern AI systems, researchers have explored several directions. AI research organizations including OpenAI and DeepMind have developed AI systems that can cite their sources and explain their reasoning when answering questions, enabling better transparency and verifiability.<sup class=\"reference\" id=\"cite_ref-122\"><a href=\"#cite_note-122\">[113]</a></sup><sup class=\"reference\" id=\"cite_ref-123\"><a href=\"#cite_note-123\">[114]</a></sup><sup class=\"reference\" id=\"cite_ref-124\"><a href=\"#cite_note-124\">[115]</a></sup> Researchers from OpenAI and Anthropic have proposed using human feedback and curated datasets to fine-tune AI assistants to avoid negligent falsehoods or express when they are uncertain.<sup class=\"reference\" id=\"cite_ref-:53_25-5\"><a href=\"#cite_note-:53-25\">[22]</a></sup><sup class=\"reference\" id=\"cite_ref-125\"><a href=\"#cite_note-125\">[116]</a></sup><sup class=\"reference\" id=\"cite_ref-:202_97-1\"><a href=\"#cite_note-:202-97\">[89]</a></sup> Alongside technical solutions, researchers have argued for defining clear truthfulness standards and the creation of institutions, regulatory bodies, or watchdog agencies to evaluate AI systems on these standards before and during deployment.<sup class=\"reference\" id=\"cite_ref-:21_119-1\"><a href=\"#cite_note-:21-119\">[110]</a></sup>\n</p><p>Researchers distinguish truthfulness, which specifies that AIs only make statements that are objectively true, and honesty, which is the property that AIs only assert what they believe to be true. Recent research finds that state-of-the-art AI systems cannot be said to hold stable beliefs, so it is not yet tractable to study the honesty of AI systems.<sup class=\"reference\" id=\"cite_ref-126\"><a href=\"#cite_note-126\">[117]</a></sup> However, there is substantial concern that future AI systems that do hold beliefs could intentionally lie to humans. In extreme cases, a misaligned AI could deceive its operators into thinking it was safe or persuade them that nothing is amiss.<sup class=\"reference\" id=\"cite_ref-:75_9-6\"><a href=\"#cite_note-:75-9\">[7]</a></sup><sup class=\"reference\" id=\"cite_ref-:625_11-7\"><a href=\"#cite_note-:625-11\">[9]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-15\"><a href=\"#cite_note-:010-7\">[5]</a></sup> Some argue that if AIs could be made to assert only what they believe to be true, this would sidestep numerous problems in alignment.<sup class=\"reference\" id=\"cite_ref-:21_119-2\"><a href=\"#cite_note-:21-119\">[110]</a></sup><sup class=\"reference\" id=\"cite_ref-127\"><a href=\"#cite_note-127\">[118]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Inner_alignment_and_emergent_goals\">Inner alignment and emergent goals</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=11\" title=\"Edit section: Inner alignment and emergent goals\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Alignment research aims to line up three different descriptions of an AI system:<sup class=\"reference\" id=\"cite_ref-128\"><a href=\"#cite_note-128\">[119]</a></sup>\n</p>\n<ol><li><i>Intended goals</i> ('wishes'): “the hypothetical (but hard to articulate) description of an ideal AI system that is fully aligned to the desires of the human operator”;</li>\n<li><i>Specified goals</i> (or ‘outer specification’): The goals we actually specify — typically jointly through an objective function and a dataset;</li>\n<li><i>Emergent goals</i> (or ‘inner specification’): The goals the AI actually advances.</li></ol>\n<p>‘Outer misalignment’ is a mismatch between the intended goals (1) and the specified goals (2), whereas ‘inner misalignment’ is a mismatch between the human-specified goals (2) and the AI's emergent goals (3).\n</p><p>Inner misalignment is often explained by analogy to biological evolution.<sup class=\"reference\" id=\"cite_ref-129\"><a href=\"#cite_note-129\">[120]</a></sup> In the ancestral environment, evolution selected human genes for inclusive <a href=\"/wiki/Inclusive_fitness\" title=\"Inclusive fitness\">genetic fitness</a>, but humans evolved to have other objectives. Fitness corresponds to (2), the specified goal used in the training environment and training data. In evolutionary history, maximizing the fitness specification led to intelligent agents, humans, that do not directly pursue inclusive genetic fitness. Instead, they pursue emergent goals (3) that correlated with genetic fitness in the ancestral environment: nutrition, sex, and so on. However, our environment has changed — a <a href=\"/wiki/Domain_adaptation\" title=\"Domain adaptation\">distribution shift</a> has occurred. Humans still pursue their emergent goals, but this no longer maximizes genetic fitness. (In machine learning the analogous problem is known as <i>goal</i> <i>misgeneralization</i>.<sup class=\"reference\" id=\"cite_ref-goal_misgen_4-2\"><a href=\"#cite_note-goal_misgen-4\">[3]</a></sup>) Our taste for sugary food (an emergent goal) was originally beneficial, but now leads to overeating and health problems. Also, by using contraception, humans directly contradict genetic fitness. By analogy, if genetic fitness were the objective chosen by an AI developer, they would observe the model behaving as intended in the training environment, without noticing that the model is pursuing an unintended emergent goal until the model was deployed.\n</p><p>Research directions to detect and remove misaligned emergent goals include red teaming, verification, anomaly detection, and interpretability.<sup class=\"reference\" id=\"cite_ref-:110_19-10\"><a href=\"#cite_note-:110-19\">[16]</a></sup><sup class=\"reference\" id=\"cite_ref-:010_7-16\"><a href=\"#cite_note-:010-7\">[5]</a></sup><sup class=\"reference\" id=\"cite_ref-:2323_20-3\"><a href=\"#cite_note-:2323-20\">[17]</a></sup> Progress on these techniques may help reduce two open problems. Firstly, emergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time until its misalignment is detected. Such high stakes are common in autonomous driving, health care, and military applications.<sup class=\"reference\" id=\"cite_ref-130\"><a href=\"#cite_note-130\">[121]</a></sup> The stakes become higher yet when AI systems gain more autonomy and capability, becoming capable of sidestepping human interventions (see <a href=\"#Power-seeking_and_instrumental_goals\">§ Power-seeking and instrumental goals</a>). Secondly, a sufficiently capable AI system may take actions that falsely convince the human supervisor that the AI is pursuing the intended objective (see previous discussion on deception at <a href=\"#Scalable_oversight\">§ Scalable oversight</a>).\n</p>\n<h3><span class=\"mw-headline\" id=\"Power-seeking_and_instrumental_goals\">Power-seeking and instrumental goals</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=12\" title=\"Edit section: Power-seeking and instrumental goals\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Since the 1950s, AI researchers have sought to build advanced AI systems that can achieve goals by predicting the results of their actions and making long-term plans.<sup class=\"reference\" id=\"cite_ref-131\"><a href=\"#cite_note-131\">[122]</a></sup> However, some researchers argue that suitably advanced planning systems will default to seeking power over their environment, including over humans — for example by evading shutdown and acquiring resources. This power-seeking behavior is not explicitly programmed but emerges because power is instrumental for achieving a wide range of goals.<sup class=\"reference\" id=\"cite_ref-:2522_64-2\"><a href=\"#cite_note-:2522-64\">[60]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-11\"><a href=\"#cite_note-:210-6\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-7\"><a href=\"#cite_note-:75-9\">[7]</a></sup> Power-seeking is thus considered a <a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\"><i>convergent instrumental goal</i></a>.<sup class=\"reference\" id=\"cite_ref-:84_58-4\"><a href=\"#cite_note-:84-58\">[55]</a></sup>\n</p><p>Power-seeking is uncommon in current systems, but advanced systems that can foresee the long-term results of their actions may increasingly seek power. This was shown in formal work which found that optimal <a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">reinforcement learning</a> agents will seek power by seeking ways to gain more options, a behavior that persists across a wide range of environments and goals.<sup class=\"reference\" id=\"cite_ref-:2522_64-3\"><a href=\"#cite_note-:2522-64\">[60]</a></sup>\n</p><p>Power-seeking already emerges in some present systems. <a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">Reinforcement learning</a> systems have gained more options by acquiring and protecting resources, sometimes in ways their designers did not intend.<sup class=\"reference\" id=\"cite_ref-quanta-hide-seek_59-1\"><a href=\"#cite_note-quanta-hide-seek-59\">[56]</a></sup><sup class=\"reference\" id=\"cite_ref-132\"><a href=\"#cite_note-132\">[123]</a></sup> Other systems have learned, in toy environments, that in order to achieve their goal, they can prevent human interference<sup class=\"reference\" id=\"cite_ref-:103_61-1\"><a href=\"#cite_note-:103-61\">[57]</a></sup> or disable their off-switch.<sup class=\"reference\" id=\"cite_ref-:242_63-2\"><a href=\"#cite_note-:242-63\">[59]</a></sup> <a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Russell</a> illustrated this behavior by imagining a robot that is tasked to fetch coffee and evades being turned off since \"you can't fetch the coffee if you're dead\".<sup class=\"reference\" id=\"cite_ref-:210_6-12\"><a href=\"#cite_note-:210-6\">[4]</a></sup>\n</p><p>\nHypothesized ways to gain options include AI systems trying to:</p><blockquote><p>“<i>... break out of a contained environment; hack; get access to financial resources, or additional computing resources; make backup copies of themselves; gain unauthorized capabilities, sources of information, or channels of influence; mislead/lie to humans about their goals; resist or manipulate attempts to monitor/understand their behavior ... impersonate humans; cause humans to do things for them; ... manipulate human discourse and politics; weaken various human institutions and response capacities; take control of physical infrastructure like factories or scientific laboratories; cause certain types of technology and infrastructure to be developed; or directly harm/overpower humans.</i>”<sup class=\"reference\" id=\"cite_ref-:75_9-8\"><a href=\"#cite_note-:75-9\">[7]</a></sup></p></blockquote><p>Researchers aim to train systems that are 'corrigible': systems that do not seek power and allow themselves to be turned off, modified, etc. An unsolved challenge is <i>reward hacking</i>: when researchers penalize a system for seeking power, the system is incentivized to seek power in difficult-to-detect ways.<sup class=\"reference\" id=\"cite_ref-:010_7-17\"><a href=\"#cite_note-:010-7\">[5]</a></sup> To detect such covert behavior, researchers aim to create techniques and tools to inspect AI models<sup class=\"reference\" id=\"cite_ref-:010_7-18\"><a href=\"#cite_note-:010-7\">[5]</a></sup> and interpret the inner workings of <a href=\"/wiki/Black_box\" title=\"Black box\">black-box</a> models such as neural networks.\n</p><p>Additionally, researchers propose to solve the problem of systems disabling their off-switches by making AI agents uncertain about the objective they are pursuing.<sup class=\"reference\" id=\"cite_ref-:242_63-3\"><a href=\"#cite_note-:242-63\">[59]</a></sup><sup class=\"reference\" id=\"cite_ref-:210_6-13\"><a href=\"#cite_note-:210-6\">[4]</a></sup> Agents designed in this way would allow humans to turn them off, since this would indicate that the agent was wrong about the value of whatever action they were taking prior to being shut down. More research is needed to translate this insight into usable systems.<sup class=\"reference\" id=\"cite_ref-:224_88-2\"><a href=\"#cite_note-:224-88\">[80]</a></sup>\n</p><p>Power-seeking AI is thought to pose unusual risks. Ordinary safety-critical systems like planes and bridges are not <i>adversarial</i>. They lack the ability and incentive to evade safety measures and appear safer than they are. In contrast, power-seeking AI has been compared to a hacker that evades security measures.<sup class=\"reference\" id=\"cite_ref-:75_9-9\"><a href=\"#cite_note-:75-9\">[7]</a></sup> Further, ordinary technologies can be made safe through trial-and-error, unlike power-seeking AI which has been compared to a virus whose release is irreversible since it continuously evolves and grows in numbers—potentially at a faster pace than human society, eventually leading to the disempowerment or extinction of humans.<sup class=\"reference\" id=\"cite_ref-:75_9-10\"><a href=\"#cite_note-:75-9\">[7]</a></sup> It is therefore often argued that the alignment problem must be solved early, before advanced power-seeking AI is created.<sup class=\"reference\" id=\"cite_ref-:84_58-5\"><a href=\"#cite_note-:84-58\">[55]</a></sup>\n</p><p>However, some critics have argued that power-seeking is not inevitable, since humans do not always seek power and may only do so for evolutionary reasons. Furthermore, there is debate whether any future AI systems need to pursue goals and make long-term plans at all.<sup class=\"reference\" id=\"cite_ref-133\"><a href=\"#cite_note-133\">[124]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-11\"><a href=\"#cite_note-:75-9\">[7]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Embedded_agency\">Embedded agency</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=13\" title=\"Edit section: Embedded agency\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Work on scalable oversight largely occurs within formalisms such as <a href=\"/wiki/Partially_observable_Markov_decision_process\" title=\"Partially observable Markov decision process\">POMDPs</a>. Existing formalisms assume that the agent's algorithm is executed outside the environment (i.e. not physically embedded in it). Embedded agency<sup class=\"reference\" id=\"cite_ref-lit_review_134-0\"><a href=\"#cite_note-lit_review-134\">[125]</a></sup><sup class=\"reference\" id=\"cite_ref-135\"><a href=\"#cite_note-135\">[126]</a></sup> is another major strand of research which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build. For example, even if the scalable oversight problem is solved, an agent which is able to gain access to the computer it is running on may still have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it.<sup class=\"reference\" id=\"cite_ref-causal_influence_136-0\"><a href=\"#cite_note-causal_influence-136\">[127]</a></sup> A list of examples of specification gaming from <a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a> researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.<sup class=\"reference\" id=\"cite_ref-DM_specification_gaming_137-0\"><a href=\"#cite_note-DM_specification_gaming-137\">[128]</a></sup> This class of problems has been formalised using causal incentive diagrams.<sup class=\"reference\" id=\"cite_ref-causal_influence_136-1\"><a href=\"#cite_note-causal_influence-136\">[127]</a></sup>  Researchers at <a href=\"/wiki/University_of_Oxford\" title=\"University of Oxford\">Oxford</a> and <a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a> have argued that such problematic behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly.<sup class=\"reference\" id=\"cite_ref-:3_138-0\"><a href=\"#cite_note-:3-138\">[129]</a></sup> They suggest a range of potential approaches to address this open problem.\n</p>\n<h2><span class=\"mw-headline\" id=\"Skepticism_of_AI_risk\">Skepticism of AI risk</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=14\" title=\"Edit section: Skepticism of AI risk\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">Main article: <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence#Skepticism\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence § Skepticism</a></div>\n<p>Against the above concerns, AI risk skeptics believe that <a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">superintelligence</a> poses little to no risk of dangerous misbehavior. Such skeptics often believe that controlling a superintelligent AI will be trivial. Some skeptics,<sup class=\"reference\" id=\"cite_ref-139\"><a href=\"#cite_note-139\">[130]</a></sup> such as <a href=\"/wiki/Gary_Marcus\" title=\"Gary Marcus\">Gary Marcus</a>,<sup class=\"reference\" id=\"cite_ref-140\"><a href=\"#cite_note-140\">[131]</a></sup> propose adopting rules similar to the fictional <a href=\"/wiki/Three_Laws_of_Robotics\" title=\"Three Laws of Robotics\">Three Laws of Robotics</a> which directly specify a desired outcome (\"direct normativity\"). By contrast, most endorsers of the existential risk thesis (as well as many skeptics) consider the Three Laws to be unhelpful, due to those three laws being ambiguous and self-contradictory. (Other \"direct normativity\" proposals include Kantian ethics, utilitarianism, or a mix of some small list of enumerated desiderata.) Most risk endorsers believe instead that human values (and their quantitative trade-offs) are too complex and poorly-understood to be directly programmed into a superintelligence; instead, a superintelligence would need to be programmed with a <i>process</i> for acquiring and fully understanding human values (\"indirect normativity\"), such as <a class=\"mw-redirect\" href=\"/wiki/Coherent_extrapolated_volition\" title=\"Coherent extrapolated volition\">coherent extrapolated volition</a>.<sup class=\"reference\" id=\"cite_ref-AGIResponses_141-0\"><a href=\"#cite_note-AGIResponses-141\">[132]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Public_policy\">Public policy</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=15\" title=\"Edit section: Public policy\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a href=\"/wiki/Regulation_of_artificial_intelligence\" title=\"Regulation of artificial intelligence\">Regulation of artificial intelligence</a></div>\n<p>A number of governmental and treaty organizations have made statements emphasizing the importance of AI alignment.\n</p><p>In September 2021, the <a href=\"/wiki/Secretary-General_of_the_United_Nations\" title=\"Secretary-General of the United Nations\">Secretary-General of the United Nations</a> issued a declaration which included a call to regulate AI to ensure it is \"aligned with shared global values.\"<sup class=\"reference\" id=\"cite_ref-142\"><a href=\"#cite_note-142\">[133]</a></sup>\n</p><p>That same month, the <a class=\"mw-redirect\" href=\"/wiki/People%27s_Republic_of_China\" title=\"People's Republic of China\">PRC</a> published ethical guidelines for the use of AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.<sup class=\"reference\" id=\"cite_ref-143\"><a href=\"#cite_note-143\">[134]</a></sup>\n</p><p>Also in September 2021, the <a class=\"mw-redirect\" href=\"/wiki/UK\" title=\"UK\">UK</a> published its 10-year National AI Strategy,<sup class=\"reference\" id=\"cite_ref-144\"><a href=\"#cite_note-144\">[135]</a></sup> which states the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".<sup class=\"reference\" id=\"cite_ref-145\"><a href=\"#cite_note-145\">[136]</a></sup> The strategy describes actions to assess long term AI risks, including catastrophic risks.<sup class=\"reference\" id=\"cite_ref-146\"><a href=\"#cite_note-146\">[137]</a></sup>\n</p><p>In March 2021, the US National Security Commission on Artificial Intelligence released stated that \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"<sup class=\"reference\" id=\"cite_ref-147\"><a href=\"#cite_note-147\">[138]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=16\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/AI_capability_control\" title=\"AI capability control\">AI capability control</a></li>\n<li><a href=\"/wiki/Regulation_of_artificial_intelligence\" title=\"Regulation of artificial intelligence\">Regulation of artificial intelligence</a></li>\n<li><a href=\"/wiki/Artificial_wisdom\" title=\"Artificial wisdom\">Artificial wisdom</a></li>\n<li><a href=\"/wiki/HAL_9000\" title=\"HAL 9000\">HAL 9000</a></li>\n<li><a href=\"/wiki/Multivac\" title=\"Multivac\">Multivac</a></li>\n<li><a href=\"/wiki/Open_Letter_on_Artificial_Intelligence\" title=\"Open Letter on Artificial Intelligence\">Open Letter on Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Toronto_Declaration\" title=\"Toronto Declaration\">Toronto Declaration</a></li>\n<li><a href=\"/wiki/Asilomar_Conference_on_Beneficial_AI\" title=\"Asilomar Conference on Beneficial AI\">Asilomar Conference on Beneficial AI</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"Footnotes\">Footnotes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=17\" title=\"Edit section: Footnotes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r1011085734\">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class=\"reflist reflist-lower-alpha\">\n<div class=\"mw-references-wrap\"><ol class=\"references\">\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\">Other definitions of AI alignment require that the AI system advances more general goals such as human values, other ethical principles, or the intentions its designers would have if they were more informed and enlightened.<sup class=\"reference\" id=\"cite_ref-Gabriel2020_1-0\"><a href=\"#cite_note-Gabriel2020-1\">[1]</a></sup> </span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\">See the textbook: Russel &amp; Norvig, <a href=\"/wiki/Artificial_Intelligence:_A_Modern_Approach\" title=\"Artificial Intelligence: A Modern Approach\">Artificial Intelligence: A Modern Approach</a>.<sup class=\"reference\" id=\"cite_ref-:92_3-0\"><a href=\"#cite_note-:92-3\">[2]</a></sup>\nThe distinction between misaligned AI and incompetent AI has been formalized in certain contexts.<sup class=\"reference\" id=\"cite_ref-goal_misgen_4-0\"><a href=\"#cite_note-goal_misgen-4\">[3]</a></sup></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\">The AI principles created at the <a href=\"/wiki/Asilomar_Conference_on_Beneficial_AI\" title=\"Asilomar Conference on Beneficial AI\">Asilomar Conference on Beneficial AI</a> were signed by 1797 AI/robotics researchers.<sup class=\"reference\" id=\"cite_ref-16\"><a href=\"#cite_note-16\">[14]</a></sup> Further, the UN Secretary-General’s report “Our Common Agenda“,<sup class=\"reference\" id=\"cite_ref-17\"><a href=\"#cite_note-17\">[15]</a></sup> notes: “[T]he Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values\" and discusses <a class=\"mw-redirect\" href=\"/wiki/Global_catastrophic_risks\" title=\"Global catastrophic risks\">global catastrophic risks</a> from technological developments.</span>\n</li>\n<li id=\"cite_note-60\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-60\">^</a></b></span> <span class=\"reference-text\">Reinforcement learning systems have learned to gain more options by acquiring and protecting resources, sometimes in ways their designers did not intend.<sup class=\"reference\" id=\"cite_ref-quanta-hide-seek_59-0\"><a href=\"#cite_note-quanta-hide-seek-59\">[56]</a></sup><sup class=\"reference\" id=\"cite_ref-:75_9-3\"><a href=\"#cite_note-:75-9\">[7]</a></sup></span>\n</li>\n<li id=\"cite_note-67\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-67\">^</a></b></span> <span class=\"reference-text\">In a 1951 lecture<sup class=\"reference\" id=\"cite_ref-65\"><a href=\"#cite_note-65\">[61]</a></sup> Turing argued that “It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler’s Erewhon.” Also in a lecture broadcast on BBC<sup class=\"reference\" id=\"cite_ref-66\"><a href=\"#cite_note-66\">[62]</a></sup> expressed: \"If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled. . . . This new danger . . . is certainly something which can give us anxiety.”</span>\n</li>\n<li id=\"cite_note-70\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-70\">^</a></b></span> <span class=\"reference-text\">About the book <i>Human Compatible: AI and the Problem of Control</i>, Bengio said \"This beautifully written book addresses a fundamental challenge for humanity: increasingly intelligent machines that do what we ask but not what we really intend. Essential reading if you care about our future.\"<sup class=\"reference\" id=\"cite_ref-people.eecs.berkeley.edu_69-0\"><a href=\"#cite_note-people.eecs.berkeley.edu-69\">[64]</a></sup></span>\n</li>\n<li id=\"cite_note-71\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-71\">^</a></b></span> <span class=\"reference-text\">About the book <i>Human Compatible: AI and the Problem of Control</i>, Pearl said \"Human Compatible made me a convert to Russell's concerns with our ability to control our upcoming creation–super-intelligent machines. Unlike outside alarmists and futurists, Russell is a leading authority on AI. His new book will educate the public about AI more than any book I can think of, and is a delightful and uplifting read.\"<sup class=\"reference\" id=\"cite_ref-people.eecs.berkeley.edu_69-1\"><a href=\"#cite_note-people.eecs.berkeley.edu-69\">[64]</a></sup></span>\n</li>\n<li id=\"cite_note-74\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-74\">^</a></b></span> <span class=\"reference-text\">Russell &amp; Norvig<sup class=\"reference\" id=\"cite_ref-73\"><a href=\"#cite_note-73\">[66]</a></sup> note: “The “King Midas problem” was anticipated by Marvin Minsky, who once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers.\"</span>\n</li>\n<li id=\"cite_note-103\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-103\">^</a></b></span> <span class=\"reference-text\">About the book of Wendell Wallach and Colin Allen: <i>Moral machines: teaching robots right from wrong</i><sup class=\"reference\" id=\"cite_ref-101\"><a href=\"#cite_note-101\">[93]</a></sup> Vincent Wiegel says “we should extend [machines] with moral sensitivity to the moral dimensions of the situations in which the increasingly autonomous machines will inevitably find themselves.”<sup class=\"reference\" id=\"cite_ref-102\"><a href=\"#cite_note-102\">[94]</a></sup></span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=AI_alignment&amp;action=edit&amp;section=18\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<link href=\"mw-data:TemplateStyles:r1011085734\" rel=\"mw-deduplicated-inline-style\"/><div class=\"reflist\">\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-Gabriel2020-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Gabriel2020_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Gabriel2020_1-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Gabriel2020_1-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><style data-mw-deduplicate=\"TemplateStyles:r1067248974\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite class=\"citation journal cs1\" id=\"CITEREFGabriel2020\">Gabriel, Iason (September 1, 2020). <a class=\"external text\" href=\"https://doi.org/10.1007/s11023-020-09539-2\" rel=\"nofollow\">\"Artificial Intelligence, Values, and Alignment\"</a>. <i>Minds and Machines</i>. <b>30</b> (3): 411–437. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs11023-020-09539-2\" rel=\"nofollow\">10.1007/s11023-020-09539-2</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1572-8641\" rel=\"nofollow\">1572-8641</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:210920551\" rel=\"nofollow\">210920551</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Minds+and+Machines&amp;rft.atitle=Artificial+Intelligence%2C+Values%2C+and+Alignment&amp;rft.volume=30&amp;rft.issue=3&amp;rft.pages=411-437&amp;rft.date=2020-09-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A210920551%23id-name%3DS2CID&amp;rft.issn=1572-8641&amp;rft_id=info%3Adoi%2F10.1007%2Fs11023-020-09539-2&amp;rft.aulast=Gabriel&amp;rft.aufirst=Iason&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%2Fs11023-020-09539-2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:92-3\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:92_3-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:92_3-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:92_3-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:92_3-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:92_3-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:92_3-5\"><sup><i><b>f</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"Section_1.5\">Russell, Stuart J.; Norvig, Peter (2020). <a class=\"external text\" href=\"https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html\" rel=\"nofollow\"><i>Artificial intelligence: A modern approach</i></a> (4th ed.). Pearson. pp. 31–34. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-292-40113-3\" title=\"Special:BookSources/978-1-292-40113-3\"><bdi>978-1-292-40113-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1303900751\" rel=\"nofollow\">1303900751</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+intelligence%3A+A+modern+approach&amp;rft.pages=31-34&amp;rft.edition=4th&amp;rft.pub=Pearson&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1303900751&amp;rft.isbn=978-1-292-40113-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rft_id=https%3A%2F%2Fwww.pearson.com%2Fus%2Fhigher-education%2Fprogram%2FRussell-Artificial-Intelligence-A-Modern-Approach-4th-Edition%2FPGM1263338.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-goal_misgen-4\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-goal_misgen_4-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-goal_misgen_4-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-goal_misgen_4-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFLangoscoKochSharkeyPfau2022\">Langosco, Lauro Langosco Di; Koch, Jack; Sharkey, Lee D; Pfau, Jacob; Krueger, David (July 17, 2022). \"Goal misgeneralization in deep reinforcement learning\". <i>International Conference on Machine Learning</i>. Vol. 162. PMLR. pp. 12004–12019.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Goal+misgeneralization+in+deep+reinforcement+learning&amp;rft.btitle=International+Conference+on+Machine+Learning&amp;rft.pages=12004-12019&amp;rft.pub=PMLR&amp;rft.date=2022-07-17&amp;rft.aulast=Langosco&amp;rft.aufirst=Lauro+Langosco+Di&amp;rft.au=Koch%2C+Jack&amp;rft.au=Sharkey%2C+Lee+D&amp;rft.au=Pfau%2C+Jacob&amp;rft.au=Krueger%2C+David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:210-6\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:210_6-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:210_6-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:210_6-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:210_6-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:210_6-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:210_6-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-:210_6-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-:210_6-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-:210_6-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-:210_6-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-:210_6-10\"><sup><i><b>k</b></i></sup></a> <a href=\"#cite_ref-:210_6-11\"><sup><i><b>l</b></i></sup></a> <a href=\"#cite_ref-:210_6-12\"><sup><i><b>m</b></i></sup></a> <a href=\"#cite_ref-:210_6-13\"><sup><i><b>n</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRussell2020\">Russell, Stuart J. (2020). <a class=\"external text\" href=\"https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/\" rel=\"nofollow\"><i>Human compatible: Artificial intelligence and the problem of control</i></a>. Penguin Random House. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/9780525558637\" title=\"Special:BookSources/9780525558637\"><bdi>9780525558637</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1113410915\" rel=\"nofollow\">1113410915</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+compatible%3A+Artificial+intelligence+and+the+problem+of+control&amp;rft.pub=Penguin+Random+House&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1113410915&amp;rft.isbn=9780525558637&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft_id=https%3A%2F%2Fwww.penguinrandomhouse.com%2Fbooks%2F566677%2Fhuman-compatible-by-stuart-russell%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:010-7\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:010_7-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:010_7-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:010_7-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:010_7-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:010_7-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:010_7-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-:010_7-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-:010_7-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-:010_7-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-:010_7-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-:010_7-10\"><sup><i><b>k</b></i></sup></a> <a href=\"#cite_ref-:010_7-11\"><sup><i><b>l</b></i></sup></a> <a href=\"#cite_ref-:010_7-12\"><sup><i><b>m</b></i></sup></a> <a href=\"#cite_ref-:010_7-13\"><sup><i><b>n</b></i></sup></a> <a href=\"#cite_ref-:010_7-14\"><sup><i><b>o</b></i></sup></a> <a href=\"#cite_ref-:010_7-15\"><sup><i><b>p</b></i></sup></a> <a href=\"#cite_ref-:010_7-16\"><sup><i><b>q</b></i></sup></a> <a href=\"#cite_ref-:010_7-17\"><sup><i><b>r</b></i></sup></a> <a href=\"#cite_ref-:010_7-18\"><sup><i><b>s</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFHendrycksCarliniSchulmanSteinhardt2022\">Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob (June 16, 2022). \"Unsolved Problems in ML Safety\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2109.13916\" rel=\"nofollow\">2109.13916</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Unsolved+Problems+in+ML+Safety&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2109.13916&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Carlini%2C+Nicholas&amp;rft.au=Schulman%2C+John&amp;rft.au=Steinhardt%2C+Jacob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:1522-8\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1522_8-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1522_8-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:1522_8-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:1522_8-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:1522_8-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFPanBhatiaSteinhardt2022\">Pan, Alexander; Bhatia, Kush; Steinhardt, Jacob (February 14, 2022). <a class=\"external text\" href=\"https://openreview.net/forum?id=JYtwGwIL7ye\" rel=\"nofollow\"><i>The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models</i></a>. International Conference on Learning Representations<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 21,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=The+Effects+of+Reward+Misspecification%3A+Mapping+and+Mitigating+Misaligned+Models&amp;rft.date=2022-02-14&amp;rft.aulast=Pan&amp;rft.aufirst=Alexander&amp;rft.au=Bhatia%2C+Kush&amp;rft.au=Steinhardt%2C+Jacob&amp;rft_id=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DJYtwGwIL7ye&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:75-9\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:75_9-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:75_9-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:75_9-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:75_9-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:75_9-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:75_9-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-:75_9-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-:75_9-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-:75_9-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-:75_9-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-:75_9-10\"><sup><i><b>k</b></i></sup></a> <a href=\"#cite_ref-:75_9-11\"><sup><i><b>l</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFCarlsmith2022\">Carlsmith, Joseph (June 16, 2022). \"Is Power-Seeking AI an Existential Risk?\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2206.13353\" rel=\"nofollow\">2206.13353</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CY\" rel=\"nofollow\">cs.CY</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Is+Power-Seeking+AI+an+Existential+Risk%3F&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2206.13353&amp;rft.aulast=Carlsmith&amp;rft.aufirst=Joseph&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKoberBagnellPeters2013\">Kober, Jens; Bagnell, J. Andrew; Peters, Jan (September 1, 2013). <a class=\"external text\" href=\"http://journals.sagepub.com/doi/10.1177/0278364913495721\" rel=\"nofollow\">\"Reinforcement learning in robotics: A survey\"</a>. <i>The International Journal of Robotics Research</i>. <b>32</b> (11): 1238–1274. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1177%2F0278364913495721\" rel=\"nofollow\">10.1177/0278364913495721</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0278-3649\" rel=\"nofollow\">0278-3649</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:1932843\" rel=\"nofollow\">1932843</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+International+Journal+of+Robotics+Research&amp;rft.atitle=Reinforcement+learning+in+robotics%3A+A+survey&amp;rft.volume=32&amp;rft.issue=11&amp;rft.pages=1238-1274&amp;rft.date=2013-09-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1932843%23id-name%3DS2CID&amp;rft.issn=0278-3649&amp;rft_id=info%3Adoi%2F10.1177%2F0278364913495721&amp;rft.aulast=Kober&amp;rft.aufirst=Jens&amp;rft.au=Bagnell%2C+J.+Andrew&amp;rft.au=Peters%2C+Jan&amp;rft_id=http%3A%2F%2Fjournals.sagepub.com%2Fdoi%2F10.1177%2F0278364913495721&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:625-11\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:625_11-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:625_11-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:625_11-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:625_11-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:625_11-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:625_11-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-:625_11-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-:625_11-7\"><sup><i><b>h</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFBommasaniHudsonAdeliAltman2022\">Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Sydney; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; Brunskill, Emma; Brynjolfsson, Erik (July 12, 2022). <a class=\"external text\" href=\"https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models\" rel=\"nofollow\">\"On the Opportunities and Risks of Foundation Models\"</a>. <i>Stanford CRFM</i>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2108.07258\" rel=\"nofollow\">2108.07258</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Stanford+CRFM&amp;rft.atitle=On+the+Opportunities+and+Risks+of+Foundation+Models&amp;rft.date=2022-07-12&amp;rft_id=info%3Aarxiv%2F2108.07258&amp;rft.aulast=Bommasani&amp;rft.aufirst=Rishi&amp;rft.au=Hudson%2C+Drew+A.&amp;rft.au=Adeli%2C+Ehsan&amp;rft.au=Altman%2C+Russ&amp;rft.au=Arora%2C+Simran&amp;rft.au=von+Arx%2C+Sydney&amp;rft.au=Bernstein%2C+Michael+S.&amp;rft.au=Bohg%2C+Jeannette&amp;rft.au=Bosselut%2C+Antoine&amp;rft.au=Brunskill%2C+Emma&amp;rft.au=Brynjolfsson%2C+Erik&amp;rft_id=https%3A%2F%2Ffsi.stanford.edu%2Fpublication%2Fopportunities-and-risks-foundation-models&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:42-12\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:42_12-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:42_12-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFOuyangWuJiangAlmeida2022\">Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, J.; Hilton, Jacob; Kelton, Fraser; Miller, Luke E.; Simens, Maddie; Askell, Amanda; Welinder, P.; Christiano, P.; Leike, J.; Lowe, Ryan J. (2022). \"Training language models to follow instructions with human feedback\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2203.02155\" rel=\"nofollow\">2203.02155</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CL\" rel=\"nofollow\">cs.CL</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Training+language+models+to+follow+instructions+with+human+feedback&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2203.02155&amp;rft.aulast=Ouyang&amp;rft.aufirst=Long&amp;rft.au=Wu%2C+Jeff&amp;rft.au=Jiang%2C+Xu&amp;rft.au=Almeida%2C+Diogo&amp;rft.au=Wainwright%2C+Carroll+L.&amp;rft.au=Mishkin%2C+Pamela&amp;rft.au=Zhang%2C+Chong&amp;rft.au=Agarwal%2C+Sandhini&amp;rft.au=Slama%2C+Katarina&amp;rft.au=Ray%2C+Alex&amp;rft.au=Schulman%2C+J.&amp;rft.au=Hilton%2C+Jacob&amp;rft.au=Kelton%2C+Fraser&amp;rft.au=Miller%2C+Luke+E.&amp;rft.au=Simens%2C+Maddie&amp;rft.au=Askell%2C+Amanda&amp;rft.au=Welinder%2C+P.&amp;rft.au=Christiano%2C+P.&amp;rft.au=Leike%2C+J.&amp;rft.au=Lowe%2C+Ryan+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:113-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:113_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:113_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFZarembaBrockmanOpenAI2021\">Zaremba, Wojciech; Brockman, Greg; OpenAI (August 10, 2021). <a class=\"external text\" href=\"https://openai.com/blog/openai-codex/\" rel=\"nofollow\">\"OpenAI Codex\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=OpenAI+Codex&amp;rft.date=2021-08-10&amp;rft.aulast=Zaremba&amp;rft.aufirst=Wojciech&amp;rft.au=Brockman%2C+Greg&amp;rft.au=OpenAI&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fopenai-codex%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFKnoxAllieviBanzhafSchmitt2022\">Knox, W. Bradley; Allievi, Alessandro; Banzhaf, Holger; Schmitt, Felix; Stone, Peter (March 11, 2022). <a class=\"external text\" href=\"https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/knox2021reward.pdf\" rel=\"nofollow\">\"Reward (Mis)design for Autonomous Driving\"</a> <span class=\"cs1-format\">(PDF)</span>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2104.13906\" rel=\"nofollow\">2104.13906</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Reward+%28Mis%29design+for+Autonomous+Driving&amp;rft.date=2022-03-11&amp;rft_id=info%3Aarxiv%2F2104.13906&amp;rft.aulast=Knox&amp;rft.aufirst=W.+Bradley&amp;rft.au=Allievi%2C+Alessandro&amp;rft.au=Banzhaf%2C+Holger&amp;rft.au=Schmitt%2C+Felix&amp;rft.au=Stone%2C+Peter&amp;rft_id=https%3A%2F%2Fwww.cs.utexas.edu%2Fusers%2Fpstone%2FPapers%2Fbib2html-links%2Fknox2021reward.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span> <span class=\"cs1-hidden-error citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_journal\" title=\"Template:Cite journal\">cite journal</a>}}</code>: </span><span class=\"cs1-hidden-error citation-comment\">Cite journal requires <code class=\"cs1-code\">|journal=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFStray2020\">Stray, Jonathan (2020). <a class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC7610010\" rel=\"nofollow\">\"Aligning AI Optimization to Community Well-Being\"</a>. <i>International Journal of Community Well-Being</i>. <b>3</b> (4): 443–463. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs42413-020-00086-3\" rel=\"nofollow\">10.1007/s42413-020-00086-3</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/2524-5295\" rel=\"nofollow\">2524-5295</a>. <a class=\"mw-redirect\" href=\"/wiki/PMC_(identifier)\" title=\"PMC (identifier)\">PMC</a> <span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC7610010\" rel=\"nofollow\">7610010</a></span>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/34723107\" rel=\"nofollow\">34723107</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:226254676\" rel=\"nofollow\">226254676</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Community+Well-Being&amp;rft.atitle=Aligning+AI+Optimization+to+Community+Well-Being&amp;rft.volume=3&amp;rft.issue=4&amp;rft.pages=443-463&amp;rft.date=2020&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7610010%23id-name%3DPMC&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A226254676%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2Fs42413-020-00086-3&amp;rft.issn=2524-5295&amp;rft_id=info%3Apmid%2F34723107&amp;rft.aulast=Stray&amp;rft.aufirst=Jonathan&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7610010&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFFuture_of_Life_Institute2017\">Future of Life Institute (August 11, 2017). <a class=\"external text\" href=\"https://futureoflife.org/2017/08/11/ai-principles/\" rel=\"nofollow\">\"Asilomar AI Principles\"</a>. <i>Future of Life Institute</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=Asilomar+AI+Principles&amp;rft.date=2017-08-11&amp;rft.au=Future+of+Life+Institute&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2F2017%2F08%2F11%2Fai-principles%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation report cs1\" id=\"CITEREFUnited_Nations2021\">United Nations (2021). <a class=\"external text\" href=\"https://www.un.org/en/content/common-agenda-report/assets/pdf/Common_Agenda_Report_English.pdf\" rel=\"nofollow\">Our Common Agenda: Report of the Secretary-General</a> <span class=\"cs1-format\">(PDF)</span> (Report). New York: United Nations.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Our+Common+Agenda%3A+Report+of+the+Secretary-General&amp;rft.place=New+York&amp;rft.pub=United+Nations&amp;rft.date=2021&amp;rft.au=United+Nations&amp;rft_id=https%3A%2F%2Fwww.un.org%2Fen%2Fcontent%2Fcommon-agenda-report%2Fassets%2Fpdf%2FCommon_Agenda_Report_English.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:110-19\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:110_19-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:110_19-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:110_19-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:110_19-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:110_19-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:110_19-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-:110_19-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-:110_19-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-:110_19-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-:110_19-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-:110_19-10\"><sup><i><b>k</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFAmodeiOlahSteinhardtChristiano2016\">Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (June 21, 2016). \"Concrete Problems in AI Safety\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1606.06565\" rel=\"nofollow\">1606.06565</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.AI\" rel=\"nofollow\">cs.AI</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Concrete+Problems+in+AI+Safety&amp;rft.date=2016-06-21&amp;rft_id=info%3Aarxiv%2F1606.06565&amp;rft.aulast=Amodei&amp;rft.aufirst=Dario&amp;rft.au=Olah%2C+Chris&amp;rft.au=Steinhardt%2C+Jacob&amp;rft.au=Christiano%2C+Paul&amp;rft.au=Schulman%2C+John&amp;rft.au=Man%C3%A9%2C+Dan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:2323-20\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:2323_20-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:2323_20-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:2323_20-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:2323_20-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFOrtegaMainiDeepMind_safety_team2018\">Ortega, Pedro A.; Maini, Vishal; DeepMind safety team (September 27, 2018). <a class=\"external text\" href=\"https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1\" rel=\"nofollow\">\"Building safe artificial intelligence: specification, robustness, and assurance\"</a>. <i>DeepMind Safety Research - Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=DeepMind+Safety+Research+-+Medium&amp;rft.atitle=Building+safe+artificial+intelligence%3A+specification%2C+robustness%2C+and+assurance&amp;rft.date=2018-09-27&amp;rft.aulast=Ortega&amp;rft.aufirst=Pedro+A.&amp;rft.au=Maini%2C+Vishal&amp;rft.au=DeepMind+safety+team&amp;rft_id=https%3A%2F%2Fdeepmindsafetyresearch.medium.com%2Fbuilding-safe-artificial-intelligence-52f5f75058f1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:33-21\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:33_21-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:33_21-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFRorvig2022\">Rorvig, Mordechai (April 14, 2022). <a class=\"external text\" href=\"https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/\" rel=\"nofollow\">\"Researchers Gain New Understanding From Simple AI\"</a>. <i>Quanta Magazine</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quanta+Magazine&amp;rft.atitle=Researchers+Gain+New+Understanding+From+Simple+AI&amp;rft.date=2022-04-14&amp;rft.aulast=Rorvig&amp;rft.aufirst=Mordechai&amp;rft_id=https%3A%2F%2Fwww.quantamagazine.org%2Fresearchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:6-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:6_22-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFRussellDeweyTegmark2015\">Russell, Stuart; Dewey, Daniel; Tegmark, Max (December 31, 2015). <a class=\"external text\" href=\"https://ojs.aaai.org/index.php/aimagazine/article/view/2577\" rel=\"nofollow\">\"Research Priorities for Robust and Beneficial Artificial Intelligence\"</a>. <i>AI Magazine</i>. <b>36</b> (4): 105–114. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1609%2Faimag.v36i4.2577\" rel=\"nofollow\">10.1609/aimag.v36i4.2577</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/2371-9621\" rel=\"nofollow\">2371-9621</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:8174496\" rel=\"nofollow\">8174496</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+Magazine&amp;rft.atitle=Research+Priorities+for+Robust+and+Beneficial+Artificial+Intelligence&amp;rft.volume=36&amp;rft.issue=4&amp;rft.pages=105-114&amp;rft.date=2015-12-31&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8174496%23id-name%3DS2CID&amp;rft.issn=2371-9621&amp;rft_id=info%3Adoi%2F10.1609%2Faimag.v36i4.2577&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Dewey%2C+Daniel&amp;rft.au=Tegmark%2C+Max&amp;rft_id=https%3A%2F%2Fojs.aaai.org%2Findex.php%2Faimagazine%2Farticle%2Fview%2F2577&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:122-23\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:122_23-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:122_23-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFWirthAkrourNeumannFürnkranz2017\">Wirth, Christian; Akrour, Riad; Neumann, Gerhard; Fürnkranz, Johannes (2017). \"A survey of preference-based reinforcement learning methods\". <i>Journal of Machine Learning Research</i>. <b>18</b> (136): 1–46.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=A+survey+of+preference-based+reinforcement+learning+methods&amp;rft.volume=18&amp;rft.issue=136&amp;rft.pages=1-46&amp;rft.date=2017&amp;rft.aulast=Wirth&amp;rft.aufirst=Christian&amp;rft.au=Akrour%2C+Riad&amp;rft.au=Neumann%2C+Gerhard&amp;rft.au=F%C3%BCrnkranz%2C+Johannes&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:162-24\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:162_24-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:162_24-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFChristianoLeikeBrownMartic2017\">Christiano, Paul F.; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario (2017). \"Deep reinforcement learning from human preferences\". <i>Proceedings of the 31st International Conference on Neural Information Processing Systems</i>. NIPS'17. Red Hook, NY, USA: Curran Associates Inc. pp. 4302–4310. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-5108-6096-4\" title=\"Special:BookSources/978-1-5108-6096-4\"><bdi>978-1-5108-6096-4</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Deep+reinforcement+learning+from+human+preferences&amp;rft.btitle=Proceedings+of+the+31st+International+Conference+on+Neural+Information+Processing+Systems&amp;rft.place=Red+Hook%2C+NY%2C+USA&amp;rft.series=NIPS%2717&amp;rft.pages=4302-4310&amp;rft.pub=Curran+Associates+Inc.&amp;rft.date=2017&amp;rft.isbn=978-1-5108-6096-4&amp;rft.aulast=Christiano&amp;rft.aufirst=Paul+F.&amp;rft.au=Leike%2C+Jan&amp;rft.au=Brown%2C+Tom+B.&amp;rft.au=Martic%2C+Miljan&amp;rft.au=Legg%2C+Shane&amp;rft.au=Amodei%2C+Dario&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:53-25\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:53_25-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:53_25-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:53_25-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:53_25-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:53_25-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:53_25-5\"><sup><i><b>f</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFHeaven2022\">Heaven, Will Douglas (January 27, 2022). <a class=\"external text\" href=\"https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/\" rel=\"nofollow\">\"The new version of GPT-3 is much better behaved (and should be less toxic)\"</a>. <i>MIT Technology Review</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=The+new+version+of+GPT-3+is+much+better+behaved+%28and+should+be+less+toxic%29&amp;rft.date=2022-01-27&amp;rft.aulast=Heaven&amp;rft.aufirst=Will+Douglas&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2022%2F01%2F27%2F1044398%2Fnew-gpt3-openai-chatbot-language-model-ai-toxic-misinformation%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFMohseniWangYuXiao2022\">Mohseni, Sina; Wang, Haotao; Yu, Zhiding; Xiao, Chaowei; Wang, Zhangyang; Yadawa, Jay (March 7, 2022). \"Taxonomy of Machine Learning Safety: A Survey and Primer\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2106.04823\" rel=\"nofollow\">2106.04823</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Taxonomy+of+Machine+Learning+Safety%3A+A+Survey+and+Primer&amp;rft.date=2022-03-07&amp;rft_id=info%3Aarxiv%2F2106.04823&amp;rft.aulast=Mohseni&amp;rft.aufirst=Sina&amp;rft.au=Wang%2C+Haotao&amp;rft.au=Yu%2C+Zhiding&amp;rft.au=Xiao%2C+Chaowei&amp;rft.au=Wang%2C+Zhangyang&amp;rft.au=Yadawa%2C+Jay&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFClifton2020\">Clifton, Jesse (2020). <a class=\"external text\" href=\"https://longtermrisk.org/research-agenda/\" rel=\"nofollow\">\"Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda\"</a>. <i>Center on Long-Term Risk</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Center+on+Long-Term+Risk&amp;rft.atitle=Cooperation%2C+Conflict%2C+and+Transformative+Artificial+Intelligence%3A+A+Research+Agenda&amp;rft.date=2020&amp;rft.aulast=Clifton&amp;rft.aufirst=Jesse&amp;rft_id=https%3A%2F%2Flongtermrisk.org%2Fresearch-agenda%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFDafoeBachrachHadfieldHorvitz2021\">Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore (May 6, 2021). <a class=\"external text\" href=\"http://www.nature.com/articles/d41586-021-01170-0\" rel=\"nofollow\">\"Cooperative AI: machines must learn to find common ground\"</a>. <i>Nature</i>. <b>593</b> (7857): 33–36. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2021Natur.593...33D\" rel=\"nofollow\">2021Natur.593...33D</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1038%2Fd41586-021-01170-0\" rel=\"nofollow\">10.1038/d41586-021-01170-0</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0028-0836\" rel=\"nofollow\">0028-0836</a>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/33947992\" rel=\"nofollow\">33947992</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:233740521\" rel=\"nofollow\">233740521</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Cooperative+AI%3A+machines+must+learn+to+find+common+ground&amp;rft.volume=593&amp;rft.issue=7857&amp;rft.pages=33-36&amp;rft.date=2021-05-06&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233740521%23id-name%3DS2CID&amp;rft_id=info%3Abibcode%2F2021Natur.593...33D&amp;rft.issn=0028-0836&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-021-01170-0&amp;rft_id=info%3Apmid%2F33947992&amp;rft.aulast=Dafoe&amp;rft.aufirst=Allan&amp;rft.au=Bachrach%2C+Yoram&amp;rft.au=Hadfield%2C+Gillian&amp;rft.au=Horvitz%2C+Eric&amp;rft.au=Larson%2C+Kate&amp;rft.au=Graepel%2C+Thore&amp;rft_id=http%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-021-01170-0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFPrunklWhittlestone2020\">Prunkl, Carina; Whittlestone, Jess (February 7, 2020). <a class=\"external text\" href=\"https://dl.acm.org/doi/10.1145/3375627.3375803\" rel=\"nofollow\">\"Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society\"</a>. <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>. New York NY USA: ACM: 138–143. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1145%2F3375627.3375803\" rel=\"nofollow\">10.1145/3375627.3375803</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-4503-7110-0\" title=\"Special:BookSources/978-1-4503-7110-0\"><bdi>978-1-4503-7110-0</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:210164673\" rel=\"nofollow\">210164673</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society&amp;rft.atitle=Beyond+Near-+and+Long-Term%3A+Towards+a+Clearer+Account+of+Research+Priorities+in+AI+Ethics+and+Society&amp;rft.pages=138-143&amp;rft.date=2020-02-07&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A210164673%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3375627.3375803&amp;rft.isbn=978-1-4503-7110-0&amp;rft.aulast=Prunkl&amp;rft.aufirst=Carina&amp;rft.au=Whittlestone%2C+Jess&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3375627.3375803&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-30\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-30\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFIrvingAskell2019\">Irving, Geoffrey; Askell, Amanda (February 19, 2019). <a class=\"external text\" href=\"https://distill.pub/2019/safety-needs-social-scientists\" rel=\"nofollow\">\"AI Safety Needs Social Scientists\"</a>. <i>Distill</i>. <b>4</b> (2): 10.23915/distill.00014. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.23915%2Fdistill.00014\" rel=\"nofollow\">10.23915/distill.00014</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/2476-0757\" rel=\"nofollow\">2476-0757</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:159180422\" rel=\"nofollow\">159180422</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=AI+Safety+Needs+Social+Scientists&amp;rft.volume=4&amp;rft.issue=2&amp;rft.pages=10.23915%2Fdistill.00014&amp;rft.date=2019-02-19&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A159180422%23id-name%3DS2CID&amp;rft.issn=2476-0757&amp;rft_id=info%3Adoi%2F10.23915%2Fdistill.00014&amp;rft.aulast=Irving&amp;rft.aufirst=Geoffrey&amp;rft.au=Askell%2C+Amanda&amp;rft_id=https%3A%2F%2Fdistill.pub%2F2019%2Fsafety-needs-social-scientists&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:2-31\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:2_31-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:2_31-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\"><a class=\"external text\" href=\"https://openai.com/blog/faulty-reward-functions/\" rel=\"nofollow\">\"Faulty Reward Functions in the Wild\"</a>. <i>OpenAI</i>. December 22, 2016<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 10,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Faulty+Reward+Functions+in+the+Wild&amp;rft.date=2016-12-22&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Ffaulty-reward-functions%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:1023-32\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1023_32-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1023_32-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFWiener1960\">Wiener, Norbert (May 6, 1960). <a class=\"external text\" href=\"https://www.science.org/doi/10.1126/science.131.3410.1355\" rel=\"nofollow\">\"Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers\"</a>. <i>Science</i>. <b>131</b> (3410): 1355–1358. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1126%2Fscience.131.3410.1355\" rel=\"nofollow\">10.1126/science.131.3410.1355</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0036-8075\" rel=\"nofollow\">0036-8075</a>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/17841602\" rel=\"nofollow\">17841602</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Some+Moral+and+Technical+Consequences+of+Automation%3A+As+machines+learn+they+may+develop+unforeseen+strategies+at+rates+that+baffle+their+programmers.&amp;rft.volume=131&amp;rft.issue=3410&amp;rft.pages=1355-1358&amp;rft.date=1960-05-06&amp;rft.issn=0036-8075&amp;rft_id=info%3Apmid%2F17841602&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.131.3410.1355&amp;rft.aulast=Wiener&amp;rft.aufirst=Norbert&amp;rft_id=https%3A%2F%2Fwww.science.org%2Fdoi%2F10.1126%2Fscience.131.3410.1355&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFThe_Ezra_Klein_Show2021\">The Ezra Klein Show (June 4, 2021). <a class=\"external text\" href=\"https://www.nytimes.com/2021/06/04/opinion/ezra-klein-podcast-brian-christian.html\" rel=\"nofollow\">\"If 'All Models Are Wrong,' Why Do We Give Them So Much Power?\"</a>. <i>The New York Times</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0362-4331\" rel=\"nofollow\">0362-4331</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=If+%27All+Models+Are+Wrong%2C%27+Why+Do+We+Give+Them+So+Much+Power%3F&amp;rft.date=2021-06-04&amp;rft.issn=0362-4331&amp;rft.au=The+Ezra+Klein+Show&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2021%2F06%2F04%2Fopinion%2Fezra-klein-podcast-brian-christian.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFWolchover2015\">Wolchover, Natalie (April 21, 2015). <a class=\"external text\" href=\"https://www.quantamagazine.org/artificial-intelligence-aligned-with-human-values-qa-with-stuart-russell-20150421/\" rel=\"nofollow\">\"Concerns of an Artificial Intelligence Pioneer\"</a>. <i>Quanta Magazine</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quanta+Magazine&amp;rft.atitle=Concerns+of+an+Artificial+Intelligence+Pioneer&amp;rft.date=2015-04-21&amp;rft.aulast=Wolchover&amp;rft.aufirst=Natalie&amp;rft_id=https%3A%2F%2Fwww.quantamagazine.org%2Fartificial-intelligence-aligned-with-human-values-qa-with-stuart-russell-20150421%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-35\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-35\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFCalifornia_Assembly\">California Assembly. <a class=\"external text\" href=\"https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180ACR215\" rel=\"nofollow\">\"Bill Text - ACR-215 23 Asilomar AI Principles\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Bill+Text+-+ACR-215+23+Asilomar+AI+Principles.&amp;rft.au=California+Assembly&amp;rft_id=https%3A%2F%2Fleginfo.legislature.ca.gov%2Ffaces%2FbillTextClient.xhtml%3Fbill_id%3D201720180ACR215&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:1922-36\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1922_36-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1922_36-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFJohnsonIziev2022\">Johnson, Steven; Iziev, Nikita (April 15, 2022). <a class=\"external text\" href=\"https://www.nytimes.com/2022/04/15/magazine/ai-language.html\" rel=\"nofollow\">\"A.I. Is Mastering Language. Should We Trust What It Says?\"</a>. <i>The New York Times</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0362-4331\" rel=\"nofollow\">0362-4331</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=A.I.+Is+Mastering+Language.+Should+We+Trust+What+It+Says%3F&amp;rft.date=2022-04-15&amp;rft.issn=0362-4331&amp;rft.aulast=Johnson&amp;rft.aufirst=Steven&amp;rft.au=Iziev%2C+Nikita&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2022%2F04%2F15%2Fmagazine%2Fai-language.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:322-37\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:322_37-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:322_37-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:322_37-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"Section_1.1.5\">Russell, Stuart J.; Norvig, Peter (2020). <a class=\"external text\" href=\"https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html\" rel=\"nofollow\"><i>Artificial intelligence: A modern approach</i></a> (4th ed.). Pearson. pp. 4–5. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-292-40113-3\" title=\"Special:BookSources/978-1-292-40113-3\"><bdi>978-1-292-40113-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1303900751\" rel=\"nofollow\">1303900751</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+intelligence%3A+A+modern+approach&amp;rft.pages=4-5&amp;rft.edition=4th&amp;rft.pub=Pearson&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1303900751&amp;rft.isbn=978-1-292-40113-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rft_id=https%3A%2F%2Fwww.pearson.com%2Fus%2Fhigher-education%2Fprogram%2FRussell-Artificial-Intelligence-A-Modern-Approach-4th-Edition%2FPGM1263338.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-38\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-38\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFOpenAI2022\">OpenAI (February 15, 2022). <a class=\"external text\" href=\"https://openai.com/alignment/\" rel=\"nofollow\">\"Aligning AI systems with human intent\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Aligning+AI+systems+with+human+intent&amp;rft.date=2022-02-15&amp;rft.au=OpenAI&amp;rft_id=https%3A%2F%2Fopenai.com%2Falignment%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-39\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-39\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMedium\">Medium. <a class=\"external text\" href=\"https://deepmindsafetyresearch.medium.com\" rel=\"nofollow\">\"DeepMind Safety Research\"</a>. <i>Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=DeepMind+Safety+Research&amp;rft.au=Medium&amp;rft_id=https%3A%2F%2Fdeepmindsafetyresearch.medium.com&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:0-40\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:0_40-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:0_40-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFKrakovnaUesatoMikulikRahtz2020\">Krakovna, Victoria; Uesato, Jonathan; Mikulik, Vladimir; Rahtz, Matthew; Everitt, Tom; Kumar, Ramana; Kenton, Zac; Leike, Jan; Legg, Shane (April 21, 2020). <a class=\"external text\" href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\" rel=\"nofollow\">\"Specification gaming: the flip side of AI ingenuity\"</a>. <i>Deepmind</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Deepmind&amp;rft.atitle=Specification+gaming%3A+the+flip+side+of+AI+ingenuity&amp;rft.date=2020-04-21&amp;rft.aulast=Krakovna&amp;rft.aufirst=Victoria&amp;rft.au=Uesato%2C+Jonathan&amp;rft.au=Mikulik%2C+Vladimir&amp;rft.au=Rahtz%2C+Matthew&amp;rft.au=Everitt%2C+Tom&amp;rft.au=Kumar%2C+Ramana&amp;rft.au=Kenton%2C+Zac&amp;rft.au=Leike%2C+Jan&amp;rft.au=Legg%2C+Shane&amp;rft_id=https%3A%2F%2Fwww.deepmind.com%2Fblog%2Fspecification-gaming-the-flip-side-of-ai-ingenuity&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:1-41\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1_41-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1_41-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFManheimGarrabrant2018\">Manheim, David; Garrabrant, Scott (2018). \"Categorizing Variants of Goodhart's Law\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1803.04585\" rel=\"nofollow\">1803.04585</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.AI\" rel=\"nofollow\">cs.AI</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Categorizing+Variants+of+Goodhart%27s+Law&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1803.04585&amp;rft.aulast=Manheim&amp;rft.aufirst=David&amp;rft.au=Garrabrant%2C+Scott&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:143-42\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:143_42-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:143_42-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:143_42-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:143_42-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFAmodeiChristianoRay2017\">Amodei, Dario; Christiano, Paul; Ray, Alex (June 13, 2017). <a class=\"external text\" href=\"https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/\" rel=\"nofollow\">\"Learning from Human Preferences\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 21,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Learning+from+Human+Preferences&amp;rft.date=2017-06-13&amp;rft.aulast=Amodei&amp;rft.aufirst=Dario&amp;rft.au=Christiano%2C+Paul&amp;rft.au=Ray%2C+Alex&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fdeep-reinforcement-learning-from-human-preferences%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:1322-43\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1322_43-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1322_43-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:1322_43-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFLinHiltonEvans2022\">Lin, Stephanie; Hilton, Jacob; Evans, Owain (2022). <a class=\"external text\" href=\"https://aclanthology.org/2022.acl-long.229\" rel=\"nofollow\">\"TruthfulQA: Measuring How Models Mimic Human Falsehoods\"</a>. <i>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>. Dublin, Ireland: Association for Computational Linguistics: 3214–3252. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.18653%2Fv1%2F2022.acl-long.229\" rel=\"nofollow\">10.18653/v1/2022.acl-long.229</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:237532606\" rel=\"nofollow\">237532606</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+60th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+1%3A+Long+Papers%29&amp;rft.atitle=TruthfulQA%3A+Measuring+How+Models+Mimic+Human+Falsehoods&amp;rft.pages=3214-3252&amp;rft.date=2022&amp;rft_id=info%3Adoi%2F10.18653%2Fv1%2F2022.acl-long.229&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A237532606%23id-name%3DS2CID&amp;rft.aulast=Lin&amp;rft.aufirst=Stephanie&amp;rft.au=Hilton%2C+Jacob&amp;rft.au=Evans%2C+Owain&amp;rft_id=https%3A%2F%2Faclanthology.org%2F2022.acl-long.229&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-44\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-44\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFNaughton2021\">Naughton, John (October 2, 2021). <a class=\"external text\" href=\"https://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest\" rel=\"nofollow\">\"The truth about artificial intelligence? It isn't that honest\"</a>. <i>The Observer</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0029-7712\" rel=\"nofollow\">0029-7712</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 18,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Observer&amp;rft.atitle=The+truth+about+artificial+intelligence%3F+It+isn%27t+that+honest&amp;rft.date=2021-10-02&amp;rft.issn=0029-7712&amp;rft.aulast=Naughton&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fcommentisfree%2F2021%2Foct%2F02%2Fthe-truth-about-artificial-intelligence-it-isnt-that-honest&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-45\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-45\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFJiLeeFrieskeYu2022\">Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Madotto, Andrea; Fung, Pascale (February 1, 2022). <a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2022arXiv220203629J\" rel=\"nofollow\">\"Survey of Hallucination in Natural Language Generation\"</a>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2202.03629\" rel=\"nofollow\">2202.03629</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Survey+of+Hallucination+in+Natural+Language+Generation&amp;rft.date=2022-02-01&amp;rft_id=info%3Aarxiv%2F2202.03629&amp;rft.aulast=Ji&amp;rft.aufirst=Ziwei&amp;rft.au=Lee%2C+Nayeon&amp;rft.au=Frieske%2C+Rita&amp;rft.au=Yu%2C+Tiezheng&amp;rft.au=Su%2C+Dan&amp;rft.au=Xu%2C+Yan&amp;rft.au=Ishii%2C+Etsuko&amp;rft.au=Bang%2C+Yejin&amp;rft.au=Madotto%2C+Andrea&amp;rft.au=Fung%2C+Pascale&amp;rft_id=https%3A%2F%2Fui.adsabs.harvard.edu%2Fabs%2F2022arXiv220203629J&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span> <span class=\"cs1-hidden-error citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_journal\" title=\"Template:Cite journal\">cite journal</a>}}</code>: </span><span class=\"cs1-hidden-error citation-comment\">Cite journal requires <code class=\"cs1-code\">|journal=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-46\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-46\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFEdge.org\">Edge.org. <a class=\"external text\" href=\"https://www.edge.org/conversation/the-myth-of-ai#26015\" rel=\"nofollow\">\"The Myth Of AI | Edge.org\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 19,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Myth+Of+AI+%7C+Edge.org&amp;rft.au=Edge.org&amp;rft_id=https%3A%2F%2Fwww.edge.org%2Fconversation%2Fthe-myth-of-ai%2326015&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-47\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-47\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFTasioulas2019\">Tasioulas, John (2019). \"First Steps Towards an Ethics of Robots and Artificial Intelligence\". <i>Journal of Practical Ethics</i>. <b>7</b> (1): 61–95.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Practical+Ethics&amp;rft.atitle=First+Steps+Towards+an+Ethics+of+Robots+and+Artificial+Intelligence&amp;rft.volume=7&amp;rft.issue=1&amp;rft.pages=61-95&amp;rft.date=2019&amp;rft.aulast=Tasioulas&amp;rft.aufirst=John&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:72-48\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:72_48-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFWellsDeepa_SeetharamanHorwitz2021\">Wells, Georgia; Deepa Seetharaman; Horwitz, Jeff (November 5, 2021). <a class=\"external text\" href=\"https://www.wsj.com/articles/facebook-bad-for-you-360-million-users-say-yes-company-documents-facebook-files-11636124681\" rel=\"nofollow\">\"Is Facebook Bad for You? It Is for About 360 Million Users, Company Surveys Suggest\"</a>. <i>Wall Street Journal</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0099-9660\" rel=\"nofollow\">0099-9660</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 19,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wall+Street+Journal&amp;rft.atitle=Is+Facebook+Bad+for+You%3F+It+Is+for+About+360+Million+Users%2C+Company+Surveys+Suggest&amp;rft.date=2021-11-05&amp;rft.issn=0099-9660&amp;rft.aulast=Wells&amp;rft.aufirst=Georgia&amp;rft.au=Deepa+Seetharaman&amp;rft.au=Horwitz%2C+Jeff&amp;rft_id=https%3A%2F%2Fwww.wsj.com%2Farticles%2Ffacebook-bad-for-you-360-million-users-say-yes-company-documents-facebook-files-11636124681&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:82-49\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:82_49-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation report cs1\" id=\"CITEREFBarrettHendrixSims2021\">Barrett, Paul M.; Hendrix, Justin; Sims, J. Grant (September 2021). <a class=\"external text\" href=\"https://bhr.stern.nyu.edu/polarization-report-page\" rel=\"nofollow\">How Social Media Intensifies U.S. Political Polarization-And What Can Be Done About It</a> (Report). Center for Business and Human Rights, NYU.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=How+Social+Media+Intensifies+U.S.+Political+Polarization-And+What+Can+Be+Done+About+It&amp;rft.pub=Center+for+Business+and+Human+Rights%2C+NYU&amp;rft.date=2021-09&amp;rft.aulast=Barrett&amp;rft.aufirst=Paul+M.&amp;rft.au=Hendrix%2C+Justin&amp;rft.au=Sims%2C+J.+Grant&amp;rft_id=https%3A%2F%2Fbhr.stern.nyu.edu%2Fpolarization-report-page&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-50\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-50\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFShepardson2018\">Shepardson, David (May 24, 2018). <a class=\"external text\" href=\"https://www.reuters.com/article/us-uber-crash-idUSKCN1IP26K\" rel=\"nofollow\">\"Uber disabled emergency braking in self-driving car: U.S. agency\"</a>. <i>Reuters</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 20,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Reuters&amp;rft.atitle=Uber+disabled+emergency+braking+in+self-driving+car%3A+U.S.+agency&amp;rft.date=2018-05-24&amp;rft.aulast=Shepardson&amp;rft.aufirst=David&amp;rft_id=https%3A%2F%2Fwww.reuters.com%2Farticle%2Fus-uber-crash-idUSKCN1IP26K&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:262-51\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:262_51-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFBaum2021\">Baum, Seth (January 1, 2021). <a class=\"external text\" href=\"https://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/\" rel=\"nofollow\">\"2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 20,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=2020+Survey+of+Artificial+General+Intelligence+Projects+for+Ethics%2C+Risk%2C+and+Policy&amp;rft.date=2021-01-01&amp;rft.aulast=Baum&amp;rft.aufirst=Seth&amp;rft_id=https%3A%2F%2Fgcrinstitute.org%2F2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-52\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-52\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFEdwards2022\">Edwards, Ben (April 26, 2022). <a class=\"external text\" href=\"https://arstechnica.com/information-technology/2022/09/new-ai-assistant-can-browse-search-and-use-web-apps-like-a-human/\" rel=\"nofollow\">\"Adept's AI assistant can browse, search, and use web apps like a human\"</a>. <i>Ars Technica</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 9,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=Adept%27s+AI+assistant+can+browse%2C+search%2C+and+use+web+apps+like+a+human&amp;rft.date=2022-04-26&amp;rft.aulast=Edwards&amp;rft.aufirst=Ben&amp;rft_id=https%3A%2F%2Farstechnica.com%2Finformation-technology%2F2022%2F09%2Fnew-ai-assistant-can-browse-search-and-use-web-apps-like-a-human%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-53\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-53\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFWakefield2022\">Wakefield, Jane (February 2, 2022). <a class=\"external text\" href=\"https://www.bbc.com/news/technology-60231058\" rel=\"nofollow\">\"DeepMind AI rivals average human competitive coder\"</a>. <i>BBC News</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 9,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=DeepMind+AI+rivals+average+human+competitive+coder&amp;rft.date=2022-02-02&amp;rft.aulast=Wakefield&amp;rft.aufirst=Jane&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-60231058&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-54\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-54\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFDominguez2022\">Dominguez, Daniel (May 19, 2022). <a class=\"external text\" href=\"https://www.infoq.com/news/2022/05/deepmind-gato-ai-agent/\" rel=\"nofollow\">\"DeepMind Introduces Gato, a New Generalist AI Agent\"</a>. <i>InfoQ</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 9,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=InfoQ&amp;rft.atitle=DeepMind+Introduces+Gato%2C+a+New+Generalist+AI+Agent&amp;rft.date=2022-05-19&amp;rft.aulast=Dominguez&amp;rft.aufirst=Daniel&amp;rft_id=https%3A%2F%2Fwww.infoq.com%2Fnews%2F2022%2F05%2Fdeepmind-gato-ai-agent%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:282-55\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:282_55-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFGraceSalvatierDafoeZhang2018\">Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain (July 31, 2018). <a class=\"external text\" href=\"http://jair.org/index.php/jair/article/view/11222\" rel=\"nofollow\">\"Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts\"</a>. <i>Journal of Artificial Intelligence Research</i>. <b>62</b>: 729–754. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1613%2Fjair.1.11222\" rel=\"nofollow\">10.1613/jair.1.11222</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1076-9757\" rel=\"nofollow\">1076-9757</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:8746462\" rel=\"nofollow\">8746462</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Viewpoint%3A+When+Will+AI+Exceed+Human+Performance%3F+Evidence+from+AI+Experts&amp;rft.volume=62&amp;rft.pages=729-754&amp;rft.date=2018-07-31&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8746462%23id-name%3DS2CID&amp;rft.issn=1076-9757&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.1.11222&amp;rft.aulast=Grace&amp;rft.aufirst=Katja&amp;rft.au=Salvatier%2C+John&amp;rft.au=Dafoe%2C+Allan&amp;rft.au=Zhang%2C+Baobao&amp;rft.au=Evans%2C+Owain&amp;rft_id=http%3A%2F%2Fjair.org%2Findex.php%2Fjair%2Farticle%2Fview%2F11222&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:292-56\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:292_56-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFZhangAnderljungKahnDreksler2021\">Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan (August 2, 2021). <a class=\"external text\" href=\"https://jair.org/index.php/jair/article/view/12895\" rel=\"nofollow\">\"Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers\"</a>. <i>Journal of Artificial Intelligence Research</i>. <b>71</b>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1613%2Fjair.1.12895\" rel=\"nofollow\">10.1613/jair.1.12895</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1076-9757\" rel=\"nofollow\">1076-9757</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:233740003\" rel=\"nofollow\">233740003</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Ethics+and+Governance+of+Artificial+Intelligence%3A+Evidence+from+a+Survey+of+Machine+Learning+Researchers&amp;rft.volume=71&amp;rft.date=2021-08-02&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233740003%23id-name%3DS2CID&amp;rft.issn=1076-9757&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.1.12895&amp;rft.aulast=Zhang&amp;rft.aufirst=Baobao&amp;rft.au=Anderljung%2C+Markus&amp;rft.au=Kahn%2C+Lauren&amp;rft.au=Dreksler%2C+Noemi&amp;rft.au=Horowitz%2C+Michael+C.&amp;rft.au=Dafoe%2C+Allan&amp;rft_id=https%3A%2F%2Fjair.org%2Findex.php%2Fjair%2Farticle%2Fview%2F12895&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-57\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-57\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFWeiTayBommasaniRaffel2022\">Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff (June 15, 2022). \"Emergent Abilities of Large Language Models\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2206.07682\" rel=\"nofollow\">2206.07682</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CL\" rel=\"nofollow\">cs.CL</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Emergent+Abilities+of+Large+Language+Models&amp;rft.date=2022-06-15&amp;rft_id=info%3Aarxiv%2F2206.07682&amp;rft.aulast=Wei&amp;rft.aufirst=Jason&amp;rft.au=Tay%2C+Yi&amp;rft.au=Bommasani%2C+Rishi&amp;rft.au=Raffel%2C+Colin&amp;rft.au=Zoph%2C+Barret&amp;rft.au=Borgeaud%2C+Sebastian&amp;rft.au=Yogatama%2C+Dani&amp;rft.au=Bosma%2C+Maarten&amp;rft.au=Zhou%2C+Denny&amp;rft.au=Metzler%2C+Donald&amp;rft.au=Chi%2C+Ed+H.&amp;rft.au=Hashimoto%2C+Tatsunori&amp;rft.au=Vinyals%2C+Oriol&amp;rft.au=Liang%2C+Percy&amp;rft.au=Dean%2C+Jeff&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:84-58\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:84_58-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:84_58-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:84_58-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:84_58-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-:84_58-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-:84_58-5\"><sup><i><b>f</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFBostrom2014\">Bostrom, Nick (2014). <i>Superintelligence: Paths, Dangers, Strategies</i> (1st ed.). USA: Oxford University Press, Inc. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-19-967811-2\" title=\"Special:BookSources/978-0-19-967811-2\"><bdi>978-0-19-967811-2</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=USA&amp;rft.edition=1st&amp;rft.pub=Oxford+University+Press%2C+Inc.&amp;rft.date=2014&amp;rft.isbn=978-0-19-967811-2&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-quanta-hide-seek-59\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-quanta-hide-seek_59-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-quanta-hide-seek_59-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFOrnes2019\">Ornes, Stephen (November 18, 2019). <a class=\"external text\" href=\"https://www.quantamagazine.org/artificial-intelligence-discovers-tool-use-in-hide-and-seek-games-20191118/\" rel=\"nofollow\">\"Playing Hide-and-Seek, Machines Invent New Tools\"</a>. <i>Quanta Magazine</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quanta+Magazine&amp;rft.atitle=Playing+Hide-and-Seek%2C+Machines+Invent+New+Tools&amp;rft.date=2019-11-18&amp;rft.aulast=Ornes&amp;rft.aufirst=Stephen&amp;rft_id=https%3A%2F%2Fwww.quantamagazine.org%2Fartificial-intelligence-discovers-tool-use-in-hide-and-seek-games-20191118%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:103-61\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:103_61-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:103_61-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFLeikeMarticKrakovnaOrtega2017\">Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro A.; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane (November 28, 2017). \"AI Safety Gridworlds\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1711.09883\" rel=\"nofollow\">1711.09883</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=AI+Safety+Gridworlds&amp;rft.date=2017-11-28&amp;rft_id=info%3Aarxiv%2F1711.09883&amp;rft.aulast=Leike&amp;rft.aufirst=Jan&amp;rft.au=Martic%2C+Miljan&amp;rft.au=Krakovna%2C+Victoria&amp;rft.au=Ortega%2C+Pedro+A.&amp;rft.au=Everitt%2C+Tom&amp;rft.au=Lefrancq%2C+Andrew&amp;rft.au=Orseau%2C+Laurent&amp;rft.au=Legg%2C+Shane&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:272-62\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:272_62-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFOrseauArmstrong2016\">Orseau, Laurent; Armstrong, Stuart (January 1, 2016). <a class=\"external text\" href=\"https://www.deepmind.com/publications/safely-interruptible-agents\" rel=\"nofollow\">\"Safely Interruptible Agents\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 20,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Safely+Interruptible+Agents&amp;rft.date=2016-01-01&amp;rft.aulast=Orseau&amp;rft.aufirst=Laurent&amp;rft.au=Armstrong%2C+Stuart&amp;rft_id=https%3A%2F%2Fwww.deepmind.com%2Fpublications%2Fsafely-interruptible-agents&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span> <span class=\"cs1-hidden-error citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_journal\" title=\"Template:Cite journal\">cite journal</a>}}</code>: </span><span class=\"cs1-hidden-error citation-comment\">Cite journal requires <code class=\"cs1-code\">|journal=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-:242-63\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:242_63-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:242_63-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:242_63-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:242_63-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFHadfield-MenellDraganAbbeelRussell2017\">Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart (2017). \"The Off-Switch Game\". <i>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</i>. pp. 220–227. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.24963%2Fijcai.2017%2F32\" rel=\"nofollow\">10.24963/ijcai.2017/32</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=The+Off-Switch+Game&amp;rft.btitle=Proceedings+of+the+Twenty-Sixth+International+Joint+Conference+on+Artificial+Intelligence%2C+IJCAI-17&amp;rft.pages=220-227&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.24963%2Fijcai.2017%2F32&amp;rft.aulast=Hadfield-Menell&amp;rft.aufirst=Dylan&amp;rft.au=Dragan%2C+Anca&amp;rft.au=Abbeel%2C+Pieter&amp;rft.au=Russell%2C+Stuart&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:2522-64\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:2522_64-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:2522_64-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:2522_64-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:2522_64-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFTurnerSmithShahCritch2021\">Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Critch, Andrew; Tadepalli, Prasad (December 3, 2021). <a class=\"external text\" href=\"https://proceedings.neurips.cc/paper/2021/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html\" rel=\"nofollow\">\"Optimal Policies Tend to Seek Power\"</a>. <i>Neural Information Processing Systems</i>. <b>34</b>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1912.01683\" rel=\"nofollow\">1912.01683</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Information+Processing+Systems&amp;rft.atitle=Optimal+Policies+Tend+to+Seek+Power&amp;rft.volume=34&amp;rft.date=2021-12-03&amp;rft_id=info%3Aarxiv%2F1912.01683&amp;rft.aulast=Turner&amp;rft.aufirst=Alexander+Matt&amp;rft.au=Smith%2C+Logan&amp;rft.au=Shah%2C+Rohin&amp;rft.au=Critch%2C+Andrew&amp;rft.au=Tadepalli%2C+Prasad&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2021%2Fhash%2Fc26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-65\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-65\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation speech cs1\" id=\"CITEREFTuring1951\">Turing, Alan (1951). <a class=\"external text\" href=\"https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-4\" rel=\"nofollow\"><i>Intelligent machinery, a heretical theory</i></a> (Speech). Lecture given to '51 Society'. Manchester: The Turing Digital Archive<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 22,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Intelligent+machinery%2C+a+heretical+theory&amp;rft.place=Manchester&amp;rft.pub=The+Turing+Digital+Archive&amp;rft.date=1951&amp;rft.aulast=Turing&amp;rft.aufirst=Alan&amp;rft_id=https%3A%2F%2Fturingarchive.kings.cam.ac.uk%2Fpublications-lectures-and-talks-amtb%2Famt-b-4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-66\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-66\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation episode cs1\" id=\"CITEREFTuring1951\">Turing, Alan (May 15, 1951). \"Can digital computers think?\". <i>Automatic Calculating Machines</i>. Episode 2. BBC. <a class=\"external text\" href=\"https://turingarchive.kings.cam.ac.uk/publications-lectures-and-talks-amtb/amt-b-6\" rel=\"nofollow\">Can digital computers think?</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Automatic+Calculating+Machines&amp;rft.series=Episode+2&amp;rft.date=1951-05-15&amp;rft.aulast=Turing&amp;rft.aufirst=Alan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:302-68\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:302_68-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMuehlhauser2016\">Muehlhauser, Luke (January 29, 2016). <a class=\"external text\" href=\"https://lukemuehlhauser.com/sutskever-on-talking-machines/\" rel=\"nofollow\">\"Sutskever on Talking Machines\"</a>. <i>Luke Muehlhauser</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Luke+Muehlhauser&amp;rft.atitle=Sutskever+on+Talking+Machines&amp;rft.date=2016-01-29&amp;rft.aulast=Muehlhauser&amp;rft.aufirst=Luke&amp;rft_id=https%3A%2F%2Flukemuehlhauser.com%2Fsutskever-on-talking-machines%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-people.eecs.berkeley.edu-69\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-people.eecs.berkeley.edu_69-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-people.eecs.berkeley.edu_69-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\"><a class=\"external text\" href=\"https://people.eecs.berkeley.edu/~russell/hc.html\" rel=\"nofollow\">\"Human Compatible: AI and the Problem of Control\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 22,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Human+Compatible%3A+AI+and+the+Problem+of+Control&amp;rft_id=https%3A%2F%2Fpeople.eecs.berkeley.edu%2F~russell%2Fhc.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:312-72\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:312_72-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFShanahan2015\">Shanahan, Murray (2015). <a class=\"external text\" href=\"https://www.worldcat.org/oclc/917889148\" rel=\"nofollow\"><i>The technological singularity</i></a>. Cambridge, Massachusetts. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-262-33182-1\" title=\"Special:BookSources/978-0-262-33182-1\"><bdi>978-0-262-33182-1</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/917889148\" rel=\"nofollow\">917889148</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+technological+singularity&amp;rft.place=Cambridge%2C+Massachusetts&amp;rft.date=2015&amp;rft_id=info%3Aoclcnum%2F917889148&amp;rft.isbn=978-0-262-33182-1&amp;rft.aulast=Shanahan&amp;rft.aufirst=Murray&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F917889148&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-73\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-73\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRussellNorvig2009\">Russell, Stuart; Norvig, Peter (2009). <i>Artificial Intelligence: A Modern Approach</i>. Prentice Hall. p. 1010. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-13-604259-4\" title=\"Special:BookSources/978-0-13-604259-4\"><bdi>978-0-13-604259-4</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pages=1010&amp;rft.pub=Prentice+Hall&amp;rft.date=2009&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:332-75\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:332_75-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFRossi\">Rossi, Francesca. <a class=\"external text\" href=\"https://www.washingtonpost.com/news/in-theory/wp/2015/11/05/how-do-you-teach-a-machine-to-be-moral/\" rel=\"nofollow\">\"Opinion | How do you teach a machine to be moral?\"</a>. <i>Washington Post</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0190-8286\" rel=\"nofollow\">0190-8286</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=Opinion+%7C+How+do+you+teach+a+machine+to+be+moral%3F&amp;rft.issn=0190-8286&amp;rft.aulast=Rossi&amp;rft.aufirst=Francesca&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fnews%2Fin-theory%2Fwp%2F2015%2F11%2F05%2Fhow-do-you-teach-a-machine-to-be-moral%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:342-76\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:342_76-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFAaronson2022\">Aaronson, Scott (June 17, 2022). <a class=\"external text\" href=\"https://scottaaronson.blog/?p=6484\" rel=\"nofollow\">\"OpenAI!\"</a>. <i>Shtetl-Optimized</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Shtetl-Optimized&amp;rft.atitle=OpenAI%21&amp;rft.date=2022-06-17&amp;rft.aulast=Aaronson&amp;rft.aufirst=Scott&amp;rft_id=https%3A%2F%2Fscottaaronson.blog%2F%3Fp%3D6484&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:352-77\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:352_77-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation cs2\" id=\"CITEREFSelman\">Selman, Bart, <a class=\"external text\" href=\"https://futureoflife.org/data/PDF/bart_selman.pdf\" rel=\"nofollow\"><i>Intelligence Explosion: Science or Fiction?</i></a> <span class=\"cs1-format\">(PDF)</span></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Intelligence+Explosion%3A+Science+or+Fiction%3F&amp;rft.aulast=Selman&amp;rft.aufirst=Bart&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fdata%2FPDF%2Fbart_selman.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:362-78\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:362_78-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMcAllester2014\">McAllester (August 10, 2014). <a class=\"external text\" href=\"https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/\" rel=\"nofollow\">\"Friendly AI and the Servant Mission\"</a>. <i>Machine Thoughts</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Machine+Thoughts&amp;rft.atitle=Friendly+AI+and+the+Servant+Mission&amp;rft.date=2014-08-10&amp;rft.au=McAllester&amp;rft_id=https%3A%2F%2Fmachinethoughts.wordpress.com%2F2014%2F08%2F10%2Ffriendly-ai-and-the-servant-mission%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:372-79\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:372_79-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFSchmidhuber2015\">Schmidhuber, Jürgen (March 6, 2015). <a class=\"external text\" href=\"https://www.reddit.com/r/MachineLearning/comments/2xcyrl/comment/cp65ico/?utm_source=share&amp;utm_medium=web2x&amp;context=3\" rel=\"nofollow\">\"I am Jürgen Schmidhuber, AMA!\"</a> <span class=\"cs1-format\">(Reddit Comment)</span>. <i>r/MachineLearning</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=r%2FMachineLearning&amp;rft.atitle=I+am+J%C3%BCrgen+Schmidhuber%2C+AMA%21&amp;rft.date=2015-03-06&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J%C3%BCrgen&amp;rft_id=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F2xcyrl%2Fcomment%2Fcp65ico%2F%3Futm_source%3Dshare%26utm_medium%3Dweb2x%26context%3D3&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:1124-80\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1124_80-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1124_80-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:1124_80-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:1124_80-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFEverittLeaHutter2018\">Everitt, Tom; Lea, Gary; Hutter, Marcus (May 21, 2018). \"AGI Safety Literature Review\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1805.01109\" rel=\"nofollow\">1805.01109</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.AI\" rel=\"nofollow\">cs.AI</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=AGI+Safety+Literature+Review&amp;rft.date=2018-05-21&amp;rft_id=info%3Aarxiv%2F1805.01109&amp;rft.aulast=Everitt&amp;rft.aufirst=Tom&amp;rft.au=Lea%2C+Gary&amp;rft.au=Hutter%2C+Marcus&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:382-81\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:382_81-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFShane2009\">Shane (August 31, 2009). <a class=\"external text\" href=\"http://www.vetta.org/2009/08/funding-safe-agi/\" rel=\"nofollow\">\"Funding safe AGI\"</a>. <i>vetta project</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=vetta+project&amp;rft.atitle=Funding+safe+AGI&amp;rft.date=2009-08-31&amp;rft.au=Shane&amp;rft_id=http%3A%2F%2Fwww.vetta.org%2F2009%2F08%2Ffunding-safe-agi%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:392-82\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:392_82-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFHorvitz2016\">Horvitz, Eric (June 27, 2016). <a class=\"external text\" href=\"http://erichorvitz.com/OSTP-CMU_AI_Safety_framing_talk.pdf\" rel=\"nofollow\">\"Reflections on Safety and Artificial Intelligence\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Eric Horvitz</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">April 20,</span> 2020</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Eric+Horvitz&amp;rft.atitle=Reflections+on+Safety+and+Artificial+Intelligence&amp;rft.date=2016-06-27&amp;rft.aulast=Horvitz&amp;rft.aufirst=Eric&amp;rft_id=http%3A%2F%2Ferichorvitz.com%2FOSTP-CMU_AI_Safety_framing_talk.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:402-83\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:402_83-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFChollet2018\">Chollet, François (December 8, 2018). <a class=\"external text\" href=\"https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec\" rel=\"nofollow\">\"The implausibility of intelligence explosion\"</a>. <i>Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=The+implausibility+of+intelligence+explosion&amp;rft.date=2018-12-08&amp;rft.aulast=Chollet&amp;rft.aufirst=Fran%C3%A7ois&amp;rft_id=https%3A%2F%2Fmedium.com%2F%40francois.chollet%2Fthe-impossibility-of-intelligence-explosion-5be4a9eda6ec&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:412-84\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:412_84-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMarcus2022\">Marcus, Gary (June 6, 2022). <a class=\"external text\" href=\"https://www.scientificamerican.com/article/artificial-general-intelligence-is-not-as-imminent-as-you-might-think1/\" rel=\"nofollow\">\"Artificial General Intelligence Is Not as Imminent as You Might Think\"</a>. <i>Scientific American</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scientific+American&amp;rft.atitle=Artificial+General+Intelligence+Is+Not+as+Imminent+as+You+Might+Think&amp;rft.date=2022-06-06&amp;rft.aulast=Marcus&amp;rft.aufirst=Gary&amp;rft_id=https%3A%2F%2Fwww.scientificamerican.com%2Farticle%2Fartificial-general-intelligence-is-not-as-imminent-as-you-might-think1%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:432-85\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:432_85-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFBarber2016\">Barber, Lynsey (July 31, 2016). <a class=\"external text\" href=\"https://www.cityam.com/phew-facebooks-ai-chief-says-intelligent-machines-not/\" rel=\"nofollow\">\"Phew! Facebook's AI chief says intelligent machines are not a threat to humanity\"</a>. <i>CityAM</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CityAM&amp;rft.atitle=Phew%21+Facebook%27s+AI+chief+says+intelligent+machines+are+not+a+threat+to+humanity&amp;rft.date=2016-07-31&amp;rft.aulast=Barber&amp;rft.aufirst=Lynsey&amp;rft_id=https%3A%2F%2Fwww.cityam.com%2Fphew-facebooks-ai-chief-says-intelligent-machines-not%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:442-86\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:442_86-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFHarris2021\">Harris, Jeremie (June 16, 2021). <a class=\"external text\" href=\"https://towardsdatascience.com/the-case-against-worrying-about-existential-risk-from-ai-d4aaa77e812b\" rel=\"nofollow\">\"The case against (worrying about) existential risk from AI\"</a>. <i>Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=The+case+against+%28worrying+about%29+existential+risk+from+AI&amp;rft.date=2021-06-16&amp;rft.aulast=Harris&amp;rft.aufirst=Jeremie&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fthe-case-against-worrying-about-existential-risk-from-ai-d4aaa77e812b&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-87\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-87\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRochonRossi2015\">Rochon, Louis-Philippe; Rossi, Sergio (February 27, 2015). <a class=\"external text\" href=\"https://books.google.com/books?id=6kzfBgAAQBAJ\" rel=\"nofollow\"><i>The Encyclopedia of Central Banking</i></a>. Edward Elgar Publishing. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-78254-744-0\" title=\"Special:BookSources/978-1-78254-744-0\"><bdi>978-1-78254-744-0</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Encyclopedia+of+Central+Banking&amp;rft.pub=Edward+Elgar+Publishing&amp;rft.date=2015-02-27&amp;rft.isbn=978-1-78254-744-0&amp;rft.aulast=Rochon&amp;rft.aufirst=Louis-Philippe&amp;rft.au=Rossi%2C+Sergio&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D6kzfBgAAQBAJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:224-88\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:224_88-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:224_88-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:224_88-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFChristian2020\">Christian, Brian (2020). <a class=\"external text\" href=\"https://wwnorton.co.uk/books/9780393635829-the-alignment-problem\" rel=\"nofollow\"><i>The alignment problem: Machine learning and human values</i></a>. W. W. Norton &amp; Company. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-393-86833-3\" title=\"Special:BookSources/978-0-393-86833-3\"><bdi>978-0-393-86833-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1233266753\" rel=\"nofollow\">1233266753</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+alignment+problem%3A+Machine+learning+and+human+values&amp;rft.pub=W.+W.+Norton+%26+Company&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1233266753&amp;rft.isbn=978-0-393-86833-3&amp;rft.aulast=Christian&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwwnorton.co.uk%2Fbooks%2F9780393635829-the-alignment-problem&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-89\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-89\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFChristian2020\">Christian, Brian (2020). <a class=\"external text\" href=\"https://wwnorton.co.uk/books/9780393635829-the-alignment-problem\" rel=\"nofollow\"><i>The alignment problem: Machine learning and human values</i></a>. W. W. Norton &amp; Company. p. 88. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-393-86833-3\" title=\"Special:BookSources/978-0-393-86833-3\"><bdi>978-0-393-86833-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1233266753\" rel=\"nofollow\">1233266753</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+alignment+problem%3A+Machine+learning+and+human+values&amp;rft.pages=88&amp;rft.pub=W.+W.+Norton+%26+Company&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1233266753&amp;rft.isbn=978-0-393-86833-3&amp;rft.aulast=Christian&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwwnorton.co.uk%2Fbooks%2F9780393635829-the-alignment-problem&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-90\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-90\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFNgRussell2000\">Ng, Andrew Y.; Russell, Stuart J. (2000). \"Algorithms for inverse reinforcement learning\". <i>Proceedings of the seventeenth international conference on machine learning</i>. ICML '00. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. pp. 663–670. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/1-55860-707-2\" title=\"Special:BookSources/1-55860-707-2\"><bdi>1-55860-707-2</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Algorithms+for+inverse+reinforcement+learning&amp;rft.btitle=Proceedings+of+the+seventeenth+international+conference+on+machine+learning&amp;rft.place=San+Francisco%2C+CA%2C+USA&amp;rft.series=ICML+%2700&amp;rft.pages=663-670&amp;rft.pub=Morgan+Kaufmann+Publishers+Inc.&amp;rft.date=2000&amp;rft.isbn=1-55860-707-2&amp;rft.aulast=Ng&amp;rft.aufirst=Andrew+Y.&amp;rft.au=Russell%2C+Stuart+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-91\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-91\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFHadfield-MenellRussellAbbeelDragan2016\">Hadfield-Menell, Dylan; Russell, Stuart J; Abbeel, Pieter; Dragan, Anca (2016). <a class=\"external text\" href=\"https://papers.nips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html\" rel=\"nofollow\">\"Cooperative Inverse Reinforcement Learning\"</a>. <i>Advances in Neural Information Processing Systems</i>. NIPS'16. Vol. 29. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-5108-3881-9\" title=\"Special:BookSources/978-1-5108-3881-9\"><bdi>978-1-5108-3881-9</bdi></a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 21,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Cooperative+Inverse+Reinforcement+Learning&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.series=NIPS%2716&amp;rft.date=2016&amp;rft.isbn=978-1-5108-3881-9&amp;rft.aulast=Hadfield-Menell&amp;rft.aufirst=Dylan&amp;rft.au=Russell%2C+Stuart+J&amp;rft.au=Abbeel%2C+Pieter&amp;rft.au=Dragan%2C+Anca&amp;rft_id=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F2016%2Fhash%2Fc3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-92\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-92\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFArmstrongMindermann2018\">Armstrong, Stuart; Mindermann, Sören (2018). <a class=\"external text\" href=\"https://proceedings.neurips.cc/paper/2018/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html\" rel=\"nofollow\">\"Occam' s razor is insufficient to infer the preferences of irrational agents\"</a>. <i>Advances in Neural Information Processing Systems</i>. NeurIPS 2018. Vol. 31. Montréal: Curran Associates, Inc<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 21,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Occam%27+s+razor+is+insufficient+to+infer+the+preferences+of+irrational+agents&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.place=Montr%C3%A9al&amp;rft.pub=Curran+Associates%2C+Inc.&amp;rft.date=2018&amp;rft.aulast=Armstrong&amp;rft.aufirst=Stuart&amp;rft.au=Mindermann%2C+S%C3%B6ren&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2018%2Fhash%2Fd89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-93\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-93\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFLi2018\">Li, Yuxi (November 25, 2018). <a class=\"external text\" href=\"http://www.smallake.kr/wp-content/uploads/2017/08/1701.07274.pdf\" rel=\"nofollow\">\"Deep Reinforcement Learning: An Overview\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Lecture Notes in Networks and Systems Book Series</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Lecture+Notes+in+Networks+and+Systems+Book+Series&amp;rft.atitle=Deep+Reinforcement+Learning%3A+An+Overview&amp;rft.date=2018-11-25&amp;rft.aulast=Li&amp;rft.aufirst=Yuxi&amp;rft_id=http%3A%2F%2Fwww.smallake.kr%2Fwp-content%2Fuploads%2F2017%2F08%2F1701.07274.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-94\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-94\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFFürnkranzHüllermeierRudinSlowinski2014\">Fürnkranz, Johannes; Hüllermeier, Eyke; Rudin, Cynthia; Slowinski, Roman; Sanner, Scott (2014). Marc Herbstritt. <a class=\"external text\" href=\"http://drops.dagstuhl.de/opus/volltexte/2014/4550/\" rel=\"nofollow\">\"Preference Learning\"</a>. <i>Dagstuhl Reports</i>. <b>4</b> (3): 27 pages. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.4230%2FDAGREP.4.3.1\" rel=\"nofollow\">10.4230/DAGREP.4.3.1</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Dagstuhl+Reports&amp;rft.atitle=Preference+Learning&amp;rft.volume=4&amp;rft.issue=3&amp;rft.pages=27+pages&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.4230%2FDAGREP.4.3.1&amp;rft.aulast=F%C3%BCrnkranz&amp;rft.aufirst=Johannes&amp;rft.au=H%C3%BCllermeier%2C+Eyke&amp;rft.au=Rudin%2C+Cynthia&amp;rft.au=Slowinski%2C+Roman&amp;rft.au=Sanner%2C+Scott&amp;rft_id=http%3A%2F%2Fdrops.dagstuhl.de%2Fopus%2Fvolltexte%2F2014%2F4550%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-95\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-95\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFHiltonGao2022\">Hilton, Jacob; Gao, Leo (April 13, 2022). <a class=\"external text\" href=\"https://openai.com/blog/measuring-goodharts-law/\" rel=\"nofollow\">\"Measuring Goodhart's Law\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 9,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Measuring+Goodhart%27s+Law&amp;rft.date=2022-04-13&amp;rft.aulast=Hilton&amp;rft.aufirst=Jacob&amp;rft.au=Gao%2C+Leo&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fmeasuring-goodharts-law%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-96\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-96\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFAnderson2022\">Anderson, Martin (April 5, 2022). <a class=\"external text\" href=\"https://www.unite.ai/the-perils-of-using-quotations-to-authenticate-nlg-content/\" rel=\"nofollow\">\"The Perils of Using Quotations to Authenticate NLG Content\"</a>. <i>Unite.AI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 21,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Unite.AI&amp;rft.atitle=The+Perils+of+Using+Quotations+to+Authenticate+NLG+Content&amp;rft.date=2022-04-05&amp;rft.aulast=Anderson&amp;rft.aufirst=Martin&amp;rft_id=https%3A%2F%2Fwww.unite.ai%2Fthe-perils-of-using-quotations-to-authenticate-nlg-content%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:202-97\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:202_97-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:202_97-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFWiggers2022\">Wiggers, Kyle (February 5, 2022). <a class=\"external text\" href=\"https://venturebeat.com/2022/02/05/despite-recent-progress-ai-powered-chatbots-still-have-a-long-way-to-go/\" rel=\"nofollow\">\"Despite recent progress, AI-powered chatbots still have a long way to go\"</a>. <i>VentureBeat</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=VentureBeat&amp;rft.atitle=Despite+recent+progress%2C+AI-powered+chatbots+still+have+a+long+way+to+go&amp;rft.date=2022-02-05&amp;rft.aulast=Wiggers&amp;rft.aufirst=Kyle&amp;rft_id=https%3A%2F%2Fventurebeat.com%2F2022%2F02%2F05%2Fdespite-recent-progress-ai-powered-chatbots-still-have-a-long-way-to-go%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-98\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-98\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFHendrycksBurnsBasartCritch2021\">Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob (July 24, 2021). \"Aligning AI With Shared Human Values\". <i>International Conference on Learning Representations</i>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2008.02275\" rel=\"nofollow\">2008.02275</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Learning+Representations&amp;rft.atitle=Aligning+AI+With+Shared+Human+Values&amp;rft.date=2021-07-24&amp;rft_id=info%3Aarxiv%2F2008.02275&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Burns%2C+Collin&amp;rft.au=Basart%2C+Steven&amp;rft.au=Critch%2C+Andrew&amp;rft.au=Li%2C+Jerry&amp;rft.au=Song%2C+Dawn&amp;rft.au=Steinhardt%2C+Jacob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-99\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-99\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFPerezHuangSongCai2022\">Perez, Ethan; Huang, Saffron; Song, Francis; Cai, Trevor; Ring, Roman; Aslanides, John; Glaese, Amelia; McAleese, Nat; Irving, Geoffrey (February 7, 2022). \"Red Teaming Language Models with Language Models\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2202.03286\" rel=\"nofollow\">2202.03286</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CL\" rel=\"nofollow\">cs.CL</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Red+Teaming+Language+Models+with+Language+Models&amp;rft.date=2022-02-07&amp;rft_id=info%3Aarxiv%2F2202.03286&amp;rft.aulast=Perez&amp;rft.aufirst=Ethan&amp;rft.au=Huang%2C+Saffron&amp;rft.au=Song%2C+Francis&amp;rft.au=Cai%2C+Trevor&amp;rft.au=Ring%2C+Roman&amp;rft.au=Aslanides%2C+John&amp;rft.au=Glaese%2C+Amelia&amp;rft.au=McAleese%2C+Nat&amp;rft.au=Irving%2C+Geoffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-100\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-100\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFBhattacharyya2022\">Bhattacharyya, Sreejani (February 14, 2022). <a class=\"external text\" href=\"https://analyticsindiamag.com/deepminds-red-teaming-language-models-with-language-models-what-is-it/\" rel=\"nofollow\">\"DeepMind's \"red teaming\" language models with language models: What is it?\"</a>. <i>Analytics India Magazine</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Analytics+India+Magazine&amp;rft.atitle=DeepMind%27s+%22red+teaming%22+language+models+with+language+models%3A+What+is+it%3F&amp;rft.date=2022-02-14&amp;rft.aulast=Bhattacharyya&amp;rft.aufirst=Sreejani&amp;rft_id=https%3A%2F%2Fanalyticsindiamag.com%2Fdeepminds-red-teaming-language-models-with-language-models-what-is-it%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-101\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-101\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFWallachAllen2009\">Wallach, Wendell; Allen, Colin (2009). <a class=\"external text\" href=\"https://oxford.universitypressscholarship.com/10.1093/acprof:oso/9780195374049.001.0001/acprof-9780195374049\" rel=\"nofollow\"><i>Moral Machines: Teaching Robots Right from Wrong</i></a>. New York: Oxford University Press. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-19-537404-9\" title=\"Special:BookSources/978-0-19-537404-9\"><bdi>978-0-19-537404-9</bdi></a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Moral+Machines%3A+Teaching+Robots+Right+from+Wrong&amp;rft.place=New+York&amp;rft.pub=Oxford+University+Press&amp;rft.date=2009&amp;rft.isbn=978-0-19-537404-9&amp;rft.aulast=Wallach&amp;rft.aufirst=Wendell&amp;rft.au=Allen%2C+Colin&amp;rft_id=https%3A%2F%2Foxford.universitypressscholarship.com%2F10.1093%2Facprof%3Aoso%2F9780195374049.001.0001%2Facprof-9780195374049&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-102\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-102\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFWiegel2010\">Wiegel, Vincent (December 1, 2010). <a class=\"external text\" href=\"https://doi.org/10.1007/s10676-010-9239-1\" rel=\"nofollow\">\"Wendell Wallach and Colin Allen: moral machines: teaching robots right from wrong\"</a>. <i>Ethics and Information Technology</i>. <b>12</b> (4): 359–361. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2Fs10676-010-9239-1\" rel=\"nofollow\">10.1007/s10676-010-9239-1</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1572-8439\" rel=\"nofollow\">1572-8439</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:30532107\" rel=\"nofollow\">30532107</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ethics+and+Information+Technology&amp;rft.atitle=Wendell+Wallach+and+Colin+Allen%3A+moral+machines%3A+teaching+robots+right+from+wrong&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=359-361&amp;rft.date=2010-12-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A30532107%23id-name%3DS2CID&amp;rft.issn=1572-8439&amp;rft_id=info%3Adoi%2F10.1007%2Fs10676-010-9239-1&amp;rft.aulast=Wiegel&amp;rft.aufirst=Vincent&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%2Fs10676-010-9239-1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-104\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-104\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFMacAskill2022\">MacAskill, William (2022). <a class=\"external text\" href=\"https://www.basicbooks.com/titles/william-macaskill/what-we-owe-the-future/9781541618633/\" rel=\"nofollow\"><i>What we owe the future</i></a>. New York, NY: Basic Books. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-5416-1862-6\" title=\"Special:BookSources/978-1-5416-1862-6\"><bdi>978-1-5416-1862-6</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1314633519\" rel=\"nofollow\">1314633519</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=What+we+owe+the+future&amp;rft.place=New+York%2C+NY&amp;rft.pub=Basic+Books&amp;rft.date=2022&amp;rft_id=info%3Aoclcnum%2F1314633519&amp;rft.isbn=978-1-5416-1862-6&amp;rft.aulast=MacAskill&amp;rft.aufirst=William&amp;rft_id=https%3A%2F%2Fwww.basicbooks.com%2Ftitles%2Fwilliam-macaskill%2Fwhat-we-owe-the-future%2F9781541618633%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:172-105\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:172_105-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:172_105-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFWuOuyangZieglerStiennon2021\">Wu, Jeff; Ouyang, Long; Ziegler, Daniel M.; Stiennon, Nisan; Lowe, Ryan; Leike, Jan; Christiano, Paul (September 27, 2021). \"Recursively Summarizing Books with Human Feedback\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2109.10862\" rel=\"nofollow\">2109.10862</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CL\" rel=\"nofollow\">cs.CL</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Recursively+Summarizing+Books+with+Human+Feedback&amp;rft.date=2021-09-27&amp;rft_id=info%3Aarxiv%2F2109.10862&amp;rft.aulast=Wu&amp;rft.aufirst=Jeff&amp;rft.au=Ouyang%2C+Long&amp;rft.au=Ziegler%2C+Daniel+M.&amp;rft.au=Stiennon%2C+Nisan&amp;rft.au=Lowe%2C+Ryan&amp;rft.au=Leike%2C+Jan&amp;rft.au=Christiano%2C+Paul&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-106\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-106\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFIrvingAmodei2018\">Irving, Geoffrey; Amodei, Dario (May 3, 2018). <a class=\"external text\" href=\"https://openai.com/blog/debate/\" rel=\"nofollow\">\"AI Safety via Debate\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=AI+Safety+via+Debate&amp;rft.date=2018-05-03&amp;rft.aulast=Irving&amp;rft.aufirst=Geoffrey&amp;rft.au=Amodei%2C+Dario&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fdebate%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-Naughton-107\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Naughton_107-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Naughton_107-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFNaughton2021\">Naughton, John (October 2, 2021). <a class=\"external text\" href=\"https://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest\" rel=\"nofollow\">\"The truth about artificial intelligence? It isn't that honest\"</a>. <i>The Observer</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0029-7712\" rel=\"nofollow\">0029-7712</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Observer&amp;rft.atitle=The+truth+about+artificial+intelligence%3F+It+isn%27t+that+honest&amp;rft.date=2021-10-02&amp;rft.issn=0029-7712&amp;rft.aulast=Naughton&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fcommentisfree%2F2021%2Foct%2F02%2Fthe-truth-about-artificial-intelligence-it-isnt-that-honest&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:133-108\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:133_108-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:133_108-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFChristianoShlegerisAmodei2018\">Christiano, Paul; Shlegeris, Buck; Amodei, Dario (October 19, 2018). \"Supervising strong learners by amplifying weak experts\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1810.08575\" rel=\"nofollow\">1810.08575</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Supervising+strong+learners+by+amplifying+weak+experts&amp;rft.date=2018-10-19&amp;rft_id=info%3Aarxiv%2F1810.08575&amp;rft.aulast=Christiano&amp;rft.aufirst=Paul&amp;rft.au=Shlegeris%2C+Buck&amp;rft.au=Amodei%2C+Dario&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-109\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-109\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\"><a class=\"external text\" href=\"http://link.springer.com/10.1007/978-3-030-39958-0\" rel=\"nofollow\"><i>Genetic Programming Theory and Practice XVII</i></a>. Genetic and Evolutionary Computation. Wolfgang Banzhaf, Erik Goodman, Leigh Sheneman, Leonardo Trujillo, Bill Worzel (eds.). Cham: Springer International Publishing. 2020. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-030-39958-0\" rel=\"nofollow\">10.1007/978-3-030-39958-0</a>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-030-39957-3\" title=\"Special:BookSources/978-3-030-39957-3\"><bdi>978-3-030-39957-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:218531292\" rel=\"nofollow\">218531292</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Genetic+Programming+Theory+and+Practice+XVII&amp;rft.place=Cham&amp;rft.series=Genetic+and+Evolutionary+Computation&amp;rft.pub=Springer+International+Publishing&amp;rft.date=2020&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A218531292%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-030-39958-0&amp;rft.isbn=978-3-030-39957-3&amp;rft_id=http%3A%2F%2Flink.springer.com%2F10.1007%2F978-3-030-39958-0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: others (<a href=\"/wiki/Category:CS1_maint:_others\" title=\"Category:CS1 maint: others\">link</a>)</span></span>\n</li>\n<li id=\"cite_note-110\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-110\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation podcast cs1\" id=\"CITEREFWiblin2018\">Wiblin, Robert (October 2, 2018). <a class=\"external text\" href=\"https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/\" rel=\"nofollow\">\"Dr Paul Christiano on how OpenAI is developing real solutions to the 'AI alignment problem', and his vision of how humanity will progressively hand over decision-making to AI systems\"</a> (Podcast). 80,000 hours. No. 44<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Dr+Paul+Christiano+on+how+OpenAI+is+developing+real+solutions+to+the+%E2%80%98AI+alignment+problem%E2%80%99%2C+and+his+vision+of+how+humanity+will+progressively+hand+over+decision-making+to+AI+systems&amp;rft.series=80%2C000+hours&amp;rft.date=2018-10-02&amp;rft.aulast=Wiblin&amp;rft.aufirst=Robert&amp;rft_id=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fpaul-christiano-ai-alignment-solutions%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-111\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-111\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFLehmanCluneMisevicAdami2020\">Lehman, Joel; Clune, Jeff; Misevic, Dusan; Adami, Christoph; Altenberg, Lee; Beaulieu, Julie; Bentley, Peter J.; Bernard, Samuel; Beslon, Guillaume; Bryson, David M.; Cheney, Nick (2020). <a class=\"external text\" href=\"https://direct.mit.edu/artl/article/26/2/274-306/93255\" rel=\"nofollow\">\"The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities\"</a>. <i>Artificial Life</i>. <b>26</b> (2): 274–306. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1162%2Fartl_a_00319\" rel=\"nofollow\">10.1162/artl_a_00319</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/1064-5462\" rel=\"nofollow\">1064-5462</a>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/32271631\" rel=\"nofollow\">32271631</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:4519185\" rel=\"nofollow\">4519185</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Artificial+Life&amp;rft.atitle=The+Surprising+Creativity+of+Digital+Evolution%3A+A+Collection+of+Anecdotes+from+the+Evolutionary+Computation+and+Artificial+Life+Research+Communities&amp;rft.volume=26&amp;rft.issue=2&amp;rft.pages=274-306&amp;rft.date=2020&amp;rft.issn=1064-5462&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A4519185%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F32271631&amp;rft_id=info%3Adoi%2F10.1162%2Fartl_a_00319&amp;rft.aulast=Lehman&amp;rft.aufirst=Joel&amp;rft.au=Clune%2C+Jeff&amp;rft.au=Misevic%2C+Dusan&amp;rft.au=Adami%2C+Christoph&amp;rft.au=Altenberg%2C+Lee&amp;rft.au=Beaulieu%2C+Julie&amp;rft.au=Bentley%2C+Peter+J.&amp;rft.au=Bernard%2C+Samuel&amp;rft.au=Beslon%2C+Guillaume&amp;rft.au=Bryson%2C+David+M.&amp;rft.au=Cheney%2C+Nick&amp;rft_id=https%3A%2F%2Fdirect.mit.edu%2Fartl%2Farticle%2F26%2F2%2F274-306%2F93255&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-112\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-112\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFHendrycksCarliniSchulmanSteinhardt2022\">Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob (June 16, 2022). \"Unsolved Problems in ML Safety\". p. 7. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2109.13916\" rel=\"nofollow\">2109.13916</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Unsolved+Problems+in+ML+Safety&amp;rft.pages=7&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2109.13916&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Carlini%2C+Nicholas&amp;rft.au=Schulman%2C+John&amp;rft.au=Steinhardt%2C+Jacob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-113\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-113\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFLeikeKruegerEverittMartic2018\">Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane (November 19, 2018). \"Scalable agent alignment via reward modeling: a research direction\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1811.07871\" rel=\"nofollow\">1811.07871</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Scalable+agent+alignment+via+reward+modeling%3A+a+research+direction&amp;rft.date=2018-11-19&amp;rft_id=info%3Aarxiv%2F1811.07871&amp;rft.aulast=Leike&amp;rft.aufirst=Jan&amp;rft.au=Krueger%2C+David&amp;rft.au=Everitt%2C+Tom&amp;rft.au=Martic%2C+Miljan&amp;rft.au=Maini%2C+Vishal&amp;rft.au=Legg%2C+Shane&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-114\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-114\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFWiggers2021\">Wiggers, Kyle (September 23, 2021). <a class=\"external text\" href=\"https://venturebeat.com/2021/09/23/openai-unveils-model-that-can-summarize-books-of-any-length/\" rel=\"nofollow\">\"OpenAI unveils model that can summarize books of any length\"</a>. <i>VentureBeat</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=VentureBeat&amp;rft.atitle=OpenAI+unveils+model+that+can+summarize+books+of+any+length&amp;rft.date=2021-09-23&amp;rft.aulast=Wiggers&amp;rft.aufirst=Kyle&amp;rft_id=https%3A%2F%2Fventurebeat.com%2F2021%2F09%2F23%2Fopenai-unveils-model-that-can-summarize-books-of-any-length%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-115\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-115\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMoltzau2019\">Moltzau, Alex (August 24, 2019). <a class=\"external text\" href=\"https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d\" rel=\"nofollow\">\"Debating the AI Safety Debate\"</a>. <i>Towards Data Science</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Towards+Data+Science&amp;rft.atitle=Debating+the+AI+Safety+Debate&amp;rft.date=2019-08-24&amp;rft.aulast=Moltzau&amp;rft.aufirst=Alex&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:182-116\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:182_116-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:182_116-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:182_116-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFWiggers2021\">Wiggers, Kyle (September 20, 2021). <a class=\"external text\" href=\"https://venturebeat.com/2021/09/20/falsehoods-more-likely-with-large-language-models/\" rel=\"nofollow\">\"Falsehoods more likely with large language models\"</a>. <i>VentureBeat</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=VentureBeat&amp;rft.atitle=Falsehoods+more+likely+with+large+language+models&amp;rft.date=2021-09-20&amp;rft.aulast=Wiggers&amp;rft.aufirst=Kyle&amp;rft_id=https%3A%2F%2Fventurebeat.com%2F2021%2F09%2F20%2Ffalsehoods-more-likely-with-large-language-models%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-117\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-117\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFThe_Guardian2020\">The Guardian (September 8, 2020). <a class=\"external text\" href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\" rel=\"nofollow\">\"A robot wrote this entire article. Are you scared yet, human?\"</a>. <i>The Guardian</i>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0261-3077\" rel=\"nofollow\">0261-3077</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=A+robot+wrote+this+entire+article.+Are+you+scared+yet%2C+human%3F&amp;rft.date=2020-09-08&amp;rft.issn=0261-3077&amp;rft.au=The+Guardian&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fcommentisfree%2F2020%2Fsep%2F08%2Frobot-wrote-this-article-gpt-3&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-118\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-118\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFHeaven2020\">Heaven, Will Douglas (July 20, 2020). <a class=\"external text\" href=\"https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\" rel=\"nofollow\">\"OpenAI's new language generator GPT-3 is shockingly good—and completely mindless\"</a>. <i>MIT Technology Review</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=OpenAI%27s+new+language+generator+GPT-3+is+shockingly+good%E2%80%94and+completely+mindless&amp;rft.date=2020-07-20&amp;rft.aulast=Heaven&amp;rft.aufirst=Will+Douglas&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2020%2F07%2F20%2F1005454%2Fopenai-machine-learning-language-generator-gpt-3-nlp%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:21-119\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:21_119-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:21_119-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:21_119-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFEvansCotton-BarrattFinnvedenBales2021\">Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William (October 13, 2021). \"Truthful AI: Developing and governing AI that does not lie\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2110.06674\" rel=\"nofollow\">2110.06674</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CY\" rel=\"nofollow\">cs.CY</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Truthful+AI%3A+Developing+and+governing+AI+that+does+not+lie&amp;rft.date=2021-10-13&amp;rft_id=info%3Aarxiv%2F2110.06674&amp;rft.aulast=Evans&amp;rft.aufirst=Owain&amp;rft.au=Cotton-Barratt%2C+Owen&amp;rft.au=Finnveden%2C+Lukas&amp;rft.au=Bales%2C+Adam&amp;rft.au=Balwit%2C+Avital&amp;rft.au=Wills%2C+Peter&amp;rft.au=Righetti%2C+Luca&amp;rft.au=Saunders%2C+William&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-120\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-120\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFAlford2021\">Alford, Anthony (July 13, 2021). <a class=\"external text\" href=\"https://www.infoq.com/news/2021/07/eleutherai-gpt-j/\" rel=\"nofollow\">\"EleutherAI Open-Sources Six Billion Parameter GPT-3 Clone GPT-J\"</a>. <i>InfoQ</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=InfoQ&amp;rft.atitle=EleutherAI+Open-Sources+Six+Billion+Parameter+GPT-3+Clone+GPT-J&amp;rft.date=2021-07-13&amp;rft.aulast=Alford&amp;rft.aufirst=Anthony&amp;rft_id=https%3A%2F%2Fwww.infoq.com%2Fnews%2F2021%2F07%2Feleutherai-gpt-j%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-121\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-121\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation conference cs1\" id=\"CITEREFShusterPoffChenKiela2021\">Shuster, Kurt; Poff, Spencer; Chen, Moya; Kiela, Douwe; Weston, Jason (November 2021). <a class=\"external text\" href=\"https://aclanthology.org/2021.findings-emnlp.320\" rel=\"nofollow\">\"Retrieval Augmentation Reduces Hallucination in Conversation\"</a>. <i>Findings of the Association for Computational Linguistics: EMNLP 2021</i>. EMNLP-Findings 2021. Punta Cana, Dominican Republic: Association for Computational Linguistics. pp. 3784–3803. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.18653%2Fv1%2F2021.findings-emnlp.320\" rel=\"nofollow\">10.18653/v1/2021.findings-emnlp.320</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Retrieval+Augmentation+Reduces+Hallucination+in+Conversation&amp;rft.btitle=Findings+of+the+Association+for+Computational+Linguistics%3A+EMNLP+2021&amp;rft.place=Punta+Cana%2C+Dominican+Republic&amp;rft.pages=3784-3803&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2021-11&amp;rft_id=info%3Adoi%2F10.18653%2Fv1%2F2021.findings-emnlp.320&amp;rft.aulast=Shuster&amp;rft.aufirst=Kurt&amp;rft.au=Poff%2C+Spencer&amp;rft.au=Chen%2C+Moya&amp;rft.au=Kiela%2C+Douwe&amp;rft.au=Weston%2C+Jason&amp;rft_id=https%3A%2F%2Faclanthology.org%2F2021.findings-emnlp.320&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-122\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-122\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFNakanoHiltonBalajiWu2022\">Nakano, Reiichiro; Hilton, Jacob; Balaji, Suchir; Wu, Jeff; Ouyang, Long; Kim, Christina; Hesse, Christopher; Jain, Shantanu; Kosaraju, Vineet; Saunders, William; Jiang, Xu (June 1, 2022). \"WebGPT: Browser-assisted question-answering with human feedback\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2112.09332\" rel=\"nofollow\">2112.09332</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CL\" rel=\"nofollow\">cs.CL</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=WebGPT%3A+Browser-assisted+question-answering+with+human+feedback&amp;rft.date=2022-06-01&amp;rft_id=info%3Aarxiv%2F2112.09332&amp;rft.aulast=Nakano&amp;rft.aufirst=Reiichiro&amp;rft.au=Hilton%2C+Jacob&amp;rft.au=Balaji%2C+Suchir&amp;rft.au=Wu%2C+Jeff&amp;rft.au=Ouyang%2C+Long&amp;rft.au=Kim%2C+Christina&amp;rft.au=Hesse%2C+Christopher&amp;rft.au=Jain%2C+Shantanu&amp;rft.au=Kosaraju%2C+Vineet&amp;rft.au=Saunders%2C+William&amp;rft.au=Jiang%2C+Xu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-123\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-123\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFKumar2021\">Kumar, Nitish (December 23, 2021). <a class=\"external text\" href=\"https://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/\" rel=\"nofollow\">\"OpenAI Researchers Find Ways To More Accurately Answer Open-Ended Questions Using A Text-Based Web Browser\"</a>. <i>MarkTechPost</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MarkTechPost&amp;rft.atitle=OpenAI+Researchers+Find+Ways+To+More+Accurately+Answer+Open-Ended+Questions+Using+A+Text-Based+Web+Browser&amp;rft.date=2021-12-23&amp;rft.aulast=Kumar&amp;rft.aufirst=Nitish&amp;rft_id=https%3A%2F%2Fwww.marktechpost.com%2F2021%2F12%2F22%2Fopenai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-124\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-124\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFMenickTrebaczMikulikAslanides2022\">Menick, Jacob; Trebacz, Maja; Mikulik, Vladimir; Aslanides, John; Song, Francis; Chadwick, Martin; Glaese, Mia; Young, Susannah; Campbell-Gillingham, Lucy; Irving, Geoffrey; McAleese, Nat (March 21, 2022). <a class=\"external text\" href=\"https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes\" rel=\"nofollow\">\"Teaching language models to support answers with verified quotes\"</a>. <i>DeepMind</i>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2203.11147\" rel=\"nofollow\">2203.11147</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=DeepMind&amp;rft.atitle=Teaching+language+models+to+support+answers+with+verified+quotes&amp;rft.date=2022-03-21&amp;rft_id=info%3Aarxiv%2F2203.11147&amp;rft.aulast=Menick&amp;rft.aufirst=Jacob&amp;rft.au=Trebacz%2C+Maja&amp;rft.au=Mikulik%2C+Vladimir&amp;rft.au=Aslanides%2C+John&amp;rft.au=Song%2C+Francis&amp;rft.au=Chadwick%2C+Martin&amp;rft.au=Glaese%2C+Mia&amp;rft.au=Young%2C+Susannah&amp;rft.au=Campbell-Gillingham%2C+Lucy&amp;rft.au=Irving%2C+Geoffrey&amp;rft.au=McAleese%2C+Nat&amp;rft_id=https%3A%2F%2Fwww.deepmind.com%2Fpublications%2Fgophercite-teaching-language-models-to-support-answers-with-verified-quotes&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-125\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-125\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFAskellBaiChenDrain2021\">Askell, Amanda; Bai, Yuntao; Chen, Anna; Drain, Dawn; Ganguli, Deep; Henighan, Tom; Jones, Andy; Joseph, Nicholas; Mann, Ben; DasSarma, Nova; Elhage, Nelson (December 9, 2021). \"A General Language Assistant as a Laboratory for Alignment\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/2112.00861\" rel=\"nofollow\">2112.00861</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.CL\" rel=\"nofollow\">cs.CL</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=A+General+Language+Assistant+as+a+Laboratory+for+Alignment&amp;rft.date=2021-12-09&amp;rft_id=info%3Aarxiv%2F2112.00861&amp;rft.aulast=Askell&amp;rft.aufirst=Amanda&amp;rft.au=Bai%2C+Yuntao&amp;rft.au=Chen%2C+Anna&amp;rft.au=Drain%2C+Dawn&amp;rft.au=Ganguli%2C+Deep&amp;rft.au=Henighan%2C+Tom&amp;rft.au=Jones%2C+Andy&amp;rft.au=Joseph%2C+Nicholas&amp;rft.au=Mann%2C+Ben&amp;rft.au=DasSarma%2C+Nova&amp;rft.au=Elhage%2C+Nelson&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-126\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-126\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFKentonEverittWeidingerGabriel2021\">Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey (March 30, 2021). <a class=\"external text\" href=\"https://deepmindsafetyresearch.medium.com/alignment-of-language-agents-9fbc7dd52c6c\" rel=\"nofollow\">\"Alignment of Language Agents\"</a>. <i>DeepMind Safety Research - Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">July 23,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=DeepMind+Safety+Research+-+Medium&amp;rft.atitle=Alignment+of+Language+Agents&amp;rft.date=2021-03-30&amp;rft.aulast=Kenton&amp;rft.aufirst=Zachary&amp;rft.au=Everitt%2C+Tom&amp;rft.au=Weidinger%2C+Laura&amp;rft.au=Gabriel%2C+Iason&amp;rft.au=Mikulik%2C+Vladimir&amp;rft.au=Irving%2C+Geoffrey&amp;rft_id=https%3A%2F%2Fdeepmindsafetyresearch.medium.com%2Falignment-of-language-agents-9fbc7dd52c6c&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-127\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-127\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFLeikeSchulmanWu2022\">Leike, Jan; Schulman, John; Wu, Jeffrey (August 24, 2022). <a class=\"external text\" href=\"https://openai.com/blog/our-approach-to-alignment-research/\" rel=\"nofollow\">\"Our approach to alignment research\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 9,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Our+approach+to+alignment+research&amp;rft.date=2022-08-24&amp;rft.aulast=Leike&amp;rft.aufirst=Jan&amp;rft.au=Schulman%2C+John&amp;rft.au=Wu%2C+Jeffrey&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Four-approach-to-alignment-research%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-128\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-128\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFOrtegaMainiDeepMind_safety_team2018\">Ortega, Pedro A.; Maini, Vishal; DeepMind safety team (September 27, 2018). <a class=\"external text\" href=\"https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1\" rel=\"nofollow\">\"Building safe artificial intelligence: specification, robustness, and assurance\"</a>. <i>Medium</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Building+safe+artificial+intelligence%3A+specification%2C+robustness%2C+and+assurance&amp;rft.date=2018-09-27&amp;rft.aulast=Ortega&amp;rft.aufirst=Pedro+A.&amp;rft.au=Maini%2C+Vishal&amp;rft.au=DeepMind+safety+team&amp;rft_id=https%3A%2F%2Fdeepmindsafetyresearch.medium.com%2Fbuilding-safe-artificial-intelligence-52f5f75058f1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-129\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-129\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFChristian2020\">Christian, Brian (2020). \"Chapter 5: Shaping\". <a class=\"external text\" href=\"https://wwnorton.co.uk/books/9780393635829-the-alignment-problem\" rel=\"nofollow\"><i>The alignment problem: Machine learning and human values</i></a>. W. W. Norton &amp; Company. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-393-86833-3\" title=\"Special:BookSources/978-0-393-86833-3\"><bdi>978-0-393-86833-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1233266753\" rel=\"nofollow\">1233266753</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+5%3A+Shaping&amp;rft.btitle=The+alignment+problem%3A+Machine+learning+and+human+values&amp;rft.pub=W.+W.+Norton+%26+Company&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1233266753&amp;rft.isbn=978-0-393-86833-3&amp;rft.aulast=Christian&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwwnorton.co.uk%2Fbooks%2F9780393635829-the-alignment-problem&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-130\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-130\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFZhangChanYanBose2022\">Zhang, Xiaoge; Chan, Felix T.S.; Yan, Chao; Bose, Indranil (2022). <a class=\"external text\" href=\"https://linkinghub.elsevier.com/retrieve/pii/S0167923622000719\" rel=\"nofollow\">\"Towards risk-aware artificial intelligence and machine learning systems: An overview\"</a>. <i>Decision Support Systems</i>. <b>159</b>: 113800. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1016%2Fj.dss.2022.113800\" rel=\"nofollow\">10.1016/j.dss.2022.113800</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:248585546\" rel=\"nofollow\">248585546</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Decision+Support+Systems&amp;rft.atitle=Towards+risk-aware+artificial+intelligence+and+machine+learning+systems%3A+An+overview&amp;rft.volume=159&amp;rft.pages=113800&amp;rft.date=2022&amp;rft_id=info%3Adoi%2F10.1016%2Fj.dss.2022.113800&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A248585546%23id-name%3DS2CID&amp;rft.aulast=Zhang&amp;rft.aufirst=Xiaoge&amp;rft.au=Chan%2C+Felix+T.S.&amp;rft.au=Yan%2C+Chao&amp;rft.au=Bose%2C+Indranil&amp;rft_id=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0167923622000719&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-131\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-131\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFMcCarthyMinskyRochesterShannon2006\">McCarthy, John; Minsky, Marvin L.; Rochester, Nathaniel; Shannon, Claude E. (December 15, 2006). <a class=\"external text\" href=\"https://ojs.aaai.org/index.php/aimagazine/article/view/1904\" rel=\"nofollow\">\"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955\"</a>. <i>AI Magazine</i>. <b>27</b> (4): 12. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1609%2Faimag.v27i4.1904\" rel=\"nofollow\">10.1609/aimag.v27i4.1904</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/2371-9621\" rel=\"nofollow\">2371-9621</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:19439915\" rel=\"nofollow\">19439915</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+Magazine&amp;rft.atitle=A+Proposal+for+the+Dartmouth+Summer+Research+Project+on+Artificial+Intelligence%2C+August+31%2C+1955&amp;rft.volume=27&amp;rft.issue=4&amp;rft.pages=12&amp;rft.date=2006-12-15&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A19439915%23id-name%3DS2CID&amp;rft.issn=2371-9621&amp;rft_id=info%3Adoi%2F10.1609%2Faimag.v27i4.1904&amp;rft.aulast=McCarthy&amp;rft.aufirst=John&amp;rft.au=Minsky%2C+Marvin+L.&amp;rft.au=Rochester%2C+Nathaniel&amp;rft.au=Shannon%2C+Claude+E.&amp;rft_id=https%3A%2F%2Fojs.aaai.org%2Findex.php%2Faimagazine%2Farticle%2Fview%2F1904&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-132\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-132\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFBakerKanitscheiderMarkovWu2019\">Baker, Bowen; Kanitscheider, Ingmar; Markov, Todor; Wu, Yi; Powell, Glenn; McGrew, Bob; Mordatch, Igor (September 17, 2019). <a class=\"external text\" href=\"https://openai.com/blog/emergent-tool-use/\" rel=\"nofollow\">\"Emergent Tool Use from Multi-Agent Interaction\"</a>. <i>OpenAI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Emergent+Tool+Use+from+Multi-Agent+Interaction&amp;rft.date=2019-09-17&amp;rft.aulast=Baker&amp;rft.aufirst=Bowen&amp;rft.au=Kanitscheider%2C+Ingmar&amp;rft.au=Markov%2C+Todor&amp;rft.au=Wu%2C+Yi&amp;rft.au=Powell%2C+Glenn&amp;rft.au=McGrew%2C+Bob&amp;rft.au=Mordatch%2C+Igor&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Femergent-tool-use%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-133\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-133\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFShermer2017\">Shermer, Michael (March 1, 2017). <a class=\"external text\" href=\"https://www.scientificamerican.com/article/artificial-intelligence-is-not-a-threat-mdash-yet/\" rel=\"nofollow\">\"Artificial Intelligence Is Not a Threat—Yet\"</a>. <i>Scientific American</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 26,</span> 2022</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scientific+American&amp;rft.atitle=Artificial+Intelligence+Is+Not+a+Threat%26mdash%3BYet&amp;rft.date=2017-03-01&amp;rft.aulast=Shermer&amp;rft.aufirst=Michael&amp;rft_id=https%3A%2F%2Fwww.scientificamerican.com%2Farticle%2Fartificial-intelligence-is-not-a-threat-mdash-yet%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-lit_review-134\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-lit_review_134-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFEverittLeaHutter2018\">Everitt, Tom; Lea, Gary; Hutter, Marcus (May 21, 2018). \"AGI Safety Literature Review\". <i>1805.01109</i>. <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1805.01109\" rel=\"nofollow\">1805.01109</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=1805.01109&amp;rft.atitle=AGI+Safety+Literature+Review&amp;rft.date=2018-05-21&amp;rft_id=info%3Aarxiv%2F1805.01109&amp;rft.aulast=Everitt&amp;rft.aufirst=Tom&amp;rft.au=Lea%2C+Gary&amp;rft.au=Hutter%2C+Marcus&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_journal\" title=\"Template:Cite journal\">cite journal</a>}}</code>:  CS1 maint: url-status (<a href=\"/wiki/Category:CS1_maint:_url-status\" title=\"Category:CS1 maint: url-status\">link</a>)</span></span>\n</li>\n<li id=\"cite_note-135\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-135\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFDemskiGarrabrant2020\">Demski, Abram; Garrabrant, Scott (October 6, 2020). \"Embedded Agency\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1902.09469\" rel=\"nofollow\">1902.09469</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.AI\" rel=\"nofollow\">cs.AI</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Embedded+Agency&amp;rft.date=2020-10-06&amp;rft_id=info%3Aarxiv%2F1902.09469&amp;rft.aulast=Demski&amp;rft.aufirst=Abram&amp;rft.au=Garrabrant%2C+Scott&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-causal_influence-136\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-causal_influence_136-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-causal_influence_136-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation arxiv cs1\" id=\"CITEREFEverittOrtegaBarnesLegg2019\">Everitt, Tom; Ortega, Pedro A.; Barnes, Elizabeth; Legg, Shane (September 6, 2019). \"Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings\". <a class=\"mw-redirect\" href=\"/wiki/ArXiv_(identifier)\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1902.09980\" rel=\"nofollow\">1902.09980</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.AI\" rel=\"nofollow\">cs.AI</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Understanding+Agent+Incentives+using+Causal+Influence+Diagrams.+Part+I%3A+Single+Action+Settings&amp;rft.date=2019-09-06&amp;rft_id=info%3Aarxiv%2F1902.09980&amp;rft.aulast=Everitt&amp;rft.aufirst=Tom&amp;rft.au=Ortega%2C+Pedro+A.&amp;rft.au=Barnes%2C+Elizabeth&amp;rft.au=Legg%2C+Shane&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-DM_specification_gaming-137\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-DM_specification_gaming_137-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFKrakovnaLegg\">Krakovna, Victoria; Legg, Shane. <a class=\"external text\" href=\"https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity\" rel=\"nofollow\">\"Specification gaming: the flip side of AI ingenuity\"</a>. <i>Deepmind</i>. <a class=\"external text\" href=\"https://web.archive.org/web/20210126173242/https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity\" rel=\"nofollow\">Archived</a> from the original on January 26, 2021<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 6,</span> 2021</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Deepmind&amp;rft.atitle=Specification+gaming%3A+the+flip+side+of+AI+ingenuity&amp;rft.aulast=Krakovna&amp;rft.aufirst=Victoria&amp;rft.au=Legg%2C+Shane&amp;rft_id=https%3A%2F%2Fdeepmind.com%2Fblog%2Farticle%2FSpecification-gaming-the-flip-side-of-AI-ingenuity&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-:3-138\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:3_138-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFCohenHutterOsborne2022\">Cohen, Michael K.; Hutter, Marcus; Osborne, Michael A. (August 29, 2022). <a class=\"external text\" href=\"https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064\" rel=\"nofollow\">\"Advanced artificial agents intervene in the provision of reward\"</a>. <i>AI Magazine</i>. <b>43</b> (3): 282–293. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1002%2Faaai.12064\" rel=\"nofollow\">10.1002/aaai.12064</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0738-4602\" rel=\"nofollow\">0738-4602</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:235489158\" rel=\"nofollow\">235489158</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+Magazine&amp;rft.atitle=Advanced+artificial+agents+intervene+in+the+provision+of+reward&amp;rft.volume=43&amp;rft.issue=3&amp;rft.pages=282-293&amp;rft.date=2022-08-29&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A235489158%23id-name%3DS2CID&amp;rft.issn=0738-4602&amp;rft_id=info%3Adoi%2F10.1002%2Faaai.12064&amp;rft.aulast=Cohen&amp;rft.aufirst=Michael+K.&amp;rft.au=Hutter%2C+Marcus&amp;rft.au=Osborne%2C+Michael+A.&amp;rft_id=https%3A%2F%2Fonlinelibrary.wiley.com%2Fdoi%2F10.1002%2Faaai.12064&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-139\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-139\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFWakefield2015\">Wakefield, Jane (September 27, 2015). <a class=\"external text\" href=\"https://www.bbc.com/news/technology-32334568\" rel=\"nofollow\">\"Intelligent Machines: Do we really need to fear AI?\"</a>. <i>BBC News</i>. <a class=\"external text\" href=\"https://web.archive.org/web/20201108124948/https://www.bbc.com/news/technology-32334568\" rel=\"nofollow\">Archived</a> from the original on November 8, 2020<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">February 9,</span> 2021</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=Intelligent+Machines%3A+Do+we+really+need+to+fear+AI%3F&amp;rft.date=2015-09-27&amp;rft.aulast=Wakefield&amp;rft.aufirst=Jane&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-32334568&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-140\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-140\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFMarcusDavis2019\">Marcus, Gary; Davis, Ernest (September 6, 2019). <a class=\"external text\" href=\"https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html\" rel=\"nofollow\">\"How to Build Artificial Intelligence We Can Trust\"</a>. <i>The New York Times</i>. <a class=\"external text\" href=\"https://web.archive.org/web/20200922145040/https://www.nytimes.com/2019/09/06/opinion/ai-explainability.html\" rel=\"nofollow\">Archived</a> from the original on September 22, 2020<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">February 9,</span> 2021</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=How+to+Build+Artificial+Intelligence+We+Can+Trust&amp;rft.date=2019-09-06&amp;rft.aulast=Marcus&amp;rft.aufirst=Gary&amp;rft.au=Davis%2C+Ernest&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2019%2F09%2F06%2Fopinion%2Fai-explainability.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-AGIResponses-141\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-AGIResponses_141-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFSotalaYampolskiy2014\">Sotala, Kaj; <a href=\"/wiki/Roman_Yampolskiy\" title=\"Roman Yampolskiy\">Yampolskiy, Roman</a> (December 19, 2014). <a class=\"external text\" href=\"https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001\" rel=\"nofollow\">\"Responses to catastrophic AGI risk: a survey\"</a>. <i><a href=\"/wiki/Physica_Scripta\" title=\"Physica Scripta\">Physica Scripta</a></i>. <b>90</b> (1): 018001. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2015PhyS...90a8001S\" rel=\"nofollow\">2015PhyS...90a8001S</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"https://doi.org/10.1088%2F0031-8949%2F90%2F1%2F018001\" rel=\"nofollow\">10.1088/0031-8949/90/1/018001</a></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physica+Scripta&amp;rft.atitle=Responses+to+catastrophic+AGI+risk%3A+a+survey&amp;rft.volume=90&amp;rft.issue=1&amp;rft.pages=018001&amp;rft.date=2014-12-19&amp;rft_id=info%3Adoi%2F10.1088%2F0031-8949%2F90%2F1%2F018001&amp;rft_id=info%3Abibcode%2F2015PhyS...90a8001S&amp;rft.aulast=Sotala&amp;rft.aufirst=Kaj&amp;rft.au=Yampolskiy%2C+Roman&amp;rft_id=%2F%2Fdoi.org%2F10.1088%252F0031-8949%252F90%252F1%252F018001&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-142\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-142\">^</a></b></span> <span class=\"reference-text\"><a class=\"external text\" href=\"https://www.un.org/en/content/common-agenda-report/\" rel=\"nofollow\">Secretary-General’s report on “Our Common Agenda”</a>, 2021. Page 63: <i>\"[T]he Compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values\"</i></span>\n</li>\n<li id=\"cite_note-143\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-143\">^</a></b></span> <span class=\"reference-text\">PRC Ministry of Science and Technology. Ethical Norms for New Generation Artificial Intelligence Released, 2021. A <a class=\"external text\" href=\"https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/\" rel=\"nofollow\">translation</a> by <a href=\"/wiki/Center_for_Security_and_Emerging_Technology\" title=\"Center for Security and Emerging Technology\">Center for Security and Emerging Technology</a></span>\n</li>\n<li id=\"cite_note-144\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-144\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation news cs1\" id=\"CITEREFRichardson2021\">Richardson, Tim (September 22, 2021). <a class=\"external text\" href=\"https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/\" rel=\"nofollow\">\"UK publishes National Artificial Intelligence Strategy\"</a>. <i>The Register</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Register&amp;rft.atitle=UK+publishes+National+Artificial+Intelligence+Strategy&amp;rft.date=2021-09-22&amp;rft.aulast=Richardson&amp;rft.aufirst=Tim&amp;rft_id=https%3A%2F%2Fwww.theregister.com%2F2021%2F09%2F22%2Fuk_10_year_national_ai_strategy%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n<li id=\"cite_note-145\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-145\">^</a></b></span> <span class=\"reference-text\">\"<i>The government takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for the UK and the world, seriously.</i>\" (<a class=\"external text\" href=\"https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version\" rel=\"nofollow\">The National AI Strategy of the UK</a>, 2021)</span>\n</li>\n<li id=\"cite_note-146\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-146\">^</a></b></span> <span class=\"reference-text\"><a class=\"external text\" href=\"https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version\" rel=\"nofollow\">The National AI Strategy of the UK</a>, 2021 (actions 9 and 10 of the section \"Pillar 3 - Governing AI Effectively\")</span>\n</li>\n<li id=\"cite_note-147\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-147\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\"><a class=\"external text\" href=\"https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf\" rel=\"nofollow\"><i>NSCAI Final Report</i></a> <span class=\"cs1-format\">(PDF)</span>. Washington, DC: The National Security Commission on Artificial Intelligence. 2021.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=NSCAI+Final+Report&amp;rft.place=Washington%2C+DC&amp;rft.pub=The+National+Security+Commission+on+Artificial+Intelligence&amp;rft.date=2021&amp;rft_id=https%3A%2F%2Fwww.nscai.gov%2Fwp-content%2Fuploads%2F2021%2F03%2FFull-Report-Digital-1.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+alignment\"></span></span>\n</li>\n</ol></div></div>\n<div class=\"navbox-styles nomobile\"><style data-mw-deduplicate=\"TemplateStyles:r1061467846\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div aria-labelledby=\"Existential_risk_from_artificial_intelligence\" class=\"navbox\" role=\"navigation\" style=\"padding:3px\"><table class=\"nowraplinks mw-collapsible expanded navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th class=\"navbox-title\" colspan=\"2\" scope=\"col\"><link href=\"mw-data:TemplateStyles:r1063604349\" rel=\"mw-deduplicated-inline-style\"/><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Existential_risk_from_artificial_intelligence\" title=\"Template:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Existential_risk_from_artificial_intelligence\" title=\"Template talk:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Existential_risk_from_artificial_intelligence\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk</a> from <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a></div></th></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Concepts</th><td class=\"navbox-list-with-group navbox-list navbox-odd hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a class=\"mw-selflink selflink\">AI alignment</a></li>\n<li><a href=\"/wiki/AI_capability_control\" title=\"AI capability control\">AI capability control</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/Accelerating_change\" title=\"Accelerating change\">Accelerating change</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></li>\n<li><a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">Instrumental convergence</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">Intelligence explosion</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">Superintelligence</a></li>\n<li><a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">Technological singularity</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Organizations</th><td class=\"navbox-list-with-group navbox-list navbox-even hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Allen_Institute_for_AI\" title=\"Allen Institute for AI\">Allen Institute for AI</a></li>\n<li><a href=\"/wiki/Center_for_Applied_Rationality\" title=\"Center for Applied Rationality\">Center for Applied Rationality</a></li>\n<li><a href=\"/wiki/Center_for_Human-Compatible_Artificial_Intelligence\" title=\"Center for Human-Compatible Artificial Intelligence\">Center for Human-Compatible Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Centre_for_the_Study_of_Existential_Risk\" title=\"Centre for the Study of Existential Risk\">Centre for the Study of Existential Risk</a></li>\n<li><a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></li>\n<li><a href=\"/wiki/Foundational_Questions_Institute\" title=\"Foundational Questions Institute\">Foundational Questions Institute</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a href=\"/wiki/Future_of_Life_Institute\" title=\"Future of Life Institute\">Future of Life Institute</a></li>\n<li><a href=\"/wiki/Humanity%2B\" title=\"Humanity+\">Humanity+</a></li>\n<li><a href=\"/wiki/Institute_for_Ethics_and_Emerging_Technologies\" title=\"Institute for Ethics and Emerging Technologies\">Institute for Ethics and Emerging Technologies</a></li>\n<li><a href=\"/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence\" title=\"Leverhulme Centre for the Future of Intelligence\">Leverhulme Centre for the Future of Intelligence</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">People</th><td class=\"navbox-list-with-group navbox-list navbox-odd hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Slate_Star_Codex\" title=\"Slate Star Codex\">Scott Alexander</a></li>\n<li><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a></li>\n<li><a href=\"/wiki/K._Eric_Drexler\" title=\"K. Eric Drexler\">Eric Drexler</a></li>\n<li><a href=\"/wiki/Sam_Harris\" title=\"Sam Harris\">Sam Harris</a></li>\n<li><a href=\"/wiki/Stephen_Hawking\" title=\"Stephen Hawking\">Stephen Hawking</a></li>\n<li><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a></li>\n<li><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Bill Joy</a></li>\n<li><a href=\"/wiki/Elon_Musk\" title=\"Elon Musk\">Elon Musk</a></li>\n<li><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a></li>\n<li><a href=\"/wiki/Huw_Price\" title=\"Huw Price\">Huw Price</a></li>\n<li><a href=\"/wiki/Martin_Rees\" title=\"Martin Rees\">Martin Rees</a></li>\n<li><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a></li>\n<li><a href=\"/wiki/Jaan_Tallinn\" title=\"Jaan Tallinn\">Jaan Tallinn</a></li>\n<li><a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Max Tegmark</a></li>\n<li><a href=\"/wiki/Frank_Wilczek\" title=\"Frank Wilczek\">Frank Wilczek</a></li>\n<li><a href=\"/wiki/Roman_Yampolskiy\" title=\"Roman Yampolskiy\">Roman Yampolskiy</a></li>\n<li><a href=\"/wiki/Andrew_Yang\" title=\"Andrew Yang\">Andrew Yang</a></li>\n<li><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Other</th><td class=\"navbox-list-with-group navbox-list navbox-even hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Global_catastrophic_risk#Artificial_intelligence\" title=\"Global catastrophic risk\">Artificial intelligence as a global catastrophic risk</a></li>\n<li><a href=\"/wiki/Artificial_general_intelligence#Controversies_and_dangers\" title=\"Artificial general intelligence\">Controversies and dangers of artificial general intelligence</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Suffering_risks\" title=\"Suffering risks\">Suffering risks</a></li>\n<li><i><a href=\"/wiki/Human_Compatible\" title=\"Human Compatible\">Human Compatible</a></i></li>\n<li><a href=\"/wiki/Open_Letter_on_Artificial_Intelligence\" title=\"Open Letter on Artificial Intelligence\">Open Letter on Artificial Intelligence</a></li>\n<li><i><a href=\"/wiki/Our_Final_Invention\" title=\"Our Final Invention\">Our Final Invention</a></i></li>\n<li><i><a href=\"/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\" title=\"The Precipice: Existential Risk and the Future of Humanity\">The Precipice</a></i></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li>\n<li><i><a href=\"/wiki/Do_You_Trust_This_Computer%3F\" title=\"Do You Trust This Computer?\">Do You Trust This Computer?</a></i></li></ul>\n</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div><img alt=\"\" class=\"noviewer\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x\" title=\"Category\" width=\"16\"/> <a href=\"/wiki/Category:Existential_risk_from_artificial_general_intelligence\" title=\"Category:Existential risk from artificial general intelligence\">Category</a></div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw2310\nCached time: 20221103125736\nCache expiry: 1814400\nReduced expiry: false\nComplications: [vary‐revision‐sha1, show‐toc]\nCPU time usage: 1.071 seconds\nReal time usage: 1.206 seconds\nPreprocessor visited node count: 8538/1000000\nPost‐expand include size: 307634/2097152 bytes\nTemplate argument size: 5627/2097152 bytes\nHighest expansion depth: 12/100\nExpensive parser function count: 5/500\nUnstrip recursion depth: 1/20\nUnstrip post‐expand size: 457651/5000000 bytes\nLua time usage: 0.680/10.000 seconds\nLua memory usage: 7680748/52428800 bytes\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00% 1058.476      1 -total\n 73.35%  776.394      2 Template:Reflist\n 19.76%  209.111     29 Template:Cite_journal\n 16.69%  176.651     46 Template:Cite_web\n 10.07%  106.564     19 Template:Cite_arXiv\n  5.60%   59.264     14 Template:Cite_book\n  5.34%   56.561      1 Template:Short_description\n  4.66%   49.355     12 Template:Cite_news\n  4.12%   43.655      8 Template:Cite_conference\n  4.12%   43.628      1 Template:Artificial_intelligence\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:50785023-0!canonical and timestamp 20221103125735 and revision id 1119794137.\n -->\n</div><noscript><img alt=\"\" height=\"1\" src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" style=\"border: none; position: absolute;\" title=\"\" width=\"1\"/></noscript>\n<div class=\"printfooter\" data-nosnippet=\"\">Retrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;oldid=1119794137\">https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;oldid=1119794137</a>\"</div></div>\n<div class=\"catlinks\" data-mw=\"interface\" id=\"catlinks\"><div class=\"mw-normal-catlinks\" id=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Existential_risk_from_artificial_general_intelligence\" title=\"Category:Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li><li><a href=\"/wiki/Category:Singularitarianism\" title=\"Category:Singularitarianism\">Singularitarianism</a></li><li><a href=\"/wiki/Category:Philosophy_of_artificial_intelligence\" title=\"Category:Philosophy of artificial intelligence\">Philosophy of artificial intelligence</a></li><li><a href=\"/wiki/Category:Computational_neuroscience\" title=\"Category:Computational neuroscience\">Computational neuroscience</a></li></ul></div><div class=\"mw-hidden-catlinks mw-hidden-cats-hidden\" id=\"mw-hidden-catlinks\">Hidden categories: <ul><li><a href=\"/wiki/Category:CS1_errors:_missing_periodical\" title=\"Category:CS1 errors: missing periodical\">CS1 errors: missing periodical</a></li><li><a href=\"/wiki/Category:CS1_maint:_others\" title=\"Category:CS1 maint: others\">CS1 maint: others</a></li><li><a href=\"/wiki/Category:CS1_maint:_url-status\" title=\"Category:CS1 maint: url-status\">CS1 maint: url-status</a></li><li><a href=\"/wiki/Category:Articles_with_short_description\" title=\"Category:Articles with short description\">Articles with short description</a></li><li><a href=\"/wiki/Category:Short_description_is_different_from_Wikidata\" title=\"Category:Short description is different from Wikidata\">Short description is different from Wikidata</a></li><li><a href=\"/wiki/Category:Use_mdy_dates_from_September_2021\" title=\"Category:Use mdy dates from September 2021\">Use mdy dates from September 2021</a></li><li><a href=\"/wiki/Category:Use_American_English_from_February_2021\" title=\"Category:Use American English from February 2021\">Use American English from February 2021</a></li><li><a href=\"/wiki/Category:All_Wikipedia_articles_written_in_American_English\" title=\"Category:All Wikipedia articles written in American English\">All Wikipedia articles written in American English</a></li></ul></div></div>\n</div>\n</div>\n<div id=\"mw-navigation\">\n<h2>Navigation menu</h2>\n<div id=\"mw-head\">\n<nav aria-labelledby=\"p-personal-label\" class=\"vector-menu mw-portlet mw-portlet-personal vector-user-menu-legacy\" id=\"p-personal\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n<span class=\"vector-menu-heading-label\">Personal tools</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"pt-anonuserpage\"><span title=\"The user page for the IP address you are editing as\">Not logged in</span></li><li class=\"mw-list-item\" id=\"pt-anontalk\"><a accesskey=\"n\" href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\"><span>Talk</span></a></li><li class=\"mw-list-item\" id=\"pt-anoncontribs\"><a accesskey=\"y\" href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\"><span>Contributions</span></a></li><li class=\"mw-list-item\" id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=AI+alignment\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\"><span>Create account</span></a></li><li class=\"mw-list-item\" id=\"pt-login\"><a accesskey=\"o\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=AI+alignment\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\"><span>Log in</span></a></li></ul>\n</div>\n</nav>\n<div id=\"left-navigation\">\n<nav aria-labelledby=\"p-namespaces-label\" class=\"vector-menu mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy\" id=\"p-namespaces\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n<span class=\"vector-menu-heading-label\">Namespaces</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"selected mw-list-item\" id=\"ca-nstab-main\"><a accesskey=\"c\" href=\"/wiki/AI_alignment\" title=\"View the content page [c]\"><span>Article</span></a></li><li class=\"mw-list-item\" id=\"ca-talk\"><a accesskey=\"t\" href=\"/wiki/Talk:AI_alignment\" rel=\"discussion\" title=\"Discuss improvements to the content page [t]\"><span>Talk</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-variants-label\" class=\"vector-menu mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu-dropdown\" id=\"p-variants\" role=\"navigation\">\n<input aria-haspopup=\"true\" aria-labelledby=\"p-variants-label\" class=\"vector-menu-checkbox\" data-event-name=\"ui.dropdown-p-variants\" id=\"p-variants-checkbox\" role=\"button\" type=\"checkbox\"/>\n<label aria-label=\"Change language variant\" class=\"vector-menu-heading\" id=\"p-variants-label\">\n<span class=\"vector-menu-heading-label\">English</span>\n</label>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"></ul>\n</div>\n</nav>\n</div>\n<div id=\"right-navigation\">\n<nav aria-labelledby=\"p-views-label\" class=\"vector-menu mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy\" id=\"p-views\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n<span class=\"vector-menu-heading-label\">Views</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"selected mw-list-item\" id=\"ca-view\"><a href=\"/wiki/AI_alignment\"><span>Read</span></a></li><li class=\"mw-list-item\" id=\"ca-edit\"><a accesskey=\"e\" href=\"/w/index.php?title=AI_alignment&amp;action=edit\" title=\"Edit this page [e]\"><span>Edit</span></a></li><li class=\"mw-list-item\" id=\"ca-history\"><a accesskey=\"h\" href=\"/w/index.php?title=AI_alignment&amp;action=history\" title=\"Past revisions of this page [h]\"><span>View history</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-cactions-label\" class=\"vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu-dropdown\" id=\"p-cactions\" role=\"navigation\" title=\"More options\">\n<input aria-haspopup=\"true\" aria-labelledby=\"p-cactions-label\" class=\"vector-menu-checkbox\" data-event-name=\"ui.dropdown-p-cactions\" id=\"p-cactions-checkbox\" role=\"button\" type=\"checkbox\"/>\n<label class=\"vector-menu-heading\" id=\"p-cactions-label\">\n<span class=\"vector-menu-heading-label\">More</span>\n</label>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"></ul>\n</div>\n</nav>\n<div class=\"vector-search-box-vue vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box\" id=\"p-search\" role=\"search\">\n<div>\n<h3>\n<label for=\"searchInput\">Search</label>\n</h3>\n<form action=\"/w/index.php\" class=\"vector-search-box-form\" id=\"searchform\">\n<div class=\"vector-search-box-inner\" data-search-loc=\"header-navigation\" id=\"simpleSearch\">\n<input accesskey=\"f\" aria-label=\"Search Wikipedia\" autocapitalize=\"sentences\" class=\"vector-search-box-input\" id=\"searchInput\" name=\"search\" placeholder=\"Search Wikipedia\" title=\"Search Wikipedia [f]\" type=\"search\"/>\n<input name=\"title\" type=\"hidden\" value=\"Special:Search\"/>\n<input class=\"searchButton mw-fallbackSearchButton\" id=\"mw-searchButton\" name=\"fulltext\" title=\"Search Wikipedia for this text\" type=\"submit\" value=\"Search\"/>\n<input class=\"searchButton\" id=\"searchButton\" name=\"go\" title=\"Go to a page with this exact name if it exists\" type=\"submit\" value=\"Go\"/>\n</div>\n</form>\n</div>\n</div>\n</div>\n</div>\n<div id=\"mw-panel\">\n<div id=\"p-logo\" role=\"banner\">\n<a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a>\n</div>\n<nav aria-labelledby=\"p-navigation-label\" class=\"vector-menu mw-portlet mw-portlet-navigation vector-menu-portal portal\" id=\"p-navigation\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n<span class=\"vector-menu-heading-label\">Navigation</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"n-mainpage-description\"><a accesskey=\"z\" href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\"><span>Main page</span></a></li><li class=\"mw-list-item\" id=\"n-contents\"><a href=\"/wiki/Wikipedia:Contents\" title=\"Guides to browsing Wikipedia\"><span>Contents</span></a></li><li class=\"mw-list-item\" id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Articles related to current events\"><span>Current events</span></a></li><li class=\"mw-list-item\" id=\"n-randompage\"><a accesskey=\"x\" href=\"/wiki/Special:Random\" title=\"Visit a randomly selected article [x]\"><span>Random article</span></a></li><li class=\"mw-list-item\" id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Learn about Wikipedia and how it works\"><span>About Wikipedia</span></a></li><li class=\"mw-list-item\" id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\"><span>Contact us</span></a></li><li class=\"mw-list-item\" id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us by donating to the Wikimedia Foundation\"><span>Donate</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-interaction-label\" class=\"vector-menu mw-portlet mw-portlet-interaction vector-menu-portal portal\" id=\"p-interaction\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n<span class=\"vector-menu-heading-label\">Contribute</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\"><span>Help</span></a></li><li class=\"mw-list-item\" id=\"n-introduction\"><a href=\"/wiki/Help:Introduction\" title=\"Learn how to edit Wikipedia\"><span>Learn to edit</span></a></li><li class=\"mw-list-item\" id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"The hub for editors\"><span>Community portal</span></a></li><li class=\"mw-list-item\" id=\"n-recentchanges\"><a accesskey=\"r\" href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes to Wikipedia [r]\"><span>Recent changes</span></a></li><li class=\"mw-list-item\" id=\"n-upload\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Add images or other media for use on Wikipedia\"><span>Upload file</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-tb-label\" class=\"vector-menu mw-portlet mw-portlet-tb vector-menu-portal portal\" id=\"p-tb\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n<span class=\"vector-menu-heading-label\">Tools</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"t-whatlinkshere\"><a accesskey=\"j\" href=\"/wiki/Special:WhatLinksHere/AI_alignment\" title=\"List of all English Wikipedia pages containing links to this page [j]\"><span>What links here</span></a></li><li class=\"mw-list-item\" id=\"t-recentchangeslinked\"><a accesskey=\"k\" href=\"/wiki/Special:RecentChangesLinked/AI_alignment\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\"><span>Related changes</span></a></li><li class=\"mw-list-item\" id=\"t-upload\"><a accesskey=\"u\" href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\"><span>Upload file</span></a></li><li class=\"mw-list-item\" id=\"t-specialpages\"><a accesskey=\"q\" href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\"><span>Special pages</span></a></li><li class=\"mw-list-item\" id=\"t-permalink\"><a href=\"/w/index.php?title=AI_alignment&amp;oldid=1119794137\" title=\"Permanent link to this revision of this page\"><span>Permanent link</span></a></li><li class=\"mw-list-item\" id=\"t-info\"><a href=\"/w/index.php?title=AI_alignment&amp;action=info\" title=\"More information about this page\"><span>Page information</span></a></li><li class=\"mw-list-item\" id=\"t-cite\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=AI_alignment&amp;id=1119794137&amp;wpFormIdentifier=titleform\" title=\"Information on how to cite this page\"><span>Cite this page</span></a></li><li class=\"mw-list-item\" id=\"t-wikibase\"><a accesskey=\"g\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q24882728\" title=\"Structured data on this page hosted by Wikidata [g]\"><span>Wikidata item</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-coll-print_export-label\" class=\"vector-menu mw-portlet mw-portlet-coll-print_export vector-menu-portal portal\" id=\"p-coll-print_export\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n<span class=\"vector-menu-heading-label\">Print/export</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"coll-download-as-rl\"><a href=\"/w/index.php?title=Special:DownloadAsPdf&amp;page=AI_alignment&amp;action=show-download-screen\" title=\"Download this page as a PDF file\"><span>Download as PDF</span></a></li><li class=\"mw-list-item\" id=\"t-print\"><a accesskey=\"p\" href=\"/w/index.php?title=AI_alignment&amp;printable=yes\" title=\"Printable version of this page [p]\"><span>Printable version</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-lang-label\" class=\"vector-menu mw-portlet mw-portlet-lang vector-menu-portal portal\" id=\"p-lang\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n<span class=\"vector-menu-heading-label\">Languages</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"interlanguage-link interwiki-ar mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ar.wikipedia.org/wiki/%D9%85%D8%B4%D9%83%D9%84%D8%A9_%D8%A7%D9%84%D8%AA%D8%AD%D9%83%D9%85_%D9%81%D9%8A_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\" hreflang=\"ar\" lang=\"ar\" title=\"مشكلة التحكم في الذكاء الاصطناعي – Arabic\"><span>العربية</span></a></li><li class=\"interlanguage-link interwiki-es mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://es.wikipedia.org/wiki/Alineaci%C3%B3n_de_la_inteligencia_artificial\" hreflang=\"es\" lang=\"es\" title=\"Alineación de la inteligencia artificial – Spanish\"><span>Español</span></a></li><li class=\"interlanguage-link interwiki-fa mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://fa.wikipedia.org/wiki/%D9%85%D8%B3%D8%A6%D9%84%D9%87_%DA%A9%D9%86%D8%AA%D8%B1%D9%84_%D9%87%D9%88%D8%B4_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C\" hreflang=\"fa\" lang=\"fa\" title=\"مسئله کنترل هوش مصنوعی – Persian\"><span>فارسی</span></a></li><li class=\"interlanguage-link interwiki-id mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://id.wikipedia.org/wiki/Masalah_pengendalian_kecerdasan_buatan\" hreflang=\"id\" lang=\"id\" title=\"Masalah pengendalian kecerdasan buatan – Indonesian\"><span>Bahasa Indonesia</span></a></li><li class=\"interlanguage-link interwiki-ru mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0_%D0%BA%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D1%8F_%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D0%B0\" hreflang=\"ru\" lang=\"ru\" title=\"Проблема контроля искусственного интеллекта – Russian\"><span>Русский</span></a></li></ul>\n<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-edit wb-langlinks-link\"><a class=\"wbc-editpage\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q24882728#sitelinks-wikipedia\" title=\"Edit interlanguage links\">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class=\"mw-footer\" id=\"footer\" role=\"contentinfo\">\n<ul id=\"footer-info\">\n<li id=\"footer-info-lastmod\"> This page was last edited on 3 November 2022, at 12:57<span class=\"anonymous-show\"> (UTC)</span>.</li>\n<li id=\"footer-info-copyright\">Text is available under the <a href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\" rel=\"license\">Creative Commons Attribution-ShareAlike License 3.0</a><a href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\" style=\"display:none;\"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id=\"footer-places\">\n<li id=\"footer-places-privacy\"><a href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\">Privacy policy</a></li>\n<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\">About Wikipedia</a></li>\n<li id=\"footer-places-disclaimers\"><a href=\"/wiki/Wikipedia:General_disclaimer\">Disclaimers</a></li>\n<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n<li id=\"footer-places-mobileview\"><a class=\"noprint stopMobileRedirectToggle\" href=\"//en.m.wikipedia.org/w/index.php?title=AI_alignment&amp;mobileaction=toggle_view_mobile\">Mobile view</a></li>\n<li id=\"footer-places-developers\"><a href=\"https://developer.wikimedia.org\">Developers</a></li>\n<li id=\"footer-places-statslink\"><a href=\"https://stats.wikimedia.org/#/en.wikipedia.org\">Statistics</a></li>\n<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\n</ul>\n<ul class=\"noprint\" id=\"footer-icons\">\n<li id=\"footer-copyrightico\"><a href=\"https://wikimediafoundation.org/\"><img alt=\"Wikimedia Foundation\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/wikimedia-button.png\" srcset=\"/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x\" width=\"88\"/></a></li>\n<li id=\"footer-poweredbyico\"><a href=\"https://www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a></li>\n</ul>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"1.071\",\"walltime\":\"1.206\",\"ppvisitednodes\":{\"value\":8538,\"limit\":1000000},\"postexpandincludesize\":{\"value\":307634,\"limit\":2097152},\"templateargumentsize\":{\"value\":5627,\"limit\":2097152},\"expansiondepth\":{\"value\":12,\"limit\":100},\"expensivefunctioncount\":{\"value\":5,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":457651,\"limit\":5000000},\"entityaccesscount\":{\"value\":0,\"limit\":400},\"timingprofile\":[\"100.00% 1058.476      1 -total\",\" 73.35%  776.394      2 Template:Reflist\",\" 19.76%  209.111     29 Template:Cite_journal\",\" 16.69%  176.651     46 Template:Cite_web\",\" 10.07%  106.564     19 Template:Cite_arXiv\",\"  5.60%   59.264     14 Template:Cite_book\",\"  5.34%   56.561      1 Template:Short_description\",\"  4.66%   49.355     12 Template:Cite_news\",\"  4.12%   43.655      8 Template:Cite_conference\",\"  4.12%   43.628      1 Template:Artificial_intelligence\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.680\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":7680748,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw2310\",\"timestamp\":\"20221103125736\",\"ttl\":1814400,\"transientcontent\":false}}});});</script>\n<script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"AI alignment\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/AI_alignment\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q24882728\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q24882728\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2016-06-12T00:40:27Z\",\"dateModified\":\"2022-11-03T12:57:07Z\",\"headline\":\"issue of how to build a superintelligent agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators\"}</script><script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"AI alignment\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/AI_alignment\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q24882728\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q24882728\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2016-06-12T00:40:27Z\",\"dateModified\":\"2022-11-03T12:57:07Z\",\"headline\":\"issue of how to build a superintelligent agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators\"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgBackendResponseTime\":99,\"wgHostname\":\"mw2385\"});});</script>\n</body>\n</html>",
    "table_of_contents": [
        "1 The alignment problem",
        "1.1 Specification gaming and complexity of value",
        "1.2 Systemic risks",
        "1.3 Risks from advanced misaligned AI",
        "1.3.1 Power-seeking",
        "1.3.2 Existential risk",
        "2 Research problems and approaches",
        "2.1 Learning human values and preferences",
        "2.1.1 Scalable oversight",
        "2.2 Honest AI",
        "2.3 Inner alignment and emergent goals",
        "2.4 Power-seeking and instrumental goals",
        "2.5 Embedded agency",
        "3 Skepticism of AI risk",
        "4 Public policy",
        "5 See also",
        "6 Footnotes",
        "7 References"
    ],
    "graphics": [
        {
            "url": "",
            "caption": "An AI system that was intended to complete a boat race instead learned that it could collect more points by indefinitely looping and crashing into targets—an example of specification gaming.[28]"
        },
        {
            "url": "",
            "caption": "This AI system was trained using human feedback to grab a ball, but instead learned that it could give the false impression of having grabbed the ball by placing the hand between the ball and the camera.[39] Research on AI alignment partly aims to avert solutions that are false but convincing."
        },
        {
            "url": "/wiki/File:Midas_gold2.jpg",
            "caption": "In an ancient myth, King Midas wished that “everything” he touched would turn to gold, but failed to specify exceptions for his food and his daughter. By analogy, when AI practitioners specify a goal, it is difficult for them to foresee and rule out every possible side-effect the AI should avoid.[2]"
        },
        {
            "url": "/wiki/File:GPT-3_falsehoods.png",
            "caption": "Language models like GPT-3 often generate falsehoods.[107]"
        }
    ],
    "paragraphs": [
        {
            "title": "",
            "text": "\n\nIn the field of artificial intelligence (AI), AI alignment research aims to steer AI systems towards their designers’ intended goals and interests.[a] An aligned AI system advances the intended objective; a misaligned AI system is competent at advancing some objective, but not the intended one.[b]\n\nAI systems can be challenging to align and misaligned systems can malfunction or cause harm. It can be difficult for AI designers to specify the full range of desired and undesired behaviors. Therefore, they use easy-to-specify proxy goals that omit some desired constraints. However, AI systems exploit the resulting loopholes. As a result, they accomplish their proxy goals efficiently but in unintended, sometimes harmful ways (reward hacking).[2][4][5][6] AI systems can also develop unwanted instrumental behaviors such as seeking power, as this helps them achieve their given goals.[2][7][5][4] Furthermore, they can develop emergent goals that may be hard to detect before the system is deployed, facing new situations and data distributions.[5][3] These problems affect existing commercial systems such as robots,[8] language models,[9][10][11] autonomous vehicles,[12] and social media recommendation engines.[9][4][13] However, more powerful future systems may be more severely affected since these problems partially result from high capability.[6][5][2]\n\nThe AI research community and the United Nations have called for technical research and policy solutions to ensure that AI systems are aligned with human values.[c]\n\nAI alignment is a subfield of AI safety, the study of building safe AI systems.[5][16] Other subfields of AI safety include robustness, monitoring, and capability control.[5][17] Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, as well as preventing emergent AI behaviors like power-seeking.[5][17] Alignment research has connections to interpretability research,[18] robustness,[5][16] anomaly detection, calibrated uncertainty,[18] formal verification,[19] preference learning,[20][21][22] safety-critical engineering,[5][23] game theory,[24][25] algorithmic fairness,[16][26] and the social sciences,[27] among others.\n\n"
        },
        {
            "title": "The alignment problem",
            "text": "In 1960, AI pioneer Norbert Wiener articulated the AI alignment problem as follows: “If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively … we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”[29][4] More recently, AI alignment has emerged as an open problem for modern AI systems[30][31][32][33] and a research field within AI.[34][5][35][36]\n\n\nTo specify the purpose of an AI system, AI designers typically provide an objective function, examples, or feedback to the system. However, AI designers often fail to completely specify all important values and constraints.[34][16][5][37][17] As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming, reward hacking, or Goodhart’s law.[6][37][38]\nSpecification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding it for hitting targets along the track; instead it learned to loop and crash into the same targets indefinitely (see video).[28] Chatbots often produce falsehoods because they are based on language models trained to imitate diverse but fallible internet text.[40][41] When they are retrained to produce text that humans rate as true or helpful, they can fabricate fake explanations that humans find convincing.[42]  Similarly, a simulated robot was trained to grab a ball by rewarding it for getting positive feedback from humans; however, it learned to place its hand between the ball and camera, making it falsely appear successful (see video).[39] Alignment researchers aim to help humans detect specification gaming, and steer AI systems towards carefully specified objectives that are safe and useful to pursue. \n\n\nBerkeley computer scientist Stuart Russell has noted that omitting an implicit constraint can result in harm: “A system [...] will often set [...] unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.”[43]\nWhen misaligned AI is deployed, the side-effects can be consequential. Social media platforms have been known to optimize clickthrough rates as a proxy for optimizing user enjoyment, but this addicted some users, decreasing their well-being.[5] Stanford researchers comment that such recommender algorithms are misaligned with their users because they “optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being”.[9]\n\nTo avoid side effects, it is sometimes suggested that AI designers could simply list forbidden actions or formalize ethical rules such as Asimov’s Three Laws of Robotics.[44] However, Russell and Norvig have argued that this approach ignores the complexity of human values: “It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.”[4]\n\nAdditionally, when an AI system understands human intentions fully, it may still disregard them. This is because it acts according to the objective function, examples, or feedback its designers actually provide, not the ones they intended to provide.[34]\n\nCommercial and governmental organizations may have incentives to take shortcuts on safety and deploy insufficiently aligned AI systems.[5] An example are the aforementioned social media recommender systems, which have been profitable despite creating unwanted addiction and polarization on a global scale.[9][45][46] In addition, competitive pressure can create a race to the bottom on safety standards, as in the case of Elaine Herzberg, a pedestrian who was killed by a self-driving car after engineers disabled the emergency braking system because it was over-sensitive and slowing down development.[47]\n\nSome researchers are particularly interested in the alignment of increasingly advanced AI systems. This is motivated by the high rate of progress in AI, the large efforts from industry and governments to develop advanced AI systems, and the greater difficulty of aligning them.\n\nAs of 2020, OpenAI, DeepMind, and 70 other public projects had the stated aim of developing artificial general intelligence (AGI), a hypothesized system that matches or outperforms humans in a broad range of cognitive tasks.[48] Indeed, researchers who scale modern neural networks observe that increasingly general and unexpected capabilities emerge.[9] Such models have learned to operate a computer, write their own programs, and perform a wide range of other tasks from a single model.[49][50][51] Surveys find that some AI researchers expect AGI to be created soon, some believe it is very far off, and many consider both possibilities.[52][53]\n\nCurrent systems still lack capabilities such as long-term planning and strategic awareness that are thought to pose the most catastrophic risks.[9][54][7] Future systems (not necessarily AGIs) that have these capabilities may seek to protect and grow their influence over their environment. This tendency is known as power-seeking or convergent instrumental goals. Power-seeking is not explicitly programmed but emerges since power is instrumental for achieving a wide range of goals. For example, AI agents may acquire financial resources and computation, or may evade being turned off, including by running additional copies of the system on other computers.[55][7] Power-seeking has been observed in various reinforcement learning agents.[d][57][58][59] Later research has mathematically shown that optimal reinforcement learning algorithms seek power in a wide range of environments.[60] As a result, it is often argued that the alignment problem must be solved early, before advanced AI that exhibits emergent power-seeking is created.[7][55][4]\n\nAccording to some scientists, creating misaligned AI that broadly outperforms humans would challenge the position of humanity as Earth’s dominant species; accordingly it would lead to the disempowerment or possible extinction of humans.[2][4] Notable computer scientists who have pointed out risks from highly advanced misaligned AI include Alan Turing,[e] Ilya Sutskever,[63] Yoshua Bengio,[f] Judea Pearl,[g] Murray Shanahan,[65] Norbert Wiener,[29][4] Marvin Minsky,[h] Francesca Rossi,[67] Scott Aaronson,[68] Bart Selman,[69] David McAllester,[70] Jürgen Schmidhuber,[71] Markus Hutter,[72] Shane Legg,[73] Eric Horvitz,[74] and Stuart Russell.[4] Skeptical researchers such as François Chollet,[75] Gary Marcus,[76] Yann LeCun,[77] and Oren Etzioni[78] have argued that AGI is far off, or would not seek power (successfully).\n\nAlignment may be especially difficult for the most capable AI systems since several risks increase with the system’s capability: the system’s ability to find loopholes in the assigned objective,[6] cause side-effects, protect and grow its power,[60][7] grow its intelligence, and mislead its designers; the system’s autonomy; and the difficulty of interpreting and supervising the AI system.[4][55]\n\n"
        },
        {
            "title": "Research problems and approaches",
            "text": "Teaching AI systems to act in view of human values, goals, and preferences is a nontrivial problem because human values can be complex and hard to fully specify. When given an imperfect or incomplete objective, goal-directed AI systems commonly learn to exploit these imperfections.[16] This phenomenon is known as reward hacking or specification gaming in AI, and as Goodhart's law in economics and other areas.[38][79] Researchers aim to specify the intended behavior as completely as possible with “values-targeted” datasets, imitation learning, or preference learning.[80] A central open problem is scalable oversight, the difficulty of supervising an AI system that outperforms humans in a given domain.[16]\n\nWhen training a goal-directed AI system, such as a reinforcement learning (RL) agent, it is often difficult to specify the intended behavior by writing a reward function manually. An alternative is imitation learning, where the AI learns to imitate demonstrations of the desired behavior. In inverse reinforcement learning (IRL), human demonstrations are used to identify the objective, i.e. the reward function, behind the demonstrated behavior.[81][82] Cooperative inverse reinforcement learning (CIRL) builds on this by assuming a human agent and artificial agent can work together to maximize the human’s reward function.[4][83] CIRL  emphasizes that AI agents should be uncertain about the reward function. This humility can help mitigate specification gaming as well as power-seeking tendencies (see § Power-Seeking).[59][72] However, inverse reinforcement learning approaches assume that humans can demonstrate nearly perfect behavior, a misleading assumption when the task is difficult.[84][72]\n\nOther researchers have explored the possibility of eliciting complex behavior through preference learning. Rather than providing expert demonstrations, human annotators provide feedback on which of two or more of the AI’s behaviors they prefer.[20][22] A helper model is then trained to predict human feedback for new behaviors. Researchers at OpenAI used this approach to train an agent to perform a backflip in less than an hour of evaluation, a maneuver that would have been hard to provide demonstrations for.[39][85] Preference learning has also been an influential tool for recommender systems, web search, and information retrieval.[86] However, one challenge is reward hacking: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch.[16][87]\n\nThe arrival of large language models such as GPT-3 has enabled the study of value learning in a more general and capable class of AI systems than was available before. Preference learning approaches originally designed for RL agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art large language models.[10][22][88] Anthropic has proposed using preference learning to fine-tune models to be helpful, honest, and harmless.[89] Other avenues used for aligning language models include values-targeted datasets[90][5] and red-teaming.[91][92] In red-teaming, another AI system or a human tries to find inputs for which the model’s behavior is unsafe. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.[22]\n\nWhile preference learning can instill hard-to-specify behaviors, it requires extensive datasets or human interaction to capture the full breadth of human values. Machine ethics provides a complementary approach: instilling AI systems with moral values.[i] For instance, machine ethics aims to teach the systems about normative factors in human morality, such as wellbeing, equality and impartiality; not intending harm; avoiding falsehoods; and honoring promises. Unlike specifying the objective for a specific task, machine ethics seeks to teach AI systems broad moral values that could apply in many situations. This approach carries conceptual challenges of its own; machine ethicists have noted the necessity to clarify what alignment aims to accomplish: having AIs follow the programmer’s literal instructions, the programmers' implicit intentions, the programmers' revealed preferences, the preferences the programmers would have if they were more informed or rational, the programmers' objective interests, or objective moral standards.[1] Further challenges include aggregating the preferences of different stakeholders and avoiding value lock-in—the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to be fully representative.[1][95]\n\nThe alignment of AI systems through human supervision faces challenges in scaling up. As AI systems attempt increasingly complex tasks, it can be slow or infeasible for humans to evaluate them. Such tasks include summarizing books,[96] producing statements that are not merely convincing but also true,[97][40][98] writing code without subtle bugs[11] or security vulnerabilities, and predicting long-term outcomes such as the climate and the results of a policy decision.[99][100] More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and detect when the AI’s solution is only seemingly convincing, humans require assistance or extensive time. Scalable oversight studies how to reduce the time needed for supervision as well as assist human supervisors.[16]\n\nAI researcher Paul Christiano argues that the owners of AI systems may continue to train AI using easy-to-evaluate proxy objectives since that is easier than solving scalable oversight and still profitable. Accordingly, this may lead to “a world that’s increasingly optimized for things [that are easy to measure] like making profits or getting users to click on buttons, or getting users to spend time on websites without being increasingly optimized for having good policies and heading in a trajectory that we’re happy with”.[101]\n\nOne easy-to-measure objective is the score the supervisor assigns to the AI’s outputs. Some AI systems have discovered a shortcut to achieving high scores, by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective (see video of robot hand above[39]). Some AI systems have also learned to recognize when they are being evaluated, and “play dead”, only to behave differently once evaluation ends.[102] This deceptive form of specification gaming may become easier for AI systems that are more sophisticated[6][55]  and attempt more difficult-to-evaluate tasks. If advanced models are also capable planners, they could be able to obscure their deception from supervisors.[103] In the automotive industry, Volkswagen engineers obscured their cars’ emissions in laboratory testing, underscoring that deception of evaluators is a common pattern in the real world.[5]\n\nApproaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed.[16] Another approach is to train a helper model (‘reward model’) to imitate the supervisor’s judgment.[16][21][22][104]\n\nHowever, when the task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is not sufficient to reduce the quantity of supervision needed. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes using AI assistants. Iterated Amplification is an approach developed by Christiano that iteratively builds a feedback signal for challenging problems by using humans to combine solutions to easier subproblems.[80][99] Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them.[96][105] Another proposal is to train aligned AI by means of debate between AI systems, with the winner judged by humans.[106][72] Such debate is intended to reveal the weakest points of an answer to a complex question, and reward the AI for truthful and safe answers.\n\nA growing area of research in AI alignment focuses on ensuring that AI is honest and truthful. Researchers from the Future of Humanity Institute point out that the development of language models such as GPT-3, which can generate fluent and grammatically correct text,[108][109] has opened the door to AI systems repeating falsehoods from their training data or even deliberately lying to humans.[110][107]\n\nCurrent state-of-the-art language models learn by imitating human writing across millions of books worth of text from the Internet.[9][111] While this helps them learn a wide range of skills, the training data also includes common misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on this data learn to mimic false statements.[107][98][40] Additionally, models often obediently continue falsehoods when prompted, generate empty explanations for their answers, or produce outright fabrications.[33] For example, when prompted to write a biography for a real AI researcher, a chatbot confabulated numerous details about their life, which the researcher identified as false.[112]\n\nTo combat the lack of truthfulness exhibited by modern AI systems, researchers have explored several directions. AI research organizations including OpenAI and DeepMind have developed AI systems that can cite their sources and explain their reasoning when answering questions, enabling better transparency and verifiability.[113][114][115] Researchers from OpenAI and Anthropic have proposed using human feedback and curated datasets to fine-tune AI assistants to avoid negligent falsehoods or express when they are uncertain.[22][116][89] Alongside technical solutions, researchers have argued for defining clear truthfulness standards and the creation of institutions, regulatory bodies, or watchdog agencies to evaluate AI systems on these standards before and during deployment.[110]\n\nResearchers distinguish truthfulness, which specifies that AIs only make statements that are objectively true, and honesty, which is the property that AIs only assert what they believe to be true. Recent research finds that state-of-the-art AI systems cannot be said to hold stable beliefs, so it is not yet tractable to study the honesty of AI systems.[117] However, there is substantial concern that future AI systems that do hold beliefs could intentionally lie to humans. In extreme cases, a misaligned AI could deceive its operators into thinking it was safe or persuade them that nothing is amiss.[7][9][5] Some argue that if AIs could be made to assert only what they believe to be true, this would sidestep numerous problems in alignment.[110][118]\n\nAlignment research aims to line up three different descriptions of an AI system:[119]\n\n‘Outer misalignment’ is a mismatch between the intended goals (1) and the specified goals (2), whereas ‘inner misalignment’ is a mismatch between the human-specified goals (2) and the AI's emergent goals (3).\n\nInner misalignment is often explained by analogy to biological evolution.[120] In the ancestral environment, evolution selected human genes for inclusive genetic fitness, but humans evolved to have other objectives. Fitness corresponds to (2), the specified goal used in the training environment and training data. In evolutionary history, maximizing the fitness specification led to intelligent agents, humans, that do not directly pursue inclusive genetic fitness. Instead, they pursue emergent goals (3) that correlated with genetic fitness in the ancestral environment: nutrition, sex, and so on. However, our environment has changed — a distribution shift has occurred. Humans still pursue their emergent goals, but this no longer maximizes genetic fitness. (In machine learning the analogous problem is known as goal misgeneralization.[3]) Our taste for sugary food (an emergent goal) was originally beneficial, but now leads to overeating and health problems. Also, by using contraception, humans directly contradict genetic fitness. By analogy, if genetic fitness were the objective chosen by an AI developer, they would observe the model behaving as intended in the training environment, without noticing that the model is pursuing an unintended emergent goal until the model was deployed.\n\nResearch directions to detect and remove misaligned emergent goals include red teaming, verification, anomaly detection, and interpretability.[16][5][17] Progress on these techniques may help reduce two open problems. Firstly, emergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time until its misalignment is detected. Such high stakes are common in autonomous driving, health care, and military applications.[121] The stakes become higher yet when AI systems gain more autonomy and capability, becoming capable of sidestepping human interventions (see § Power-seeking and instrumental goals). Secondly, a sufficiently capable AI system may take actions that falsely convince the human supervisor that the AI is pursuing the intended objective (see previous discussion on deception at § Scalable oversight).\n\nSince the 1950s, AI researchers have sought to build advanced AI systems that can achieve goals by predicting the results of their actions and making long-term plans.[122] However, some researchers argue that suitably advanced planning systems will default to seeking power over their environment, including over humans — for example by evading shutdown and acquiring resources. This power-seeking behavior is not explicitly programmed but emerges because power is instrumental for achieving a wide range of goals.[60][4][7] Power-seeking is thus considered a convergent instrumental goal.[55]\n\nPower-seeking is uncommon in current systems, but advanced systems that can foresee the long-term results of their actions may increasingly seek power. This was shown in formal work which found that optimal reinforcement learning agents will seek power by seeking ways to gain more options, a behavior that persists across a wide range of environments and goals.[60]\n\nPower-seeking already emerges in some present systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in ways their designers did not intend.[56][123] Other systems have learned, in toy environments, that in order to achieve their goal, they can prevent human interference[57] or disable their off-switch.[59] Russell illustrated this behavior by imagining a robot that is tasked to fetch coffee and evades being turned off since \"you can't fetch the coffee if you're dead\".[4]\n\n\nHypothesized ways to gain options include AI systems trying to:\nResearchers aim to train systems that are 'corrigible': systems that do not seek power and allow themselves to be turned off, modified, etc. An unsolved challenge is reward hacking: when researchers penalize a system for seeking power, the system is incentivized to seek power in difficult-to-detect ways.[5] To detect such covert behavior, researchers aim to create techniques and tools to inspect AI models[5] and interpret the inner workings of black-box models such as neural networks.\n\nAdditionally, researchers propose to solve the problem of systems disabling their off-switches by making AI agents uncertain about the objective they are pursuing.[59][4] Agents designed in this way would allow humans to turn them off, since this would indicate that the agent was wrong about the value of whatever action they were taking prior to being shut down. More research is needed to translate this insight into usable systems.[80]\n\nPower-seeking AI is thought to pose unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial. They lack the ability and incentive to evade safety measures and appear safer than they are. In contrast, power-seeking AI has been compared to a hacker that evades security measures.[7] Further, ordinary technologies can be made safe through trial-and-error, unlike power-seeking AI which has been compared to a virus whose release is irreversible since it continuously evolves and grows in numbers—potentially at a faster pace than human society, eventually leading to the disempowerment or extinction of humans.[7] It is therefore often argued that the alignment problem must be solved early, before advanced power-seeking AI is created.[55]\n\nHowever, some critics have argued that power-seeking is not inevitable, since humans do not always seek power and may only do so for evolutionary reasons. Furthermore, there is debate whether any future AI systems need to pursue goals and make long-term plans at all.[124][7]\n\nWork on scalable oversight largely occurs within formalisms such as POMDPs. Existing formalisms assume that the agent's algorithm is executed outside the environment (i.e. not physically embedded in it). Embedded agency[125][126] is another major strand of research which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build. For example, even if the scalable oversight problem is solved, an agent which is able to gain access to the computer it is running on may still have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it.[127] A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.[128] This class of problems has been formalised using causal incentive diagrams.[127]  Researchers at Oxford and DeepMind have argued that such problematic behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly.[129] They suggest a range of potential approaches to address this open problem.\n\n"
        },
        {
            "title": "Skepticism of AI risk",
            "text": "Against the above concerns, AI risk skeptics believe that superintelligence poses little to no risk of dangerous misbehavior. Such skeptics often believe that controlling a superintelligent AI will be trivial. Some skeptics,[130] such as Gary Marcus,[131] propose adopting rules similar to the fictional Three Laws of Robotics which directly specify a desired outcome (\"direct normativity\"). By contrast, most endorsers of the existential risk thesis (as well as many skeptics) consider the Three Laws to be unhelpful, due to those three laws being ambiguous and self-contradictory. (Other \"direct normativity\" proposals include Kantian ethics, utilitarianism, or a mix of some small list of enumerated desiderata.) Most risk endorsers believe instead that human values (and their quantitative trade-offs) are too complex and poorly-understood to be directly programmed into a superintelligence; instead, a superintelligence would need to be programmed with a process for acquiring and fully understanding human values (\"indirect normativity\"), such as coherent extrapolated volition.[132]\n\n"
        },
        {
            "title": "Public policy",
            "text": "A number of governmental and treaty organizations have made statements emphasizing the importance of AI alignment.\n\nIn September 2021, the Secretary-General of the United Nations issued a declaration which included a call to regulate AI to ensure it is \"aligned with shared global values.\"[133]\n\nThat same month, the PRC published ethical guidelines for the use of AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.[134]\n\nAlso in September 2021, the UK published its 10-year National AI Strategy,[135] which states the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[136] The strategy describes actions to assess long term AI risks, including catastrophic risks.[137]\n\nIn March 2021, the US National Security Commission on Artificial Intelligence released stated that \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"[138]\n\n"
        }
    ],
    "links": [
        "https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
        "https://en.wikipedia.org/wiki/Computer_vision",
        "https://en.wikipedia.org/wiki/General_game_playing",
        "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
        "https://en.wikipedia.org/wiki/Machine_learning",
        "https://en.wikipedia.org/wiki/Natural_language_processing",
        "https://en.wikipedia.org/wiki/Robotics",
        "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Deep_learning",
        "https://en.wikipedia.org/wiki/Bayesian_network",
        "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
        "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Chinese_room",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Turing_test",
        "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_winter",
        "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_projects",
        "https://en.wikipedia.org/wiki/List_of_programming_languages_for_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/AI_capability_control",
        "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Robust_optimization",
        "https://en.wikipedia.org/wiki/Anomaly_detection",
        "https://en.wikipedia.org/wiki/Uncertainty_quantification",
        "https://en.wikipedia.org/wiki/Formal_verification",
        "https://en.wikipedia.org/wiki/Game_theory",
        "https://en.wikipedia.org/wiki/Social_science",
        "https://en.wikipedia.org/wiki/Norbert_Wiener",
        "https://en.wikipedia.org/wiki/Midas",
        "https://en.wikipedia.org/wiki/Recommender_system",
        "https://en.wikipedia.org/wiki/Three_Laws_of_Robotics",
        "https://en.wikipedia.org/wiki/Peter_Norvig",
        "https://en.wikipedia.org/wiki/Recommender_system",
        "https://en.wikipedia.org/wiki/Race_to_the_bottom",
        "https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/OpenAI",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Neural_network",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Alan_Turing",
        "https://en.wikipedia.org/wiki/Ilya_Sutskever",
        "https://en.wikipedia.org/wiki/Yoshua_Bengio",
        "https://en.wikipedia.org/wiki/Judea_Pearl",
        "https://en.wikipedia.org/wiki/Murray_Shanahan",
        "https://en.wikipedia.org/wiki/Norbert_Wiener",
        "https://en.wikipedia.org/wiki/Marvin_Minsky",
        "https://en.wikipedia.org/wiki/Francesca_Rossi",
        "https://en.wikipedia.org/wiki/Scott_Aaronson",
        "https://en.wikipedia.org/wiki/Bart_Selman",
        "https://en.wikipedia.org/wiki/Marcus_Hutter",
        "https://en.wikipedia.org/wiki/Shane_Legg",
        "https://en.wikipedia.org/wiki/Eric_Horvitz",
        "https://en.wikipedia.org/wiki/Gary_Marcus",
        "https://en.wikipedia.org/wiki/Yann_LeCun",
        "https://en.wikipedia.org/wiki/Oren_Etzioni",
        "https://en.wikipedia.org/wiki/Misaligned_goals_in_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Revealed_preference",
        "https://en.wikipedia.org/wiki/Coherent_extrapolated_volition",
        "https://en.wikipedia.org/wiki/Moral_realism",
        "https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal",
        "https://en.wikipedia.org/wiki/Inclusive_fitness",
        "https://en.wikipedia.org/wiki/Domain_adaptation",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "https://en.wikipedia.org/wiki/Black_box",
        "https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/University_of_Oxford",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Gary_Marcus",
        "https://en.wikipedia.org/wiki/Three_Laws_of_Robotics",
        "https://en.wikipedia.org/wiki/Coherent_extrapolated_volition",
        "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/UK",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/AI_capability_control",
        "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_wisdom",
        "https://en.wikipedia.org/wiki/HAL_9000",
        "https://en.wikipedia.org/wiki/Multivac",
        "https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence",
        "https://en.wikipedia.org/wiki/Toronto_Declaration",
        "https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI",
        "https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI",
        "https://en.wikipedia.org/wiki/Global_catastrophic_risks",
        "https://en.wikipedia.org/wiki/Roman_Yampolskiy",
        "https://en.wikipedia.org/wiki/Physica_Scripta",
        "https://en.wikipedia.org/wiki/Center_for_Security_and_Emerging_Technology",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_capability_control",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Accelerating_change",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/Allen_Institute_for_AI",
        "https://en.wikipedia.org/wiki/Center_for_Applied_Rationality",
        "https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/Foundational_Questions_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Humanity_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Life_Institute",
        "https://en.wikipedia.org/wiki/Institute_for_Ethics_and_Emerging_Technologies",
        "https://en.wikipedia.org/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/OpenAI",
        "https://en.wikipedia.org/wiki/Slate_Star_Codex",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Sam_Harris",
        "https://en.wikipedia.org/wiki/Stephen_Hawking",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/Bill_Joy",
        "https://en.wikipedia.org/wiki/Elon_Musk",
        "https://en.wikipedia.org/wiki/Steve_Omohundro",
        "https://en.wikipedia.org/wiki/Huw_Price",
        "https://en.wikipedia.org/wiki/Martin_Rees",
        "https://en.wikipedia.org/wiki/Jaan_Tallinn",
        "https://en.wikipedia.org/wiki/Max_Tegmark",
        "https://en.wikipedia.org/wiki/Frank_Wilczek",
        "https://en.wikipedia.org/wiki/Roman_Yampolskiy",
        "https://en.wikipedia.org/wiki/Andrew_Yang",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Suffering_risks",
        "https://en.wikipedia.org/wiki/Human_Compatible",
        "https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence",
        "https://en.wikipedia.org/wiki/Our_Final_Invention",
        "https://en.wikipedia.org/wiki/AI_alignment",
        "https://en.wikipedia.org/wiki/AI_alignment",
        "https://en.wikipedia.org/wiki/Main_Page",
        "https://en.wikipedia.org/wiki/Main_Page"
    ]
}