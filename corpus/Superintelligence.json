{
    "url": "https://en.wikipedia.org/wiki/Superintelligence",
    "title": "Superintelligence",
    "html": "<!DOCTYPE html>\n<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n<head>\n<meta charset=\"utf-8\"/>\n<title>Superintelligence - Wikipedia</title>\n<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"7fe2917f-385d-4975-8d22-7a152f906ebd\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Superintelligence\",\"wgTitle\":\"Superintelligence\",\"wgCurRevisionId\":1115139010,\"wgRevisionId\":1115139010,\"wgArticleId\":726659,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles with short description\",\"Short description is different from Wikidata\",\"All articles with unsourced statements\",\"Articles with unsourced statements from July 2022\",\"Articles with unsourced statements from November 2021\",\"Hypothetical technology\",\"Singularitarianism\",\n\"Intelligence\",\"Existential risk from artificial general intelligence\"],\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Superintelligence\",\"wgRelevantArticleId\":726659,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":20000,\"wgNoticeProject\":\"wikipedia\",\"wgVector2022PreviewPages\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":10,\"wgULSCurrentAutonym\":\"English\",\"wgEditSubmitButtonLabelPublish\":true,\"wgCentralAuthMobileDomain\":false,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":true,\"wgWikibaseItemId\":\"Q1566000\",\n\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"skins.vector.styles.legacy\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.legacy.js\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.cx.eventlogging.campaigns\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\n\"ext.gadget.ReferenceTooltips\",\"ext.gadget.charinsert\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.popups\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.growthExperiments.SuggestedEditSession\"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement(\"user.options@12s5i\",function($,jQuery,require,module){mw.user.tokens.set({\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});});});</script>\n<link href=\"/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<script async=\"\" src=\"/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector\"></script>\n<meta content=\"\" name=\"ResourceLoaderDynamicStyles\"/>\n<link href=\"/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<meta content=\"MediaWiki 1.40.0-wmf.8\" name=\"generator\"/>\n<meta content=\"origin\" name=\"referrer\"/>\n<meta content=\"origin-when-crossorigin\" name=\"referrer\"/>\n<meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n<meta content=\"max-image-preview:standard\" name=\"robots\"/>\n<meta content=\"telephone=no\" name=\"format-detection\"/>\n<meta content=\"width=1000\" name=\"viewport\"/>\n<meta content=\"Superintelligence - Wikipedia\" property=\"og:title\"/>\n<meta content=\"website\" property=\"og:type\"/>\n<link href=\"//upload.wikimedia.org\" rel=\"preconnect\"/>\n<link href=\"//en.m.wikipedia.org/wiki/Superintelligence\" media=\"only screen and (max-width: 720px)\" rel=\"alternate\"/>\n<link href=\"/w/index.php?title=Superintelligence&amp;action=edit\" rel=\"alternate\" title=\"Edit this page\" type=\"application/x-wiki\"/>\n<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>\n<link href=\"/static/favicon/wikipedia.ico\" rel=\"icon\"/>\n<link href=\"/w/opensearch_desc.php\" rel=\"search\" title=\"Wikipedia (en)\" type=\"application/opensearchdescription+xml\"/>\n<link href=\"//en.wikipedia.org/w/api.php?action=rsd\" rel=\"EditURI\" type=\"application/rsd+xml\"/>\n<link href=\"https://creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\"/>\n<link href=\"https://en.wikipedia.org/wiki/Superintelligence\" rel=\"canonical\"/>\n<link href=\"//meta.wikimedia.org\" rel=\"dns-prefetch\"/>\n<link href=\"//login.wikimedia.org\" rel=\"dns-prefetch\"/>\n</head>\n<body class=\"skin-vector-legacy mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Superintelligence rootpage-Superintelligence skin-vector action-view vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-sticky-header-edit-disabled vector-feature-table-of-contents-legacy-toc-disabled vector-feature-visual-enhancement-next-disabled vector-feature-page-tools-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled\"><div class=\"noprint\" id=\"mw-page-base\"></div>\n<div class=\"noprint\" id=\"mw-head-base\"></div>\n<div class=\"mw-body\" id=\"content\" role=\"main\">\n<a id=\"top\"></a>\n<div id=\"siteNotice\"><!-- CentralNotice --><!--esi <esi:include src=\"/esitest-fa8a495983347898/content\" /> --> </div>\n<div class=\"mw-indicators\">\n</div>\n<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\"><span class=\"mw-page-title-main\">Superintelligence</span></h1>\n<div class=\"vector-body\" id=\"bodyContent\">\n<div class=\"noprint\" id=\"siteSub\">From Wikipedia, the free encyclopedia</div>\n<div id=\"contentSub\"></div>\n<div id=\"contentSub2\"></div>\n<div id=\"jump-to-nav\"></div>\n<a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\n<a class=\"mw-jump-link\" href=\"#searchInput\">Jump to search</a>\n<div class=\"mw-body-content mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\"><div class=\"mw-parser-output\"><div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Hypothetical agent with intelligence surpassing human</div>\n<style data-mw-deduplicate=\"TemplateStyles:r1033289096\">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div class=\"hatnote navigation-not-searchable\" role=\"note\">For the book by Nick Bostrom, see <a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a>. For the 2020 film, see <a href=\"/wiki/Superintelligence_(film)\" title=\"Superintelligence (film)\">Superintelligence (film)</a>.</div>\n<p>A <b>superintelligence</b> is a hypothetical <a href=\"/wiki/Intelligent_agent\" title=\"Intelligent agent\">agent</a> that possesses <a href=\"/wiki/Intelligence\" title=\"Intelligence\">intelligence</a> far surpassing that of the <a href=\"/wiki/Genius\" title=\"Genius\">brightest</a> and most <a href=\"/wiki/Intellectual_giftedness\" title=\"Intellectual giftedness\">gifted</a> human minds. \"Superintelligence\" may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an <a href=\"/wiki/Technological_singularity#Intelligence_explosion\" title=\"Technological singularity\">intelligence explosion</a> and associated with a <a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">technological singularity</a>.\n</p><p><a href=\"/wiki/University_of_Oxford\" title=\"University of Oxford\">University of Oxford</a> philosopher <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> defines <i>superintelligence</i> as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\".<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom2014Chapter_2_1-0\"><a href=\"#cite_note-FOOTNOTEBostrom2014Chapter_2-1\">[1]</a></sup> The program <a href=\"/wiki/Fritz_(chess)\" title=\"Fritz (chess)\">Fritz</a> falls short of superintelligence—even though it is much better than humans at chess—because Fritz cannot outperform humans in other tasks.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201422_2-0\"><a href=\"#cite_note-FOOTNOTEBostrom201422-2\">[2]</a></sup> Following Hutter and Legg, Bostrom treats superintelligence as general dominance at goal-oriented behavior, leaving open whether an artificial or human superintelligence would possess capacities such as <a href=\"/wiki/Intentionality\" title=\"Intentionality\">intentionality</a> (cf. the <a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a> argument) or <a href=\"/wiki/Qualia\" title=\"Qualia\">first-person consciousness</a> (<a href=\"/wiki/Cf.\" title=\"Cf.\">cf.</a> the <a href=\"/wiki/Hard_problem_of_consciousness\" title=\"Hard problem of consciousness\">hard problem of consciousness</a>).\n</p><p>Technological researchers disagree about how likely present-day <a href=\"/wiki/Human_intelligence\" title=\"Human intelligence\">human intelligence</a> is to be surpassed. Some argue that advances in <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence.<sup class=\"reference\" id=\"cite_ref-3\"><a href=\"#cite_note-3\">[3]</a></sup><sup class=\"reference\" id=\"cite_ref-4\"><a href=\"#cite_note-4\">[4]</a></sup> A number of <a href=\"/wiki/Futures_studies\" title=\"Futures studies\">futures studies</a> scenarios combine elements from both of these possibilities, suggesting that humans are likely to <a href=\"/wiki/Brain%E2%80%93computer_interface\" title=\"Brain–computer interface\">interface with computers</a>, or <a href=\"/wiki/Mind_uploading\" title=\"Mind uploading\">upload their minds to computers</a>, in a way that enables substantial intelligence amplification.\n</p><p>Some researchers believe that superintelligence will likely follow shortly after the development of <a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">artificial general intelligence</a>. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of <a href=\"/wiki/Eidetic_memory\" title=\"Eidetic memory\">perfect recall</a>, a vastly superior knowledge base, and the ability to <a href=\"/wiki/Human_multitasking\" title=\"Human multitasking\">multitask</a> in ways not possible to biological entities. This may give them the opportunity to—either as a single being or as a new <a href=\"/wiki/Species\" title=\"Species\">species</a>—become much more powerful than humans, and to displace them.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom2014Chapter_2_1-1\"><a href=\"#cite_note-FOOTNOTEBostrom2014Chapter_2-1\">[1]</a></sup>\n</p><p>A number of scientists and forecasters argue for prioritizing early research into the possible benefits and risks of <a href=\"/wiki/Intelligence_amplification\" title=\"Intelligence amplification\">human and machine cognitive enhancement</a>, because of the potential social impact of such technologies.<sup class=\"reference\" id=\"cite_ref-FOOTNOTELegg2008135–137_5-0\"><a href=\"#cite_note-FOOTNOTELegg2008135–137-5\">[5]</a></sup>\n</p>\n<div aria-labelledby=\"mw-toc-heading\" class=\"toc\" id=\"toc\" role=\"navigation\"><input class=\"toctogglecheckbox\" id=\"toctogglecheckbox\" role=\"button\" style=\"display:none\" type=\"checkbox\"/><div class=\"toctitle\" dir=\"ltr\" lang=\"en\"><h2 id=\"mw-toc-heading\">Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Feasibility_of_artificial_superintelligence\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Feasibility of artificial superintelligence</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Feasibility_of_biological_superintelligence\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Feasibility of biological superintelligence</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Forecasts\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Forecasts</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Design_considerations\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Design considerations</span></a></li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Potential_threat_to_humanity\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Potential threat to humanity</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#See_also\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Citations\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Citations</span></a></li>\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#Bibliography\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">Bibliography</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Papers\"><span class=\"tocnumber\">8.1</span> <span class=\"toctext\">Papers</span></a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Books\"><span class=\"tocnumber\">8.2</span> <span class=\"toctext\">Books</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#External_links\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"Feasibility_of_artificial_superintelligence\">Feasibility of artificial superintelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=1\" title=\"Edit section: Feasibility of artificial superintelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a class=\"image\" href=\"/wiki/File:Classification_of_images_progress_human.png\"><img alt=\"\" class=\"thumbimage\" data-file-height=\"967\" data-file-width=\"1445\" decoding=\"async\" height=\"147\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/220px-Classification_of_images_progress_human.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/330px-Classification_of_images_progress_human.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/440px-Classification_of_images_progress_human.png 2x\" width=\"220\"/></a> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:Classification_of_images_progress_human.png\" title=\"Enlarge\"></a></div>Progress in machine classification of images <hr/> The error rate of AI by year. The red line represents the error rate of a trained human.</div></div></div>\n<p>Philosopher <a href=\"/wiki/David_Chalmers\" title=\"David Chalmers\">David Chalmers</a> argues that <a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">artificial general intelligence</a> is a very likely path to superhuman intelligence. Chalmers breaks this claim down into an argument that AI can achieve <i>equivalence</i> to human intelligence, that it can be <i>extended</i> to surpass human intelligence, and that it can be further <i>amplified</i> to completely dominate humans across arbitrary tasks.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEChalmers20107_6-0\"><a href=\"#cite_note-FOOTNOTEChalmers20107-6\">[6]</a></sup>\n</p><p>Concerning human-level equivalence, Chalmers argues that the human brain is a mechanical system, and therefore ought to be emulatable by synthetic materials.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEChalmers20107-9_7-0\"><a href=\"#cite_note-FOOTNOTEChalmers20107-9-7\">[7]</a></sup> He also notes that human intelligence was able to biologically evolve, making it more likely that human engineers will be able to recapitulate this invention. <a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">Evolutionary algorithms</a> in particular should be able to produce human-level AI.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEChalmers201010-11_8-0\"><a href=\"#cite_note-FOOTNOTEChalmers201010-11-8\">[8]</a></sup> Concerning intelligence extension and amplification, Chalmers argues that new AI technologies can generally be improved on, and that this is particularly likely when the invention can assist in designing new technologies.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEChalmers201011-13_9-0\"><a href=\"#cite_note-FOOTNOTEChalmers201011-13-9\">[9]</a></sup>\n</p><p>If research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\".<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (July 2022)\">citation needed</span></a></i>]</sup> It would then be even better at improving itself, and could continue doing so in a rapidly increasing cycle, leading to a superintelligence. This scenario is known as an <a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">intelligence explosion</a>.  Such an intelligence would not have the limitations of human intellect, and may be able to invent or discover almost anything. However, it is also possible that any such intelligence would conclude that <a href=\"/wiki/Existential_nihilism\" title=\"Existential nihilism\">existential nihilism</a> is correct and immediately destroy itself, making any kind of superintelligence inherently unstable.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (July 2022)\">citation needed</span></a></i>]</sup>\n</p><p>Computer components already greatly surpass human performance in speed.  Bostrom writes, \"Biological neurons operate at a peak speed of about 200 Hz, a full seven orders of magnitude slower than a modern microprocessor (~2 GHz).\"<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201459_10-0\"><a href=\"#cite_note-FOOTNOTEBostrom201459-10\">[10]</a></sup> Moreover, <a href=\"/wiki/Neuron\" title=\"Neuron\">neurons</a> transmit spike signals across <a href=\"/wiki/Axon\" title=\"Axon\">axons</a> at no greater than 120 m/s, \"whereas existing electronic processing cores can communicate optically at the speed of light\". Thus, the simplest example of a superintelligence may be an emulated human mind run on much faster hardware than the brain. A human-like reasoner that could think millions of times faster than current humans would have a dominant advantage in most reasoning tasks, particularly ones that require haste or long strings of actions.\n</p><p>Another advantage of computers is modularity, that is, their size or computational capacity can be increased.  A non-human (or modified human) brain could become much larger than a present-day human brain, like many <a href=\"/wiki/Supercomputer\" title=\"Supercomputer\">supercomputers</a>. Bostrom also raises the possibility of <i>collective superintelligence</i>: a large enough number of separate reasoning systems, if they communicated and coordinated well enough, could act in aggregate with far greater capabilities than any sub-agent.\n</p><p>There may also be ways to <i>qualitatively</i> improve on human reasoning and decision-making. Humans appear to differ from <a class=\"mw-redirect\" href=\"/wiki/Common_chimpanzee\" title=\"Common chimpanzee\">chimpanzees</a> in the ways we think more than we differ in brain size or speed.<sup class=\"reference\" id=\"cite_ref-11\"><a href=\"#cite_note-11\">[11]</a></sup> Humans outperform non-human animals in large part because of new or enhanced reasoning capacities, such as long-term planning and <a href=\"/wiki/Great_ape_language\" title=\"Great ape language\">language use</a>. (See <a href=\"/wiki/Evolution_of_human_intelligence\" title=\"Evolution of human intelligence\">evolution of human intelligence</a> and <a href=\"/wiki/Primate_cognition\" title=\"Primate cognition\">primate cognition</a>.) If there are other possible improvements to reasoning that would have a similarly large impact, this makes it likelier that an agent can be built that outperforms humans in the same fashion humans outperform chimpanzees.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201456–57_12-0\"><a href=\"#cite_note-FOOTNOTEBostrom201456–57-12\">[12]</a></sup>\n</p><p>All of the above advantages hold for artificial superintelligence, but it is not clear how many hold for biological superintelligence. Physiological constraints limit the speed and size of biological brains in many ways that are inapplicable to machine intelligence. As such, writers on superintelligence have devoted much more attention to superintelligent AI scenarios.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201452,_59–61_13-0\"><a href=\"#cite_note-FOOTNOTEBostrom201452,_59–61-13\">[13]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Feasibility_of_biological_superintelligence\">Feasibility of biological superintelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=2\" title=\"Edit section: Feasibility of biological superintelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><a href=\"/wiki/Carl_Sagan\" title=\"Carl Sagan\">Carl Sagan</a> suggested that the advent of <a href=\"/wiki/Caesarean_section\" title=\"Caesarean section\">Caesarean sections</a> and <a href=\"/wiki/In_vitro_fertilisation\" title=\"In vitro fertilisation\"><i>in vitro</i> fertilization</a> may permit humans to evolve larger heads, resulting in improvements via <a href=\"/wiki/Natural_selection\" title=\"Natural selection\">natural selection</a> in the <a href=\"/wiki/Adaptive_evolution_in_the_human_genome\" title=\"Adaptive evolution in the human genome\">heritable</a> component of <a href=\"/wiki/Human_intelligence\" title=\"Human intelligence\">human intelligence</a>.<sup class=\"reference\" id=\"cite_ref-14\"><a href=\"#cite_note-14\">[14]</a></sup> By contrast, <a href=\"/wiki/Gerald_Crabtree\" title=\"Gerald Crabtree\">Gerald Crabtree</a> has argued that decreased selection pressure is resulting in a slow, centuries-long <a href=\"/wiki/Fertility_and_intelligence\" title=\"Fertility and intelligence\">reduction in human intelligence</a>, and that this process instead is likely to continue into the future. There is no scientific consensus concerning either possibility, and in both cases the biological change would be slow, especially relative to rates of cultural change.\n</p><p><a href=\"/wiki/Selective_breeding\" title=\"Selective breeding\">Selective breeding</a>, <a class=\"mw-redirect\" href=\"/wiki/Nootropics\" title=\"Nootropics\">nootropics</a>, <a href=\"/wiki/Epigenetics\" title=\"Epigenetics\">epigenetic modulation</a>, and <a href=\"/wiki/Genetic_engineering\" title=\"Genetic engineering\">genetic engineering</a> could improve human intelligence more rapidly. Bostrom writes that if we come to understand the genetic component of intelligence, pre-implantation genetic diagnosis could be used to select for embryos with as much as 4 points of IQ gain (if one embryo is selected out of two), or with larger gains (e.g., up to 24.3 IQ points gained if one embryo is selected out of 1000). If this process is iterated over many generations, the gains could be an order of magnitude greater. Bostrom suggests that deriving new gametes from embryonic stem cells could be used to iterate the selection process very rapidly.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201437–39_15-0\"><a href=\"#cite_note-FOOTNOTEBostrom201437–39-15\">[15]</a></sup> A well-organized society of high-intelligence humans of this sort could potentially achieve <a href=\"/wiki/Collective_intelligence\" title=\"Collective intelligence\">collective</a> superintelligence.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201439_16-0\"><a href=\"#cite_note-FOOTNOTEBostrom201439-16\">[16]</a></sup>\n</p><p>Alternatively, collective intelligence might be constructible by better organizing humans at present levels of individual intelligence. A number of writers have suggested that human civilization, or some aspect of it (e.g., the Internet, or the economy), is coming to function like a <a href=\"/wiki/Global_brain\" title=\"Global brain\">global brain</a> with capacities far exceeding its component agents. If this systems-based superintelligence relies heavily on artificial components, however, it may qualify as an AI rather than as a biology-based <a href=\"/wiki/Superorganism\" title=\"Superorganism\">superorganism</a>.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201448–49_17-0\"><a href=\"#cite_note-FOOTNOTEBostrom201448–49-17\">[17]</a></sup> A <a href=\"/wiki/Prediction_market\" title=\"Prediction market\">prediction market</a> is sometimes considered an example of working collective intelligence system, consisting of humans only (assuming algorithms are not used to inform decisions).<sup class=\"reference\" id=\"cite_ref-18\"><a href=\"#cite_note-18\">[18]</a></sup>\n</p><p>A final method of intelligence amplification would be to directly <a href=\"/wiki/Neuroenhancement\" title=\"Neuroenhancement\">enhance</a> individual humans, as opposed to enhancing their social or reproductive dynamics. This could be achieved using <a class=\"mw-redirect\" href=\"/wiki/Nootropics\" title=\"Nootropics\">nootropics</a>, somatic <a href=\"/wiki/Gene_therapy\" title=\"Gene therapy\">gene therapy</a>, or <a href=\"/wiki/Brain%E2%80%93computer_interface\" title=\"Brain–computer interface\">brain–computer interfaces</a>. However, Bostrom expresses skepticism about the scalability of the first two approaches, and argues that designing a superintelligent <a href=\"/wiki/Cyborg\" title=\"Cyborg\">cyborg</a> interface is an <a href=\"/wiki/AI-complete\" title=\"AI-complete\">AI-complete</a> problem.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom201436–37,_42,_47_19-0\"><a href=\"#cite_note-FOOTNOTEBostrom201436–37,_42,_47-19\">[19]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Forecasts\">Forecasts</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=3\" title=\"Edit section: Forecasts\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Most surveyed AI researchers expect machines to eventually be able to rival humans in intelligence, though there is little consensus on when this will likely happen. At the 2006 <a href=\"/wiki/AI@50\" title=\"AI@50\">AI@50</a> conference, 18% of attendees reported expecting machines to be able \"to simulate learning and every other aspect of human intelligence\" by 2056; 41% of attendees expected this to happen sometime after 2056; and 41% expected machines to never reach that milestone.<sup class=\"reference\" id=\"cite_ref-20\"><a href=\"#cite_note-20\">[20]</a></sup>\n</p><p>In a survey of the 100 most cited authors in AI (as of May 2013, according to Microsoft academic search), the median year by which respondents expected machines \"that can carry out most human professions at least as well as a typical human\" (assuming no <a class=\"mw-redirect\" href=\"/wiki/Global_catastrophic_risks\" title=\"Global catastrophic risks\">global catastrophe</a> occurs) with 10% confidence is 2024 (mean 2034, st. dev. 33 years), with 50% confidence is 2050 (mean 2072, st. dev. 110 years), and with 90% confidence is 2070 (mean 2168, st. dev. 342 years). These estimates exclude the 1.2% of respondents who said no year would ever reach 10% confidence, the 4.1% who said 'never' for 50% confidence, and the 16.5% who said 'never' for 90% confidence. Respondents assigned a median 50% probability to the possibility that machine superintelligence will be invented within 30 years of the invention of approximately human-level machine intelligence.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEMüllerBostrom20163–4,_6,_9–12_21-0\"><a href=\"#cite_note-FOOTNOTEMüllerBostrom20163–4,_6,_9–12-21\">[21]</a></sup>\n</p><p>In a survey of 352 machine learning researchers published in 2018, the median year by which respondents expected \"High-level machine intelligence\" with 50% confidence is 2061<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (November 2021)\">citation needed</span></a></i>]</sup>. The survey defined the achievement of high-level machine intelligence as when unaided machines can accomplish every task better and more cheaply than human workers.\n</p>\n<h2><span class=\"mw-headline\" id=\"Design_considerations\">Design considerations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=4\" title=\"Edit section: Design considerations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Bostrom expressed concern about what values a superintelligence should be designed to have. He compared several proposals:<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom2014209–221_22-0\"><a href=\"#cite_note-FOOTNOTEBostrom2014209–221-22\">[22]</a></sup>\n</p>\n<ul><li>The <a class=\"mw-redirect\" href=\"/wiki/Coherent_extrapolated_volition\" title=\"Coherent extrapolated volition\">coherent extrapolated volition</a> (CEV) proposal is that it should have the values upon which humans would converge.</li>\n<li>The <a class=\"new\" href=\"/w/index.php?title=Moral_rightness&amp;action=edit&amp;redlink=1\" title=\"Moral rightness (page does not exist)\">moral rightness</a> (MR) proposal is that it should value moral rightness.</li>\n<li>The <a class=\"new\" href=\"/w/index.php?title=Moral_permissibility&amp;action=edit&amp;redlink=1\" title=\"Moral permissibility (page does not exist)\">moral permissibility</a> (MP) proposal is that it should value staying within the bounds of moral permissibility (and otherwise have CEV values).</li></ul>\n<p>Bostrom clarifies these terms:\n</p>\n<blockquote><p>instead of implementing humanity's coherent extrapolated volition, one could try to build an AI with the goal of doing what is morally right, relying on the AI’s superior cognitive capacities to figure out just which actions fit that description. We can call this proposal “moral rightness” (MR)<span class=\"nowrap\"> </span>...\nMR would also appear to have some disadvantages. It relies on the notion of “morally right,” a notoriously difficult concept, one with which philosophers have grappled since antiquity without yet attaining consensus as to its analysis. Picking an erroneous explication of “moral rightness” could result in outcomes that would be morally very wrong<span class=\"nowrap\"> </span>... The path to endowing an AI with any of these [moral] concepts might involve giving it general linguistic ability (comparable, at least, to that of a normal human adult). Such a general ability to understand natural language could then be used to understand what is meant by “morally right.” If the AI could grasp the meaning, it could search for actions that fit<span class=\"nowrap\"> </span>...<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom2014209–221_22-1\"><a href=\"#cite_note-FOOTNOTEBostrom2014209–221-22\">[22]</a></sup></p><div class=\"paragraphbreak\" style=\"margin-top:0.5em\"></div><p>One might try to preserve the basic idea of the MR model while reducing its demandingness by focusing on <i>moral permissibility</i>: the idea being that we could let the AI pursue humanity’s CEV so long as it did not act in ways that are morally impermissible.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEBostrom2014209–221_22-2\"><a href=\"#cite_note-FOOTNOTEBostrom2014209–221-22\">[22]</a></sup></p></blockquote>\n<p>Responding to Bostrom, Santos-Lang raised concern that developers may attempt to start with a single kind of superintelligence.<sup class=\"reference\" id=\"cite_ref-FOOTNOTESantos-Lang201416–19_23-0\"><a href=\"#cite_note-FOOTNOTESantos-Lang201416–19-23\">[23]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Potential_threat_to_humanity\">Potential threat to humanity</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=5\" title=\"Edit section: Potential threat to humanity\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">Main articles: <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a> and <a class=\"mw-redirect\" href=\"/wiki/AI_control_problem\" title=\"AI control problem\">AI control problem</a></div>\n<link href=\"mw-data:TemplateStyles:r1033289096\" rel=\"mw-deduplicated-inline-style\"/><div class=\"hatnote navigation-not-searchable\" role=\"note\">Further information: <a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></div>\n<p>It has been suggested that if AI systems rapidly become superintelligent, they may take unforeseen actions or out-compete humanity.<sup class=\"reference\" id=\"cite_ref-billjoy_24-0\"><a href=\"#cite_note-billjoy-24\">[24]</a></sup> Researchers have argued that, by way of an \"intelligence explosion,\" a self-improving AI could become so powerful as to be unstoppable by humans.<sup class=\"reference\" id=\"cite_ref-Muehlhauser,_Luke_2012_25-0\"><a href=\"#cite_note-Muehlhauser,_Luke_2012-25\">[25]</a></sup>\n</p><p>Concerning human extinction scenarios, <a href=\"#CITEREFBostrom2002\">Bostrom (2002)</a> identifies superintelligence as a possible cause:\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r996844942\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class=\"templatequote\"><p>When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question.</p></blockquote>\n<p>In theory, since a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled, <a href=\"/wiki/Unintended_consequences\" title=\"Unintended consequences\">unintended consequences</a> could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.<sup class=\"reference\" id=\"cite_ref-Bostrom,_Nick_2003_26-0\"><a href=\"#cite_note-Bostrom,_Nick_2003-26\">[26]</a></sup> <a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a> illustrates such <a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">instrumental convergence</a> as follows: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"<sup class=\"reference\" id=\"cite_ref-27\"><a href=\"#cite_note-27\">[27]</a></sup>\n</p><p>This presents the <a class=\"mw-redirect\" href=\"/wiki/AI_control_problem\" title=\"AI control problem\">AI control problem</a>: how to build an intelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators. The danger of not designing control right \"the first time,\" is that a superintelligence may be able to seize power over its environment and prevent humans from shutting it down. Since a superintelligent AI will likely have the ability to not fear death and instead consider it an avoidable situation which can be predicted and avoided by simply disabling the power button.<sup class=\"reference\" id=\"cite_ref-28\"><a href=\"#cite_note-28\">[28]</a></sup> Potential AI control strategies include \"capability control\" (limiting an AI's ability to influence the world) and \"motivational control\" (building an AI whose goals are aligned with human values).\n</p><p><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a> advocates for public education about superintelligence and public control over the development of superintelligence.<sup class=\"reference\" id=\"cite_ref-FOOTNOTEHibbard2002155–163_29-0\"><a href=\"#cite_note-FOOTNOTEHibbard2002155–163-29\">[29]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=6\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r998391716\">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style><div class=\"div-col\">\n<ul><li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/Artificial_brain\" title=\"Artificial brain\">Artificial brain</a></li>\n<li><a href=\"/wiki/Artificial_intelligence_arms_race\" title=\"Artificial intelligence arms race\">Artificial intelligence arms race</a></li>\n<li><a href=\"/wiki/Effective_altruism#Long-term_future_and_global_catastrophic_risks\" title=\"Effective altruism\">Effective altruism</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Existential_risk\" title=\"Existential risk\">Existential risk</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Future_of_robotics\" title=\"Future of robotics\">Future of robotics</a></li>\n<li><a href=\"/wiki/Intelligent_agent\" title=\"Intelligent agent\">Intelligent agent</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Noogenesis\" title=\"Noogenesis\">Noogenesis</a></li>\n<li><a href=\"/wiki/Outline_of_artificial_intelligence\" title=\"Outline of artificial intelligence\">Outline of artificial intelligence</a></li>\n<li><a href=\"/wiki/Posthumanism\" title=\"Posthumanism\">Posthumanism</a></li>\n<li><a href=\"/wiki/Self-replication\" title=\"Self-replication\">Self-replication</a></li>\n<li><a href=\"/wiki/Self-replicating_machine\" title=\"Self-replicating machine\">Self-replicating machine</a></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"Citations\">Citations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=7\" title=\"Edit section: Citations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r1011085734\">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class=\"reflist\">\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-FOOTNOTEBostrom2014Chapter_2-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-FOOTNOTEBostrom2014Chapter_2_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-FOOTNOTEBostrom2014Chapter_2_1-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, Chapter 2.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201422-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201422_2-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, p. 22.</span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><style data-mw-deduplicate=\"TemplateStyles:r1067248974\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite class=\"citation cs2\" id=\"CITEREFPearce2012\">Pearce, David (2012),  Eden, Amnon H.; Moor, James H.; Søraker, Johnny H.; Steinhart, Eric (eds.), <a class=\"external text\" href=\"http://link.springer.com/10.1007/978-3-642-32560-1_11\" rel=\"nofollow\">\"The Biointelligence Explosion\"</a>, <i>Singularity Hypotheses</i>, The Frontiers Collection, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 199–238, <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-642-32560-1_11\" rel=\"nofollow\">10.1007/978-3-642-32560-1_11</a>, <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-3-642-32559-5\" title=\"Special:BookSources/978-3-642-32559-5\"><bdi>978-3-642-32559-5</bdi></a><span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">2022-01-16</span></span></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Singularity+Hypotheses&amp;rft.atitle=The+Biointelligence+Explosion&amp;rft.pages=199-238&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-32560-1_11&amp;rft.isbn=978-3-642-32559-5&amp;rft.aulast=Pearce&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Flink.springer.com%2F10.1007%2F978-3-642-32560-1_11&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFGouveia2020\">Gouveia, Steven S., ed. (2020). <a class=\"external text\" href=\"https://www.biointelligence-explosion.com/parable.html\" rel=\"nofollow\">\"ch. 4, \"Humans and Intelligent Machines: Co-evolution, Fusion or Replacement?\", David Pearce\"</a>. <i>The Age of Artificial Intelligence: An Exploration</i>. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-1-62273-872-4\" title=\"Special:BookSources/978-1-62273-872-4\"><bdi>978-1-62273-872-4</bdi></a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=ch.+4%2C+%22Humans+and+Intelligent+Machines%3A+Co-evolution%2C+Fusion+or+Replacement%3F%22%2C+David+Pearce&amp;rft.btitle=The+Age+of+Artificial+Intelligence%3A+An+Exploration&amp;rft.date=2020&amp;rft.isbn=978-1-62273-872-4&amp;rft_id=https%3A%2F%2Fwww.biointelligence-explosion.com%2Fparable.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTELegg2008135–137-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTELegg2008135–137_5-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFLegg2008\">Legg 2008</a>, pp. 135–137.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers20107-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers20107_6-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p. 7.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers20107-9-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers20107-9_7-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p. 7-9.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers201010-11-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers201010-11_8-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p. 10-11.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers201011-13-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers201011-13_9-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p. 11-13.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201459-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201459_10-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, p. 59.</span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation techreport cs1\" id=\"CITEREFYudkowsky2013\"><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Yudkowsky, Eliezer</a> (2013). <a class=\"external text\" href=\"http://intelligence.org/files/IEM.pdf\" rel=\"nofollow\"><i>Intelligence Explosion Microeconomics</i></a> <span class=\"cs1-format\">(PDF)</span> (Technical report). <a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a>. p. 35. 2013-1.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Intelligence+Explosion+Microeconomics&amp;rft.pages=35&amp;rft.pub=Machine+Intelligence+Research+Institute&amp;rft.date=2013&amp;rft.aulast=Yudkowsky&amp;rft.aufirst=Eliezer&amp;rft_id=http%3A%2F%2Fintelligence.org%2Ffiles%2FIEM.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201456–57-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201456–57_12-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp. 56–57.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201452,_59–61-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201452,_59–61_13-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp. 52, 59–61.</span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFSagan1977\"><a href=\"/wiki/Carl_Sagan\" title=\"Carl Sagan\">Sagan, Carl</a> (1977). <a href=\"/wiki/The_Dragons_of_Eden\" title=\"The Dragons of Eden\"><i>The Dragons of Eden</i></a>. Random House.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Dragons+of+Eden&amp;rft.pub=Random+House&amp;rft.date=1977&amp;rft.aulast=Sagan&amp;rft.aufirst=Carl&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201437–39-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201437–39_15-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp. 37–39.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201439-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201439_16-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, p. 39.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201448–49-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201448–49_17-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp. 48–49.</span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation cs2\" id=\"CITEREFWatkins2007\">Watkins, Jennifer H. (2007), <a class=\"external text\" href=\"https://escholarship.org/uc/item/8mg0p0zc\" rel=\"nofollow\"><i>Prediction Markets as an Aggregation Mechanism for Collective Intelligence</i></a></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Prediction+Markets+as+an+Aggregation+Mechanism+for+Collective+Intelligence&amp;rft.date=2007&amp;rft.aulast=Watkins&amp;rft.aufirst=Jennifer+H.&amp;rft_id=https%3A%2F%2Fescholarship.org%2Fuc%2Fitem%2F8mg0p0zc&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201436–37,_42,_47-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201436–37,_42,_47_19-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp. 36–37, 42, 47.</span>\n</li>\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation web cs1\" id=\"CITEREFMaker2006\">Maker, Meg Houston (July 13, 2006). <a class=\"external text\" href=\"https://web.archive.org/web/20140513052243/http://www.megmaker.com/2006/07/ai50_first_poll.html\" rel=\"nofollow\">\"AI@50: First Poll\"</a>. Archived from <a class=\"external text\" href=\"http://www.megmaker.com/2006/07/ai50_first_poll.html\" rel=\"nofollow\">the original</a> on 2014-05-13.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=AI%4050%3A+First+Poll&amp;rft.date=2006-07-13&amp;rft.aulast=Maker&amp;rft.aufirst=Meg+Houston&amp;rft_id=http%3A%2F%2Fwww.megmaker.com%2F2006%2F07%2Fai50_first_poll.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEMüllerBostrom20163–4,_6,_9–12-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEMüllerBostrom20163–4,_6,_9–12_21-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFMüllerBostrom2016\">Müller &amp; Bostrom 2016</a>, pp. 3–4, 6, 9–12.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom2014209–221-22\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-FOOTNOTEBostrom2014209–221_22-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-FOOTNOTEBostrom2014209–221_22-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-FOOTNOTEBostrom2014209–221_22-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp. 209–221.</span>\n</li>\n<li id=\"cite_note-FOOTNOTESantos-Lang201416–19-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTESantos-Lang201416–19_23-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFSantos-Lang2014\">Santos-Lang 2014</a>, pp. 16–19.</span>\n</li>\n<li id=\"cite_note-billjoy-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-billjoy_24-0\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation magazine cs1\" id=\"CITEREFJoy2000\"><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Joy, Bill</a> (April 1, 2000). <a class=\"external text\" href=\"https://www.wired.com/wired/archive/8.04/joy_pr.html\" rel=\"nofollow\">\"Why the future doesn't need us\"</a>. <i>Wired</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=Why+the+future+doesn%27t+need+us&amp;rft.date=2000-04-01&amp;rft.aulast=Joy&amp;rft.aufirst=Bill&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F8.04%2Fjoy_pr.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span> See also <a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">technological singularity</a>. <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> 2002 Ethical Issues in Advanced Artificial Intelligence</span>\n</li>\n<li id=\"cite_note-Muehlhauser,_Luke_2012-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Muehlhauser,_Luke_2012_25-0\">^</a></b></span> <span class=\"reference-text\">Muehlhauser, Luke, and Louie Helm. 2012. \"Intelligence Explosion and Machine Ethics.\" In Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span>\n</li>\n<li id=\"cite_note-Bostrom,_Nick_2003-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Bostrom,_Nick_2003_26-0\">^</a></b></span> <span class=\"reference-text\">Bostrom, Nick. 2003. \"Ethical Issues in Advanced Artificial Intelligence.\" In Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, edited by Iva Smit and George E. Lasker, 12–17. Vol. 2. Windsor, ON: International Institute for Advanced Studies in Systems Research / Cybernetics.</span>\n</li>\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a> (2008) in <i><a class=\"external text\" href=\"http://intelligence.org/files/AIPosNegFactor.pdf\" rel=\"nofollow\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></i></span>\n</li>\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFRussell2016\">Russell, Stuart (2016-05-17). <a class=\"external text\" href=\"https://www.scientificamerican.com/article/should-we-fear-supersmart-robots\" rel=\"nofollow\">\"Should We Fear Supersmart Robots?\"</a>. <i>Scientific American</i>. <b>314</b> (6): 58–59. <a class=\"mw-redirect\" href=\"/wiki/Bibcode_(identifier)\" title=\"Bibcode (identifier)\">Bibcode</a>:<a class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2016SciAm.314f..58R\" rel=\"nofollow\">2016SciAm.314f..58R</a>. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1038%2Fscientificamerican0616-58\" rel=\"nofollow\">10.1038/scientificamerican0616-58</a>. <a class=\"mw-redirect\" href=\"/wiki/ISSN_(identifier)\" title=\"ISSN (identifier)\">ISSN</a> <a class=\"external text\" href=\"//www.worldcat.org/issn/0036-8733\" rel=\"nofollow\">0036-8733</a>. <a class=\"mw-redirect\" href=\"/wiki/PMID_(identifier)\" title=\"PMID (identifier)\">PMID</a> <a class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/27196844\" rel=\"nofollow\">27196844</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scientific+American&amp;rft.atitle=Should+We+Fear+Supersmart+Robots%3F&amp;rft.volume=314&amp;rft.issue=6&amp;rft.pages=58-59&amp;rft.date=2016-05-17&amp;rft_id=info%3Adoi%2F10.1038%2Fscientificamerican0616-58&amp;rft.issn=0036-8733&amp;rft_id=info%3Apmid%2F27196844&amp;rft_id=info%3Abibcode%2F2016SciAm.314f..58R&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft_id=https%3A%2F%2Fwww.scientificamerican.com%2Farticle%2Fshould-we-fear-supersmart-robots&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEHibbard2002155–163-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEHibbard2002155–163_29-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFHibbard2002\">Hibbard 2002</a>, pp. 155–163.</span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"Bibliography\">Bibliography</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=8\" title=\"Edit section: Bibliography\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<h3><span class=\"mw-headline\" id=\"Papers\">Papers</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=9\" title=\"Edit section: Papers\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<ul><li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation cs2\" id=\"CITEREFBostrom2002\"><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Bostrom, Nick</a> (2002), <a class=\"external text\" href=\"http://www.nickbostrom.com/existential/risks.html\" rel=\"nofollow\">\"Existential Risks\"</a>, <i><a class=\"mw-redirect\" href=\"/wiki/Journal_of_Evolution_and_Technology\" title=\"Journal of Evolution and Technology\">Journal of Evolution and Technology</a></i>, <b>9</b><span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">2007-08-07</span></span></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Evolution+and+Technology&amp;rft.atitle=Existential+Risks&amp;rft.volume=9&amp;rft.date=2002&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rft_id=http%3A%2F%2Fwww.nickbostrom.com%2Fexistential%2Frisks.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFChalmers2010\"><a href=\"/wiki/David_Chalmers\" title=\"David Chalmers\">Chalmers, David</a> (2010). <a class=\"external text\" href=\"http://consc.net/papers/singularity.pdf\" rel=\"nofollow\">\"The Singularity: A Philosophical Analysis\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Journal of Consciousness Studies</i>. <b>17</b>: 7–65.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=The+Singularity%3A+A+Philosophical+Analysis&amp;rft.volume=17&amp;rft.pages=7-65&amp;rft.date=2010&amp;rft.aulast=Chalmers&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Fconsc.net%2Fpapers%2Fsingularity.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation thesis cs1\" id=\"CITEREFLegg2008\">Legg, Shane (2008). <a class=\"external text\" href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\" rel=\"nofollow\"><i>Machine Super Intelligence</i></a> <span class=\"cs1-format\">(PDF)</span> (PhD). Department of Informatics, University of Lugano<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 19,</span> 2014</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Machine+Super+Intelligence&amp;rft.inst=Department+of+Informatics%2C+University+of+Lugano&amp;rft.date=2008&amp;rft.aulast=Legg&amp;rft.aufirst=Shane&amp;rft_id=http%3A%2F%2Fwww.vetta.org%2Fdocuments%2FMachine_Super_Intelligence.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation encyclopaedia cs1\" id=\"CITEREFMüllerBostrom2016\"><a href=\"/wiki/Vincent_C._M%C3%BCller\" title=\"Vincent C. Müller\">Müller, Vincent C.</a>; <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Bostrom, Nick</a> (2016). <a class=\"external text\" href=\"http://www.philpapers.org/archive/MLLFPI\" rel=\"nofollow\">\"Future Progress in Artificial Intelligence: A Survey of Expert Opinion\"</a>.  In Müller, Vincent C. (ed.). <i>Fundamental Issues of Artificial Intelligence</i>. Springer. pp. 553–571.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Future+Progress+in+Artificial+Intelligence%3A+A+Survey+of+Expert+Opinion&amp;rft.btitle=Fundamental+Issues+of+Artificial+Intelligence&amp;rft.pages=553-571&amp;rft.pub=Springer&amp;rft.date=2016&amp;rft.aulast=M%C3%BCller&amp;rft.aufirst=Vincent+C.&amp;rft.au=Bostrom%2C+Nick&amp;rft_id=http%3A%2F%2Fwww.philpapers.org%2Farchive%2FMLLFPI&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation journal cs1\" id=\"CITEREFSantos-Lang2014\">Santos-Lang, Christopher (2014). <a class=\"external text\" href=\"http://grinfree.com/Responsibility.pdf\" rel=\"nofollow\">\"Our responsibility to manage evaluative diversity\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>ACM SIGCAS Computers &amp; Society</i>. <b>44</b> (2): 16–19. <a class=\"mw-redirect\" href=\"/wiki/Doi_(identifier)\" title=\"Doi (identifier)\">doi</a>:<a class=\"external text\" href=\"https://doi.org/10.1145%2F2656870.2656874\" rel=\"nofollow\">10.1145/2656870.2656874</a>. <a class=\"mw-redirect\" href=\"/wiki/S2CID_(identifier)\" title=\"S2CID (identifier)\">S2CID</a> <a class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:5649158\" rel=\"nofollow\">5649158</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+SIGCAS+Computers+%26+Society&amp;rft.atitle=Our+responsibility+to+manage+evaluative+diversity&amp;rft.volume=44&amp;rft.issue=2&amp;rft.pages=16-19&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1145%2F2656870.2656874&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5649158%23id-name%3DS2CID&amp;rft.aulast=Santos-Lang&amp;rft.aufirst=Christopher&amp;rft_id=http%3A%2F%2Fgrinfree.com%2FResponsibility.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li></ul>\n<h3><span class=\"mw-headline\" id=\"Books\">Books</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=10\" title=\"Edit section: Books\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<ul><li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFHibbard2002\"><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Hibbard, Bill</a> (2002). <i>Super-Intelligent Machines</i>. Kluwer Academic/Plenum Publishers.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Super-Intelligent+Machines&amp;rft.pub=Kluwer+Academic%2FPlenum+Publishers&amp;rft.date=2002&amp;rft.aulast=Hibbard&amp;rft.aufirst=Bill&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFBostrom2014\"><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Bostrom, Nick</a> (2014). <a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\"><i>Superintelligence: Paths, Dangers, Strategies</i></a>. Oxford University Press.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFTegmark2018\"><a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Tegmark, Max</a> (2018). <i>Life 3.0: being human in the age of artificial intelligence</i>. London. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-14-198180-2\" title=\"Special:BookSources/978-0-14-198180-2\"><bdi>978-0-14-198180-2</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1018461467\" rel=\"nofollow\">1018461467</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Life+3.0%3A+being+human+in+the+age+of+artificial+intelligence&amp;rft.place=London&amp;rft.date=2018&amp;rft_id=info%3Aoclcnum%2F1018461467&amp;rft.isbn=978-0-14-198180-2&amp;rft.aulast=Tegmark&amp;rft.aufirst=Max&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFRussell2019\">Russell, Stuart J. (2019). <i>Human compatible: artificial intelligence and the problem of control</i>. New York. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-525-55861-3\" title=\"Special:BookSources/978-0-525-55861-3\"><bdi>978-0-525-55861-3</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1113410915\" rel=\"nofollow\">1113410915</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+compatible%3A+artificial+intelligence+and+the+problem+of+control&amp;rft.place=New+York&amp;rft.date=2019&amp;rft_id=info%3Aoclcnum%2F1113410915&amp;rft.isbn=978-0-525-55861-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li>\n<li><link href=\"mw-data:TemplateStyles:r1067248974\" rel=\"mw-deduplicated-inline-style\"/><cite class=\"citation book cs1\" id=\"CITEREFSanders2020\"><a href=\"/wiki/Nada_Sanders\" title=\"Nada Sanders\">Sanders, Nada R.</a> (2020). <i>The humachine: humankind, machines, and the future of enterprise</i>. John D. Wood (First ed.). New York, NY. <a class=\"mw-redirect\" href=\"/wiki/ISBN_(identifier)\" title=\"ISBN (identifier)\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0-429-00117-8\" title=\"Special:BookSources/978-0-429-00117-8\"><bdi>978-0-429-00117-8</bdi></a>. <a class=\"mw-redirect\" href=\"/wiki/OCLC_(identifier)\" title=\"OCLC (identifier)\">OCLC</a> <a class=\"external text\" href=\"//www.worldcat.org/oclc/1119391268\" rel=\"nofollow\">1119391268</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+humachine%3A+humankind%2C+machines%2C+and+the+future+of+enterprise&amp;rft.place=New+York%2C+NY&amp;rft.edition=First&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1119391268&amp;rft.isbn=978-0-429-00117-8&amp;rft.aulast=Sanders&amp;rft.aufirst=Nada+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\"></span></li></ul>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=11\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><i><a class=\"external text\" href=\"http://www.evolutionnews.org/2015/02/bill_gates_join093191.html\" rel=\"nofollow\">Bill Gates Joins Stephen Hawking in Fears of a Coming Threat from \"Superintelligence\"</a></i></li>\n<li><i><a class=\"external text\" href=\"http://reason.com/archives/2014/09/12/will-superintelligent-machines-destroy-h\" rel=\"nofollow\">Will Superintelligent Machines Destroy Humanity?</a></i></li>\n<li><i><a class=\"external text\" href=\"http://www.ign.com/articles/2015/03/25/apple-co-founder-has-sense-of-foreboding-about-artificial-superintelligence\" rel=\"nofollow\">Apple Co-founder Has Sense of Foreboding About Artificial Superintelligence</a></i></li></ul>\n<div class=\"navbox-styles nomobile\"><style data-mw-deduplicate=\"TemplateStyles:r1061467846\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div aria-labelledby=\"Existential_risk_from_artificial_intelligence\" class=\"navbox\" role=\"navigation\" style=\"padding:3px\"><table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th class=\"navbox-title\" colspan=\"2\" scope=\"col\"><style data-mw-deduplicate=\"TemplateStyles:r1063604349\">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Existential_risk_from_artificial_intelligence\" title=\"Template:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Existential_risk_from_artificial_intelligence\" title=\"Template talk:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit\"><abbr style=\";;background:none transparent;border:none;box-shadow:none;padding:0;\" title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Existential_risk_from_artificial_intelligence\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk</a> from <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a></div></th></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Concepts</th><td class=\"navbox-list-with-group navbox-list navbox-odd hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/AI_alignment\" title=\"AI alignment\">AI alignment</a></li>\n<li><a href=\"/wiki/AI_capability_control\" title=\"AI capability control\">AI capability control</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/Accelerating_change\" title=\"Accelerating change\">Accelerating change</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></li>\n<li><a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">Instrumental convergence</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">Intelligence explosion</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a class=\"mw-selflink selflink\">Superintelligence</a></li>\n<li><a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">Technological singularity</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Organizations</th><td class=\"navbox-list-with-group navbox-list navbox-even hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Allen_Institute_for_AI\" title=\"Allen Institute for AI\">Allen Institute for AI</a></li>\n<li><a href=\"/wiki/Center_for_Applied_Rationality\" title=\"Center for Applied Rationality\">Center for Applied Rationality</a></li>\n<li><a href=\"/wiki/Center_for_Human-Compatible_Artificial_Intelligence\" title=\"Center for Human-Compatible Artificial Intelligence\">Center for Human-Compatible Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Centre_for_the_Study_of_Existential_Risk\" title=\"Centre for the Study of Existential Risk\">Centre for the Study of Existential Risk</a></li>\n<li><a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></li>\n<li><a href=\"/wiki/Foundational_Questions_Institute\" title=\"Foundational Questions Institute\">Foundational Questions Institute</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a href=\"/wiki/Future_of_Life_Institute\" title=\"Future of Life Institute\">Future of Life Institute</a></li>\n<li><a href=\"/wiki/Humanity%2B\" title=\"Humanity+\">Humanity+</a></li>\n<li><a href=\"/wiki/Institute_for_Ethics_and_Emerging_Technologies\" title=\"Institute for Ethics and Emerging Technologies\">Institute for Ethics and Emerging Technologies</a></li>\n<li><a href=\"/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence\" title=\"Leverhulme Centre for the Future of Intelligence\">Leverhulme Centre for the Future of Intelligence</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">People</th><td class=\"navbox-list-with-group navbox-list navbox-odd hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Slate_Star_Codex\" title=\"Slate Star Codex\">Scott Alexander</a></li>\n<li><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a></li>\n<li><a href=\"/wiki/K._Eric_Drexler\" title=\"K. Eric Drexler\">Eric Drexler</a></li>\n<li><a href=\"/wiki/Sam_Harris\" title=\"Sam Harris\">Sam Harris</a></li>\n<li><a href=\"/wiki/Stephen_Hawking\" title=\"Stephen Hawking\">Stephen Hawking</a></li>\n<li><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a></li>\n<li><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Bill Joy</a></li>\n<li><a href=\"/wiki/Elon_Musk\" title=\"Elon Musk\">Elon Musk</a></li>\n<li><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a></li>\n<li><a href=\"/wiki/Huw_Price\" title=\"Huw Price\">Huw Price</a></li>\n<li><a href=\"/wiki/Martin_Rees\" title=\"Martin Rees\">Martin Rees</a></li>\n<li><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a></li>\n<li><a href=\"/wiki/Jaan_Tallinn\" title=\"Jaan Tallinn\">Jaan Tallinn</a></li>\n<li><a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Max Tegmark</a></li>\n<li><a href=\"/wiki/Frank_Wilczek\" title=\"Frank Wilczek\">Frank Wilczek</a></li>\n<li><a href=\"/wiki/Roman_Yampolskiy\" title=\"Roman Yampolskiy\">Roman Yampolskiy</a></li>\n<li><a href=\"/wiki/Andrew_Yang\" title=\"Andrew Yang\">Andrew Yang</a></li>\n<li><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Other</th><td class=\"navbox-list-with-group navbox-list navbox-even hlist\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Global_catastrophic_risk#Artificial_intelligence\" title=\"Global catastrophic risk\">Artificial intelligence as a global catastrophic risk</a></li>\n<li><a href=\"/wiki/Artificial_general_intelligence#Controversies_and_dangers\" title=\"Artificial general intelligence\">Controversies and dangers of artificial general intelligence</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Suffering_risks\" title=\"Suffering risks\">Suffering risks</a></li>\n<li><i><a href=\"/wiki/Human_Compatible\" title=\"Human Compatible\">Human Compatible</a></i></li>\n<li><a href=\"/wiki/Open_Letter_on_Artificial_Intelligence\" title=\"Open Letter on Artificial Intelligence\">Open Letter on Artificial Intelligence</a></li>\n<li><i><a href=\"/wiki/Our_Final_Invention\" title=\"Our Final Invention\">Our Final Invention</a></i></li>\n<li><i><a href=\"/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity\" title=\"The Precipice: Existential Risk and the Future of Humanity\">The Precipice</a></i></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li>\n<li><i><a href=\"/wiki/Do_You_Trust_This_Computer%3F\" title=\"Do You Trust This Computer?\">Do You Trust This Computer?</a></i></li></ul>\n</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div><img alt=\"\" class=\"noviewer\" data-file-height=\"185\" data-file-width=\"180\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x\" title=\"Category\" width=\"16\"/> <a href=\"/wiki/Category:Existential_risk_from_artificial_general_intelligence\" title=\"Category:Existential risk from artificial general intelligence\">Category</a></div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw2311\nCached time: 20221107222812\nCache expiry: 1814400\nReduced expiry: false\nComplications: [vary‐revision‐sha1, show‐toc]\nCPU time usage: 0.388 seconds\nReal time usage: 0.529 seconds\nPreprocessor visited node count: 3112/1000000\nPost‐expand include size: 57438/2097152 bytes\nTemplate argument size: 4652/2097152 bytes\nHighest expansion depth: 12/100\nExpensive parser function count: 7/500\nUnstrip recursion depth: 1/20\nUnstrip post‐expand size: 57015/5000000 bytes\nLua time usage: 0.234/10.000 seconds\nLua memory usage: 8430912/52428800 bytes\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  465.404      1 -total\n 26.36%  122.685      1 Template:Reflist\n 16.94%   78.859     21 Template:Sfn\n 15.70%   73.052      3 Template:Citation\n 10.74%   49.980      1 Template:Short_description\n 10.72%   49.902      1 Template:Existential_risk_from_artificial_intelligence\n 10.27%   47.798      1 Template:Navbox\n  8.42%   39.210      2 Template:Cn\n  7.94%   36.943      3 Template:Fix\n  5.94%   27.667      2 Template:Pagetype\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:726659-0!canonical and timestamp 20221107222811 and revision id 1115139010.\n -->\n</div><noscript><img alt=\"\" height=\"1\" src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" style=\"border: none; position: absolute;\" title=\"\" width=\"1\"/></noscript>\n<div class=\"printfooter\" data-nosnippet=\"\">Retrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Superintelligence&amp;oldid=1115139010\">https://en.wikipedia.org/w/index.php?title=Superintelligence&amp;oldid=1115139010</a>\"</div></div>\n<div class=\"catlinks\" data-mw=\"interface\" id=\"catlinks\"><div class=\"mw-normal-catlinks\" id=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Hypothetical_technology\" title=\"Category:Hypothetical technology\">Hypothetical technology</a></li><li><a href=\"/wiki/Category:Singularitarianism\" title=\"Category:Singularitarianism\">Singularitarianism</a></li><li><a href=\"/wiki/Category:Intelligence\" title=\"Category:Intelligence\">Intelligence</a></li><li><a href=\"/wiki/Category:Existential_risk_from_artificial_general_intelligence\" title=\"Category:Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li></ul></div><div class=\"mw-hidden-catlinks mw-hidden-cats-hidden\" id=\"mw-hidden-catlinks\">Hidden categories: <ul><li><a href=\"/wiki/Category:Articles_with_short_description\" title=\"Category:Articles with short description\">Articles with short description</a></li><li><a href=\"/wiki/Category:Short_description_is_different_from_Wikidata\" title=\"Category:Short description is different from Wikidata\">Short description is different from Wikidata</a></li><li><a href=\"/wiki/Category:All_articles_with_unsourced_statements\" title=\"Category:All articles with unsourced statements\">All articles with unsourced statements</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_July_2022\" title=\"Category:Articles with unsourced statements from July 2022\">Articles with unsourced statements from July 2022</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_November_2021\" title=\"Category:Articles with unsourced statements from November 2021\">Articles with unsourced statements from November 2021</a></li></ul></div></div>\n</div>\n</div>\n<div id=\"mw-navigation\">\n<h2>Navigation menu</h2>\n<div id=\"mw-head\">\n<nav aria-labelledby=\"p-personal-label\" class=\"vector-menu mw-portlet mw-portlet-personal vector-user-menu-legacy\" id=\"p-personal\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n<span class=\"vector-menu-heading-label\">Personal tools</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"pt-anonuserpage\"><span title=\"The user page for the IP address you are editing as\">Not logged in</span></li><li class=\"mw-list-item\" id=\"pt-anontalk\"><a accesskey=\"n\" href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\"><span>Talk</span></a></li><li class=\"mw-list-item\" id=\"pt-anoncontribs\"><a accesskey=\"y\" href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\"><span>Contributions</span></a></li><li class=\"mw-list-item\" id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Superintelligence\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\"><span>Create account</span></a></li><li class=\"mw-list-item\" id=\"pt-login\"><a accesskey=\"o\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Superintelligence\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\"><span>Log in</span></a></li></ul>\n</div>\n</nav>\n<div id=\"left-navigation\">\n<nav aria-labelledby=\"p-namespaces-label\" class=\"vector-menu mw-portlet mw-portlet-namespaces vector-menu-tabs vector-menu-tabs-legacy\" id=\"p-namespaces\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n<span class=\"vector-menu-heading-label\">Namespaces</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"selected mw-list-item\" id=\"ca-nstab-main\"><a accesskey=\"c\" href=\"/wiki/Superintelligence\" title=\"View the content page [c]\"><span>Article</span></a></li><li class=\"mw-list-item\" id=\"ca-talk\"><a accesskey=\"t\" href=\"/wiki/Talk:Superintelligence\" rel=\"discussion\" title=\"Discuss improvements to the content page [t]\"><span>Talk</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-variants-label\" class=\"vector-menu mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu-dropdown\" id=\"p-variants\" role=\"navigation\">\n<input aria-haspopup=\"true\" aria-labelledby=\"p-variants-label\" class=\"vector-menu-checkbox\" data-event-name=\"ui.dropdown-p-variants\" id=\"p-variants-checkbox\" role=\"button\" type=\"checkbox\"/>\n<label aria-label=\"Change language variant\" class=\"vector-menu-heading\" id=\"p-variants-label\">\n<span class=\"vector-menu-heading-label\">English</span>\n</label>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"></ul>\n</div>\n</nav>\n</div>\n<div id=\"right-navigation\">\n<nav aria-labelledby=\"p-views-label\" class=\"vector-menu mw-portlet mw-portlet-views vector-menu-tabs vector-menu-tabs-legacy\" id=\"p-views\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n<span class=\"vector-menu-heading-label\">Views</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"selected mw-list-item\" id=\"ca-view\"><a href=\"/wiki/Superintelligence\"><span>Read</span></a></li><li class=\"mw-list-item\" id=\"ca-edit\"><a accesskey=\"e\" href=\"/w/index.php?title=Superintelligence&amp;action=edit\" title=\"Edit this page [e]\"><span>Edit</span></a></li><li class=\"mw-list-item\" id=\"ca-history\"><a accesskey=\"h\" href=\"/w/index.php?title=Superintelligence&amp;action=history\" title=\"Past revisions of this page [h]\"><span>View history</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-cactions-label\" class=\"vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu-dropdown\" id=\"p-cactions\" role=\"navigation\" title=\"More options\">\n<input aria-haspopup=\"true\" aria-labelledby=\"p-cactions-label\" class=\"vector-menu-checkbox\" data-event-name=\"ui.dropdown-p-cactions\" id=\"p-cactions-checkbox\" role=\"button\" type=\"checkbox\"/>\n<label class=\"vector-menu-heading\" id=\"p-cactions-label\">\n<span class=\"vector-menu-heading-label\">More</span>\n</label>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"></ul>\n</div>\n</nav>\n<div class=\"vector-search-box-vue vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box\" id=\"p-search\" role=\"search\">\n<div>\n<h3>\n<label for=\"searchInput\">Search</label>\n</h3>\n<form action=\"/w/index.php\" class=\"vector-search-box-form\" id=\"searchform\">\n<div class=\"vector-search-box-inner\" data-search-loc=\"header-navigation\" id=\"simpleSearch\">\n<input accesskey=\"f\" aria-label=\"Search Wikipedia\" autocapitalize=\"sentences\" class=\"vector-search-box-input\" id=\"searchInput\" name=\"search\" placeholder=\"Search Wikipedia\" title=\"Search Wikipedia [f]\" type=\"search\"/>\n<input name=\"title\" type=\"hidden\" value=\"Special:Search\"/>\n<input class=\"searchButton mw-fallbackSearchButton\" id=\"mw-searchButton\" name=\"fulltext\" title=\"Search Wikipedia for this text\" type=\"submit\" value=\"Search\"/>\n<input class=\"searchButton\" id=\"searchButton\" name=\"go\" title=\"Go to a page with this exact name if it exists\" type=\"submit\" value=\"Go\"/>\n</div>\n</form>\n</div>\n</div>\n</div>\n</div>\n<div id=\"mw-panel\">\n<div id=\"p-logo\" role=\"banner\">\n<a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a>\n</div>\n<nav aria-labelledby=\"p-navigation-label\" class=\"vector-menu mw-portlet mw-portlet-navigation vector-menu-portal portal\" id=\"p-navigation\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n<span class=\"vector-menu-heading-label\">Navigation</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"n-mainpage-description\"><a accesskey=\"z\" href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\"><span>Main page</span></a></li><li class=\"mw-list-item\" id=\"n-contents\"><a href=\"/wiki/Wikipedia:Contents\" title=\"Guides to browsing Wikipedia\"><span>Contents</span></a></li><li class=\"mw-list-item\" id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Articles related to current events\"><span>Current events</span></a></li><li class=\"mw-list-item\" id=\"n-randompage\"><a accesskey=\"x\" href=\"/wiki/Special:Random\" title=\"Visit a randomly selected article [x]\"><span>Random article</span></a></li><li class=\"mw-list-item\" id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Learn about Wikipedia and how it works\"><span>About Wikipedia</span></a></li><li class=\"mw-list-item\" id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\"><span>Contact us</span></a></li><li class=\"mw-list-item\" id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us by donating to the Wikimedia Foundation\"><span>Donate</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-interaction-label\" class=\"vector-menu mw-portlet mw-portlet-interaction vector-menu-portal portal\" id=\"p-interaction\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n<span class=\"vector-menu-heading-label\">Contribute</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\"><span>Help</span></a></li><li class=\"mw-list-item\" id=\"n-introduction\"><a href=\"/wiki/Help:Introduction\" title=\"Learn how to edit Wikipedia\"><span>Learn to edit</span></a></li><li class=\"mw-list-item\" id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"The hub for editors\"><span>Community portal</span></a></li><li class=\"mw-list-item\" id=\"n-recentchanges\"><a accesskey=\"r\" href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes to Wikipedia [r]\"><span>Recent changes</span></a></li><li class=\"mw-list-item\" id=\"n-upload\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Add images or other media for use on Wikipedia\"><span>Upload file</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-tb-label\" class=\"vector-menu mw-portlet mw-portlet-tb vector-menu-portal portal\" id=\"p-tb\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n<span class=\"vector-menu-heading-label\">Tools</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"t-whatlinkshere\"><a accesskey=\"j\" href=\"/wiki/Special:WhatLinksHere/Superintelligence\" title=\"List of all English Wikipedia pages containing links to this page [j]\"><span>What links here</span></a></li><li class=\"mw-list-item\" id=\"t-recentchangeslinked\"><a accesskey=\"k\" href=\"/wiki/Special:RecentChangesLinked/Superintelligence\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\"><span>Related changes</span></a></li><li class=\"mw-list-item\" id=\"t-upload\"><a accesskey=\"u\" href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\"><span>Upload file</span></a></li><li class=\"mw-list-item\" id=\"t-specialpages\"><a accesskey=\"q\" href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\"><span>Special pages</span></a></li><li class=\"mw-list-item\" id=\"t-permalink\"><a href=\"/w/index.php?title=Superintelligence&amp;oldid=1115139010\" title=\"Permanent link to this revision of this page\"><span>Permanent link</span></a></li><li class=\"mw-list-item\" id=\"t-info\"><a href=\"/w/index.php?title=Superintelligence&amp;action=info\" title=\"More information about this page\"><span>Page information</span></a></li><li class=\"mw-list-item\" id=\"t-cite\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Superintelligence&amp;id=1115139010&amp;wpFormIdentifier=titleform\" title=\"Information on how to cite this page\"><span>Cite this page</span></a></li><li class=\"mw-list-item\" id=\"t-wikibase\"><a accesskey=\"g\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q1566000\" title=\"Structured data on this page hosted by Wikidata [g]\"><span>Wikidata item</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-coll-print_export-label\" class=\"vector-menu mw-portlet mw-portlet-coll-print_export vector-menu-portal portal\" id=\"p-coll-print_export\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n<span class=\"vector-menu-heading-label\">Print/export</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"mw-list-item\" id=\"coll-download-as-rl\"><a href=\"/w/index.php?title=Special:DownloadAsPdf&amp;page=Superintelligence&amp;action=show-download-screen\" title=\"Download this page as a PDF file\"><span>Download as PDF</span></a></li><li class=\"mw-list-item\" id=\"t-print\"><a accesskey=\"p\" href=\"/w/index.php?title=Superintelligence&amp;printable=yes\" title=\"Printable version of this page [p]\"><span>Printable version</span></a></li></ul>\n</div>\n</nav>\n<nav aria-labelledby=\"p-lang-label\" class=\"vector-menu mw-portlet mw-portlet-lang vector-menu-portal portal\" id=\"p-lang\" role=\"navigation\">\n<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n<span class=\"vector-menu-heading-label\">Languages</span>\n</h3>\n<div class=\"vector-menu-content\">\n<ul class=\"vector-menu-content-list\"><li class=\"interlanguage-link interwiki-de mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://de.wikipedia.org/wiki/Superintelligenz\" hreflang=\"de\" lang=\"de\" title=\"Superintelligenz – German\"><span>Deutsch</span></a></li><li class=\"interlanguage-link interwiki-el mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://el.wikipedia.org/wiki/%CE%A5%CF%80%CE%B5%CF%81%CE%B5%CF%85%CF%86%CF%85%CE%90%CE%B1\" hreflang=\"el\" lang=\"el\" title=\"Υπερευφυΐα – Greek\"><span>Ελληνικά</span></a></li><li class=\"interlanguage-link interwiki-es mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://es.wikipedia.org/wiki/Superinteligencia\" hreflang=\"es\" lang=\"es\" title=\"Superinteligencia – Spanish\"><span>Español</span></a></li><li class=\"interlanguage-link interwiki-fa mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://fa.wikipedia.org/wiki/%D9%81%D8%B1%D8%A7%D9%87%D9%88%D8%B4\" hreflang=\"fa\" lang=\"fa\" title=\"فراهوش – Persian\"><span>فارسی</span></a></li><li class=\"interlanguage-link interwiki-fr mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://fr.wikipedia.org/wiki/Superintelligence\" hreflang=\"fr\" lang=\"fr\" title=\"Superintelligence – French\"><span>Français</span></a></li><li class=\"interlanguage-link interwiki-id mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://id.wikipedia.org/wiki/Kecerdasan_super\" hreflang=\"id\" lang=\"id\" title=\"Kecerdasan super – Indonesian\"><span>Bahasa Indonesia</span></a></li><li class=\"interlanguage-link interwiki-ms mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ms.wikipedia.org/wiki/Kecerdasan_super\" hreflang=\"ms\" lang=\"ms\" title=\"Kecerdasan super – Malay\"><span>Bahasa Melayu</span></a></li><li class=\"interlanguage-link interwiki-nl mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://nl.wikipedia.org/wiki/Superintelligentie\" hreflang=\"nl\" lang=\"nl\" title=\"Superintelligentie – Dutch\"><span>Nederlands</span></a></li><li class=\"interlanguage-link interwiki-ja mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ja.wikipedia.org/wiki/%E8%B6%85%E7%9F%A5%E8%83%BD\" hreflang=\"ja\" lang=\"ja\" title=\"超知能 – Japanese\"><span>日本語</span></a></li><li class=\"interlanguage-link interwiki-pl mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://pl.wikipedia.org/wiki/Superinteligencja\" hreflang=\"pl\" lang=\"pl\" title=\"Superinteligencja – Polish\"><span>Polski</span></a></li><li class=\"interlanguage-link interwiki-pt mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://pt.wikipedia.org/wiki/Superintelig%C3%AAncia\" hreflang=\"pt\" lang=\"pt\" title=\"Superinteligência – Portuguese\"><span>Português</span></a></li><li class=\"interlanguage-link interwiki-ro mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://ro.wikipedia.org/wiki/Superinteligen%C8%9B%C4%83\" hreflang=\"ro\" lang=\"ro\" title=\"Superinteligență – Romanian\"><span>Română</span></a></li><li class=\"interlanguage-link interwiki-sv mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://sv.wikipedia.org/wiki/Superintelligens_(AI)\" hreflang=\"sv\" lang=\"sv\" title=\"Superintelligens (AI) – Swedish\"><span>Svenska</span></a></li><li class=\"interlanguage-link interwiki-uk mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://uk.wikipedia.org/wiki/%D0%A1%D1%83%D0%BF%D0%B5%D1%80%D1%96%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82\" hreflang=\"uk\" lang=\"uk\" title=\"Суперінтелект – Ukrainian\"><span>Українська</span></a></li><li class=\"interlanguage-link interwiki-vi mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://vi.wikipedia.org/wiki/Si%C3%AAu_tr%C3%AD_th%C3%B4ng_minh\" hreflang=\"vi\" lang=\"vi\" title=\"Siêu trí thông minh – Vietnamese\"><span>Tiếng Việt</span></a></li><li class=\"interlanguage-link interwiki-zh-yue mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://zh-yue.wikipedia.org/wiki/%E8%B6%85%E6%99%BA%E8%83%BD\" hreflang=\"yue\" lang=\"yue\" title=\"超智能 – Cantonese\"><span>粵語</span></a></li><li class=\"interlanguage-link interwiki-zh mw-list-item\"><a class=\"interlanguage-link-target\" href=\"https://zh.wikipedia.org/wiki/%E8%B6%85%E6%99%BA%E8%83%BD\" hreflang=\"zh\" lang=\"zh\" title=\"超智能 – Chinese\"><span>中文</span></a></li></ul>\n<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-edit wb-langlinks-link\"><a class=\"wbc-editpage\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q1566000#sitelinks-wikipedia\" title=\"Edit interlanguage links\">Edit links</a></span></div>\n</div>\n</nav>\n</div>\n</div>\n<footer class=\"mw-footer\" id=\"footer\" role=\"contentinfo\">\n<ul id=\"footer-info\">\n<li id=\"footer-info-lastmod\"> This page was last edited on 10 October 2022, at 00:41<span class=\"anonymous-show\"> (UTC)</span>.</li>\n<li id=\"footer-info-copyright\">Text is available under the <a href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\" rel=\"license\">Creative Commons Attribution-ShareAlike License 3.0</a><a href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\" style=\"display:none;\"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id=\"footer-places\">\n<li id=\"footer-places-privacy\"><a href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\">Privacy policy</a></li>\n<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\">About Wikipedia</a></li>\n<li id=\"footer-places-disclaimers\"><a href=\"/wiki/Wikipedia:General_disclaimer\">Disclaimers</a></li>\n<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n<li id=\"footer-places-mobileview\"><a class=\"noprint stopMobileRedirectToggle\" href=\"//en.m.wikipedia.org/w/index.php?title=Superintelligence&amp;mobileaction=toggle_view_mobile\">Mobile view</a></li>\n<li id=\"footer-places-developers\"><a href=\"https://developer.wikimedia.org\">Developers</a></li>\n<li id=\"footer-places-statslink\"><a href=\"https://stats.wikimedia.org/#/en.wikipedia.org\">Statistics</a></li>\n<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\n</ul>\n<ul class=\"noprint\" id=\"footer-icons\">\n<li id=\"footer-copyrightico\"><a href=\"https://wikimediafoundation.org/\"><img alt=\"Wikimedia Foundation\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/wikimedia-button.png\" srcset=\"/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x\" width=\"88\"/></a></li>\n<li id=\"footer-poweredbyico\"><a href=\"https://www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" loading=\"lazy\" src=\"/static/images/footer/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a></li>\n</ul>\n</footer>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.388\",\"walltime\":\"0.529\",\"ppvisitednodes\":{\"value\":3112,\"limit\":1000000},\"postexpandincludesize\":{\"value\":57438,\"limit\":2097152},\"templateargumentsize\":{\"value\":4652,\"limit\":2097152},\"expansiondepth\":{\"value\":12,\"limit\":100},\"expensivefunctioncount\":{\"value\":7,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":57015,\"limit\":5000000},\"entityaccesscount\":{\"value\":0,\"limit\":400},\"timingprofile\":[\"100.00%  465.404      1 -total\",\" 26.36%  122.685      1 Template:Reflist\",\" 16.94%   78.859     21 Template:Sfn\",\" 15.70%   73.052      3 Template:Citation\",\" 10.74%   49.980      1 Template:Short_description\",\" 10.72%   49.902      1 Template:Existential_risk_from_artificial_intelligence\",\" 10.27%   47.798      1 Template:Navbox\",\"  8.42%   39.210      2 Template:Cn\",\"  7.94%   36.943      3 Template:Fix\",\"  5.94%   27.667      2 Template:Pagetype\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.234\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":8430912,\"limit\":52428800},\"limitreport-logs\":\"anchor_id_list = table#1 {\\n    [\\\"CITEREFBostrom2002\\\"] = 1,\\n    [\\\"CITEREFBostrom2014\\\"] = 1,\\n    [\\\"CITEREFChalmers2010\\\"] = 1,\\n    [\\\"CITEREFGouveia2020\\\"] = 1,\\n    [\\\"CITEREFHibbard2002\\\"] = 1,\\n    [\\\"CITEREFJoy2000\\\"] = 1,\\n    [\\\"CITEREFLegg2008\\\"] = 1,\\n    [\\\"CITEREFMaker2006\\\"] = 1,\\n    [\\\"CITEREFMüllerBostrom2016\\\"] = 1,\\n    [\\\"CITEREFPearce2012\\\"] = 1,\\n    [\\\"CITEREFRussell2016\\\"] = 1,\\n    [\\\"CITEREFRussell2019\\\"] = 1,\\n    [\\\"CITEREFSagan1977\\\"] = 1,\\n    [\\\"CITEREFSanders2020\\\"] = 1,\\n    [\\\"CITEREFSantos-Lang2014\\\"] = 1,\\n    [\\\"CITEREFTegmark2018\\\"] = 1,\\n    [\\\"CITEREFWatkins2007\\\"] = 1,\\n    [\\\"CITEREFYudkowsky2013\\\"] = 1,\\n}\\ntemplate_list = table#1 {\\n    [\\\"Annotated link\\\"] = 1,\\n    [\\\"Citation\\\"] = 3,\\n    [\\\"Citation needed\\\"] = 1,\\n    [\\\"Cite book\\\"] = 7,\\n    [\\\"Cite encyclopedia\\\"] = 1,\\n    [\\\"Cite journal\\\"] = 3,\\n    [\\\"Cite magazine\\\"] = 1,\\n    [\\\"Cite techreport\\\"] = 1,\\n    [\\\"Cite thesis\\\"] = 1,\\n    [\\\"Cite web\\\"] = 1,\\n    [\\\"Cn\\\"] = 2,\\n    [\\\"Div col\\\"] = 1,\\n    [\\\"Div col end\\\"] = 1,\\n    [\\\"Existential risk from artificial intelligence\\\"] = 1,\\n    [\\\"For multi\\\"] = 1,\\n    [\\\"Further\\\"] = 1,\\n    [\\\"Harvtxt\\\"] = 1,\\n    [\\\"Main\\\"] = 1,\\n    [\\\"Nbsp\\\"] = 3,\\n    [\\\"Pb\\\"] = 1,\\n    [\\\"Quote\\\"] = 1,\\n    [\\\"Reflist\\\"] = 1,\\n    [\\\"Sfn\\\"] = 21,\\n    [\\\"Short description\\\"] = 1,\\n}\\narticle_whitelist = table#1 {\\n}\\n\"},\"cachereport\":{\"origin\":\"mw2311\",\"timestamp\":\"20221107222812\",\"ttl\":1814400,\"transientcontent\":false}}});});</script>\n<script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"Superintelligence\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/Superintelligence\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q1566000\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q1566000\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2004-06-15T03:33:49Z\",\"dateModified\":\"2022-10-10T00:41:11Z\",\"headline\":\"hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds\"}</script><script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"Superintelligence\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/Superintelligence\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q1566000\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q1566000\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2004-06-15T03:33:49Z\",\"dateModified\":\"2022-10-10T00:41:11Z\",\"headline\":\"hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds\"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgBackendResponseTime\":92,\"wgHostname\":\"mw2412\"});});</script>\n</body>\n</html>",
    "table_of_contents": [
        "1 Feasibility of artificial superintelligence",
        "2 Feasibility of biological superintelligence",
        "3 Forecasts",
        "4 Design considerations",
        "5 Potential threat to humanity",
        "6 See also",
        "7 Citations",
        "8 Bibliography",
        "8.1 Papers",
        "8.2 Books",
        "9 External links"
    ],
    "graphics": [
        {
            "url": "/wiki/File:Classification_of_images_progress_human.png",
            "caption": "Progress in machine classification of images  The error rate of AI by year. The red line represents the error rate of a trained human."
        }
    ],
    "paragraphs": [
        {
            "title": "",
            "text": "A superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. \"Superintelligence\" may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an intelligence explosion and associated with a technological singularity.\n\nUniversity of Oxford philosopher Nick Bostrom defines superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\".[1] The program Fritz falls short of superintelligence—even though it is much better than humans at chess—because Fritz cannot outperform humans in other tasks.[2] Following Hutter and Legg, Bostrom treats superintelligence as general dominance at goal-oriented behavior, leaving open whether an artificial or human superintelligence would possess capacities such as intentionality (cf. the Chinese room argument) or first-person consciousness (cf. the hard problem of consciousness).\n\nTechnological researchers disagree about how likely present-day human intelligence is to be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence.[3][4] A number of futures studies scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification.\n\nSome researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of perfect recall, a vastly superior knowledge base, and the ability to multitask in ways not possible to biological entities. This may give them the opportunity to—either as a single being or as a new species—become much more powerful than humans, and to displace them.[1]\n\nA number of scientists and forecasters argue for prioritizing early research into the possible benefits and risks of human and machine cognitive enhancement, because of the potential social impact of such technologies.[5]\n\n"
        },
        {
            "title": "Feasibility of artificial superintelligence",
            "text": "Philosopher David Chalmers argues that artificial general intelligence is a very likely path to superhuman intelligence. Chalmers breaks this claim down into an argument that AI can achieve equivalence to human intelligence, that it can be extended to surpass human intelligence, and that it can be further amplified to completely dominate humans across arbitrary tasks.[6]\n\nConcerning human-level equivalence, Chalmers argues that the human brain is a mechanical system, and therefore ought to be emulatable by synthetic materials.[7] He also notes that human intelligence was able to biologically evolve, making it more likely that human engineers will be able to recapitulate this invention. Evolutionary algorithms in particular should be able to produce human-level AI.[8] Concerning intelligence extension and amplification, Chalmers argues that new AI technologies can generally be improved on, and that this is particularly likely when the invention can assist in designing new technologies.[9]\n\nIf research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\".[citation needed] It would then be even better at improving itself, and could continue doing so in a rapidly increasing cycle, leading to a superintelligence. This scenario is known as an intelligence explosion.  Such an intelligence would not have the limitations of human intellect, and may be able to invent or discover almost anything. However, it is also possible that any such intelligence would conclude that existential nihilism is correct and immediately destroy itself, making any kind of superintelligence inherently unstable.[citation needed]\n\nComputer components already greatly surpass human performance in speed.  Bostrom writes, \"Biological neurons operate at a peak speed of about 200 Hz, a full seven orders of magnitude slower than a modern microprocessor (~2 GHz).\"[10] Moreover, neurons transmit spike signals across axons at no greater than 120 m/s, \"whereas existing electronic processing cores can communicate optically at the speed of light\". Thus, the simplest example of a superintelligence may be an emulated human mind run on much faster hardware than the brain. A human-like reasoner that could think millions of times faster than current humans would have a dominant advantage in most reasoning tasks, particularly ones that require haste or long strings of actions.\n\nAnother advantage of computers is modularity, that is, their size or computational capacity can be increased.  A non-human (or modified human) brain could become much larger than a present-day human brain, like many supercomputers. Bostrom also raises the possibility of collective superintelligence: a large enough number of separate reasoning systems, if they communicated and coordinated well enough, could act in aggregate with far greater capabilities than any sub-agent.\n\nThere may also be ways to qualitatively improve on human reasoning and decision-making. Humans appear to differ from chimpanzees in the ways we think more than we differ in brain size or speed.[11] Humans outperform non-human animals in large part because of new or enhanced reasoning capacities, such as long-term planning and language use. (See evolution of human intelligence and primate cognition.) If there are other possible improvements to reasoning that would have a similarly large impact, this makes it likelier that an agent can be built that outperforms humans in the same fashion humans outperform chimpanzees.[12]\n\nAll of the above advantages hold for artificial superintelligence, but it is not clear how many hold for biological superintelligence. Physiological constraints limit the speed and size of biological brains in many ways that are inapplicable to machine intelligence. As such, writers on superintelligence have devoted much more attention to superintelligent AI scenarios.[13]\n\n"
        },
        {
            "title": "Feasibility of biological superintelligence",
            "text": "Carl Sagan suggested that the advent of Caesarean sections and in vitro fertilization may permit humans to evolve larger heads, resulting in improvements via natural selection in the heritable component of human intelligence.[14] By contrast, Gerald Crabtree has argued that decreased selection pressure is resulting in a slow, centuries-long reduction in human intelligence, and that this process instead is likely to continue into the future. There is no scientific consensus concerning either possibility, and in both cases the biological change would be slow, especially relative to rates of cultural change.\n\nSelective breeding, nootropics, epigenetic modulation, and genetic engineering could improve human intelligence more rapidly. Bostrom writes that if we come to understand the genetic component of intelligence, pre-implantation genetic diagnosis could be used to select for embryos with as much as 4 points of IQ gain (if one embryo is selected out of two), or with larger gains (e.g., up to 24.3 IQ points gained if one embryo is selected out of 1000). If this process is iterated over many generations, the gains could be an order of magnitude greater. Bostrom suggests that deriving new gametes from embryonic stem cells could be used to iterate the selection process very rapidly.[15] A well-organized society of high-intelligence humans of this sort could potentially achieve collective superintelligence.[16]\n\nAlternatively, collective intelligence might be constructible by better organizing humans at present levels of individual intelligence. A number of writers have suggested that human civilization, or some aspect of it (e.g., the Internet, or the economy), is coming to function like a global brain with capacities far exceeding its component agents. If this systems-based superintelligence relies heavily on artificial components, however, it may qualify as an AI rather than as a biology-based superorganism.[17] A prediction market is sometimes considered an example of working collective intelligence system, consisting of humans only (assuming algorithms are not used to inform decisions).[18]\n\nA final method of intelligence amplification would be to directly enhance individual humans, as opposed to enhancing their social or reproductive dynamics. This could be achieved using nootropics, somatic gene therapy, or brain–computer interfaces. However, Bostrom expresses skepticism about the scalability of the first two approaches, and argues that designing a superintelligent cyborg interface is an AI-complete problem.[19]\n\n"
        },
        {
            "title": "Forecasts",
            "text": "Most surveyed AI researchers expect machines to eventually be able to rival humans in intelligence, though there is little consensus on when this will likely happen. At the 2006 AI@50 conference, 18% of attendees reported expecting machines to be able \"to simulate learning and every other aspect of human intelligence\" by 2056; 41% of attendees expected this to happen sometime after 2056; and 41% expected machines to never reach that milestone.[20]\n\nIn a survey of the 100 most cited authors in AI (as of May 2013, according to Microsoft academic search), the median year by which respondents expected machines \"that can carry out most human professions at least as well as a typical human\" (assuming no global catastrophe occurs) with 10% confidence is 2024 (mean 2034, st. dev. 33 years), with 50% confidence is 2050 (mean 2072, st. dev. 110 years), and with 90% confidence is 2070 (mean 2168, st. dev. 342 years). These estimates exclude the 1.2% of respondents who said no year would ever reach 10% confidence, the 4.1% who said 'never' for 50% confidence, and the 16.5% who said 'never' for 90% confidence. Respondents assigned a median 50% probability to the possibility that machine superintelligence will be invented within 30 years of the invention of approximately human-level machine intelligence.[21]\n\nIn a survey of 352 machine learning researchers published in 2018, the median year by which respondents expected \"High-level machine intelligence\" with 50% confidence is 2061[citation needed]. The survey defined the achievement of high-level machine intelligence as when unaided machines can accomplish every task better and more cheaply than human workers.\n\n"
        },
        {
            "title": "Design considerations",
            "text": "Bostrom expressed concern about what values a superintelligence should be designed to have. He compared several proposals:[22]\n\nBostrom clarifies these terms:\n\nResponding to Bostrom, Santos-Lang raised concern that developers may attempt to start with a single kind of superintelligence.[23]\n\n"
        },
        {
            "title": "Potential threat to humanity",
            "text": "It has been suggested that if AI systems rapidly become superintelligent, they may take unforeseen actions or out-compete humanity.[24] Researchers have argued that, by way of an \"intelligence explosion,\" a self-improving AI could become so powerful as to be unstoppable by humans.[25]\n\nConcerning human extinction scenarios, Bostrom (2002) identifies superintelligence as a possible cause:\n\nIn theory, since a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled, unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.[26] Eliezer Yudkowsky illustrates such instrumental convergence as follows: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"[27]\n\nThis presents the AI control problem: how to build an intelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators. The danger of not designing control right \"the first time,\" is that a superintelligence may be able to seize power over its environment and prevent humans from shutting it down. Since a superintelligent AI will likely have the ability to not fear death and instead consider it an avoidable situation which can be predicted and avoided by simply disabling the power button.[28] Potential AI control strategies include \"capability control\" (limiting an AI's ability to influence the world) and \"motivational control\" (building an AI whose goals are aligned with human values).\n\nBill Hibbard advocates for public education about superintelligence and public control over the development of superintelligence.[29]\n\n"
        }
    ],
    "links": [
        "https://en.wikipedia.org/wiki/Intelligent_agent",
        "https://en.wikipedia.org/wiki/Intelligence",
        "https://en.wikipedia.org/wiki/Genius",
        "https://en.wikipedia.org/wiki/Intellectual_giftedness",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/University_of_Oxford",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Intentionality",
        "https://en.wikipedia.org/wiki/Chinese_room",
        "https://en.wikipedia.org/wiki/Qualia",
        "https://en.wikipedia.org/wiki/Hard_problem_of_consciousness",
        "https://en.wikipedia.org/wiki/Human_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Futures_studies",
        "https://en.wikipedia.org/wiki/Mind_uploading",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Eidetic_memory",
        "https://en.wikipedia.org/wiki/Human_multitasking",
        "https://en.wikipedia.org/wiki/Species",
        "https://en.wikipedia.org/wiki/Intelligence_amplification",
        "https://en.wikipedia.org/wiki/David_Chalmers",
        "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Existential_nihilism",
        "https://en.wikipedia.org/wiki/Neuron",
        "https://en.wikipedia.org/wiki/Axon",
        "https://en.wikipedia.org/wiki/Supercomputer",
        "https://en.wikipedia.org/wiki/Common_chimpanzee",
        "https://en.wikipedia.org/wiki/Great_ape_language",
        "https://en.wikipedia.org/wiki/Evolution_of_human_intelligence",
        "https://en.wikipedia.org/wiki/Primate_cognition",
        "https://en.wikipedia.org/wiki/Carl_Sagan",
        "https://en.wikipedia.org/wiki/Caesarean_section",
        "https://en.wikipedia.org/wiki/In_vitro_fertilisation",
        "https://en.wikipedia.org/wiki/Natural_selection",
        "https://en.wikipedia.org/wiki/Adaptive_evolution_in_the_human_genome",
        "https://en.wikipedia.org/wiki/Human_intelligence",
        "https://en.wikipedia.org/wiki/Gerald_Crabtree",
        "https://en.wikipedia.org/wiki/Fertility_and_intelligence",
        "https://en.wikipedia.org/wiki/Selective_breeding",
        "https://en.wikipedia.org/wiki/Nootropics",
        "https://en.wikipedia.org/wiki/Epigenetics",
        "https://en.wikipedia.org/wiki/Genetic_engineering",
        "https://en.wikipedia.org/wiki/Collective_intelligence",
        "https://en.wikipedia.org/wiki/Global_brain",
        "https://en.wikipedia.org/wiki/Superorganism",
        "https://en.wikipedia.org/wiki/Prediction_market",
        "https://en.wikipedia.org/wiki/Neuroenhancement",
        "https://en.wikipedia.org/wiki/Nootropics",
        "https://en.wikipedia.org/wiki/Gene_therapy",
        "https://en.wikipedia.org/wiki/Cyborg",
        "https://en.wikipedia.org/wiki/Global_catastrophic_risks",
        "https://en.wikipedia.org/wiki/Coherent_extrapolated_volition",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Unintended_consequences",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/AI_control_problem",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Artificial_brain",
        "https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Existential_risk",
        "https://en.wikipedia.org/wiki/Future_of_Humanity_Institute",
        "https://en.wikipedia.org/wiki/Future_of_robotics",
        "https://en.wikipedia.org/wiki/Intelligent_agent",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/Machine_learning",
        "https://en.wikipedia.org/wiki/Noogenesis",
        "https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Posthumanism",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/Carl_Sagan",
        "https://en.wikipedia.org/wiki/The_Dragons_of_Eden",
        "https://en.wikipedia.org/wiki/Bill_Joy",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Journal_of_Evolution_and_Technology",
        "https://en.wikipedia.org/wiki/David_Chalmers",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Max_Tegmark",
        "https://en.wikipedia.org/wiki/Nada_Sanders",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/AI_alignment",
        "https://en.wikipedia.org/wiki/AI_capability_control",
        "https://en.wikipedia.org/wiki/AI_takeover",
        "https://en.wikipedia.org/wiki/Accelerating_change",
        "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Instrumental_convergence",
        "https://en.wikipedia.org/wiki/Intelligence_explosion",
        "https://en.wikipedia.org/wiki/Machine_ethics",
        "https://en.wikipedia.org/wiki/Technological_singularity",
        "https://en.wikipedia.org/wiki/Allen_Institute_for_AI",
        "https://en.wikipedia.org/wiki/Center_for_Applied_Rationality",
        "https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk",
        "https://en.wikipedia.org/wiki/DeepMind",
        "https://en.wikipedia.org/wiki/Foundational_Questions_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Humanity_Institute",
        "https://en.wikipedia.org/wiki/Future_of_Life_Institute",
        "https://en.wikipedia.org/wiki/Institute_for_Ethics_and_Emerging_Technologies",
        "https://en.wikipedia.org/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence",
        "https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute",
        "https://en.wikipedia.org/wiki/OpenAI",
        "https://en.wikipedia.org/wiki/Slate_Star_Codex",
        "https://en.wikipedia.org/wiki/Nick_Bostrom",
        "https://en.wikipedia.org/wiki/Sam_Harris",
        "https://en.wikipedia.org/wiki/Stephen_Hawking",
        "https://en.wikipedia.org/wiki/Bill_Hibbard",
        "https://en.wikipedia.org/wiki/Bill_Joy",
        "https://en.wikipedia.org/wiki/Elon_Musk",
        "https://en.wikipedia.org/wiki/Steve_Omohundro",
        "https://en.wikipedia.org/wiki/Huw_Price",
        "https://en.wikipedia.org/wiki/Martin_Rees",
        "https://en.wikipedia.org/wiki/Jaan_Tallinn",
        "https://en.wikipedia.org/wiki/Max_Tegmark",
        "https://en.wikipedia.org/wiki/Frank_Wilczek",
        "https://en.wikipedia.org/wiki/Roman_Yampolskiy",
        "https://en.wikipedia.org/wiki/Andrew_Yang",
        "https://en.wikipedia.org/wiki/Eliezer_Yudkowsky",
        "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "https://en.wikipedia.org/wiki/Suffering_risks",
        "https://en.wikipedia.org/wiki/Human_Compatible",
        "https://en.wikipedia.org/wiki/Open_Letter_on_Artificial_Intelligence",
        "https://en.wikipedia.org/wiki/Our_Final_Invention",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Superintelligence",
        "https://en.wikipedia.org/wiki/Main_Page",
        "https://en.wikipedia.org/wiki/Main_Page"
    ]
}